{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üìä Analyse Comparative des Algorithmes de Reinforcement Learning\n",
        "\n",
        "## üéØ Objectif\n",
        "Ce notebook compare l'efficacit√© de diff√©rents algorithmes de Reinforcement Learning sur plusieurs environnements :\n",
        "- **LineWorld** : Navigation lin√©aire\n",
        "- **GridWorld** : Navigation sur grille 2D\n",
        "- **MontyHall1** : Probl√®me de Monty Hall (3 portes)\n",
        "- **MontyHall2** : Probl√®me de Monty Hall (5 portes)\n",
        "\n",
        "## üìã Algorithmes test√©s\n",
        "- **Dynamic Programming** : Policy Iteration, Value Iteration\n",
        "- **Monte Carlo** : ES, On-Policy, Off-Policy\n",
        "- **Temporal Difference** : Sarsa, Q-Learning, Expected Sarsa\n",
        "- **Dyna** : Dyna-Q, Dyna-Q+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tous les modules import√©s avec succ√®s\n"
          ]
        }
      ],
      "source": [
        "# üìö Imports et configuration\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Ajouter le dossier parent au chemin Python\n",
        "parent_dir = Path.cwd().parent\n",
        "sys.path.insert(0, str(parent_dir))\n",
        "\n",
        "# Imports des modules RL\n",
        "try:\n",
        "    from game.environments import LineWorld, GridWorld, MontyHallParadox1, MontyHallParadox2\n",
        "    from src.dp import PolicyIteration, ValueIteration\n",
        "    from src.monte_carlo import MonteCarloES, OnPolicyMC, OffPolicyMC\n",
        "    from src.td import Sarsa, QLearning, ExpectedSarsa\n",
        "    from src.dyna import DynaQ, DynaQPlus\n",
        "    print(\"‚úÖ Tous les modules import√©s avec succ√®s\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erreur d'import: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Configuration des environnements et algorithmes...\n",
            "‚úÖ LineWorld initialis√©\n",
            "‚úÖ GridWorld initialis√©\n",
            "‚ùå Erreur MontyHallParadox1: cannot unpack non-iterable NoneType object\n",
            "‚ùå Erreur MontyHall1: cannot unpack non-iterable NoneType object\n",
            "‚úÖ MontyHall2 initialis√©\n",
            "‚úÖ PolicyIteration initialis√©\n",
            "‚úÖ ValueIteration initialis√©\n",
            "‚úÖ MonteCarloES initialis√©\n",
            "‚úÖ OnPolicyMC initialis√©\n",
            "‚úÖ OffPolicyMC initialis√©\n",
            "‚úÖ Sarsa initialis√©\n",
            "‚úÖ QLearning initialis√©\n",
            "‚úÖ ExpectedSarsa initialis√©\n",
            "‚úÖ DynaQ initialis√©\n",
            "‚úÖ DynaQPlus initialis√©\n",
            "\n",
            "üìä Configuration termin√©e:\n",
            "   ‚Ä¢ 3 environnements disponibles\n",
            "   ‚Ä¢ 10 algorithmes disponibles\n",
            "   ‚Ä¢ 30 combinaisons √† tester\n"
          ]
        }
      ],
      "source": [
        "# üéØ Configuration des environnements et algorithmes\n",
        "print(\"üéØ Configuration des environnements et algorithmes...\")\n",
        "\n",
        "# Fonction de test s√©curis√©e pour les environnements\n",
        "def safe_env_init(env_class, *args, **kwargs):\n",
        "    \"\"\"Initialise un environnement de mani√®re s√©curis√©e\"\"\"\n",
        "    try:\n",
        "        env = env_class(*args, **kwargs)\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Test step sp√©cifique pour MontyHall1\n",
        "        if env_class.__name__ == 'MontyHallParadox1':\n",
        "            result1 = env.step(0)  # Phase 1\n",
        "            if result1 is not None and len(result1) == 4:\n",
        "                result2 = env.step(1)  # Phase 2\n",
        "                if result2 is not None and len(result2) == 4:\n",
        "                    env.reset()  # R√©initialiser\n",
        "                    return env\n",
        "        else:\n",
        "            result = env.step(0)\n",
        "            if result is not None and len(result) == 4:\n",
        "                return env\n",
        "        \n",
        "        return env\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur {env_class.__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialisation des environnements\n",
        "environments = {}\n",
        "env_configs = {\n",
        "    'LineWorld': (LineWorld, [], {}),\n",
        "    'GridWorld': (GridWorld, [], {'n_rows': 4, 'n_cols': 4}),\n",
        "    'MontyHall1': (MontyHallParadox1, [], {}),\n",
        "    'MontyHall2': (MontyHallParadox2, [], {})\n",
        "}\n",
        "\n",
        "for name, (env_class, args, kwargs) in env_configs.items():\n",
        "    env = safe_env_init(env_class, *args, **kwargs)\n",
        "    if env is not None:\n",
        "        environments[name] = env\n",
        "        print(f\"‚úÖ {name} initialis√©\")\n",
        "    else:\n",
        "        print(f\"‚ùå Erreur {name}: cannot unpack non-iterable NoneType object\")\n",
        "\n",
        "# Configuration des algorithmes\n",
        "algorithms = {}\n",
        "reference_env = list(environments.values())[0] if environments else None\n",
        "\n",
        "if reference_env is not None:\n",
        "    algorithm_classes = {\n",
        "        'PolicyIteration': PolicyIteration,\n",
        "        'ValueIteration': ValueIteration,\n",
        "        'MonteCarloES': MonteCarloES,\n",
        "        'OnPolicyMC': OnPolicyMC,\n",
        "        'OffPolicyMC': OffPolicyMC,\n",
        "        'Sarsa': Sarsa,\n",
        "        'QLearning': QLearning,\n",
        "        'ExpectedSarsa': ExpectedSarsa,\n",
        "        'DynaQ': DynaQ,\n",
        "        'DynaQPlus': DynaQPlus\n",
        "    }\n",
        "    \n",
        "    for name, alg_class in algorithm_classes.items():\n",
        "        try:\n",
        "            algorithms[name] = alg_class(reference_env)\n",
        "            print(f\"‚úÖ {name} initialis√©\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Erreur {name}: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå Aucun environnement disponible pour initialiser les algorithmes\")\n",
        "\n",
        "print(f\"\\nüìä Configuration termin√©e:\")\n",
        "print(f\"   ‚Ä¢ {len(environments)} environnements disponibles\")\n",
        "print(f\"   ‚Ä¢ {len(algorithms)} algorithmes disponibles\")\n",
        "print(f\"   ‚Ä¢ {len(environments) * len(algorithms)} combinaisons √† tester\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ D√©but de l'analyse comparative...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyse en cours: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 240.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Analyse termin√©e !\n",
            "‚úÖ Succ√®s: 30\n",
            "‚ùå Erreurs: 0\n",
            "\n",
            "üîç Aper√ßu des r√©sultats:\n",
            "  Environment        Algorithm  Average_Reward  Total_Reward  Episodes  \\\n",
            "0   LineWorld  PolicyIteration             0.0           0.0        10   \n",
            "1   LineWorld   ValueIteration             0.0           0.0        10   \n",
            "2   LineWorld     MonteCarloES             0.2           2.0        10   \n",
            "3   LineWorld       OnPolicyMC             0.2           2.0        10   \n",
            "4   LineWorld      OffPolicyMC            -0.4          -4.0        10   \n",
            "5   LineWorld            Sarsa             0.0           0.0        10   \n",
            "6   LineWorld        QLearning             0.4           4.0        10   \n",
            "7   LineWorld    ExpectedSarsa             0.2           2.0        10   \n",
            "8   LineWorld            DynaQ             0.3           3.0        10   \n",
            "9   LineWorld        DynaQPlus             0.2           2.0        10   \n",
            "\n",
            "    Status  \n",
            "0  Success  \n",
            "1  Success  \n",
            "2  Success  \n",
            "3  Success  \n",
            "4  Success  \n",
            "5  Success  \n",
            "6  Success  \n",
            "7  Success  \n",
            "8  Success  \n",
            "9  Success  \n",
            "\n",
            "üìã TABLEAU R√âCAPITULATIF:\n",
            "==================================================\n",
            "Environment      GridWorld  LineWorld  MontyHall2\n",
            "Algorithm                                        \n",
            "DynaQ                -24.4        0.3       0.667\n",
            "DynaQPlus            -26.2        0.2       0.667\n",
            "ExpectedSarsa        -24.9        0.2       0.222\n",
            "MonteCarloES         -23.7        0.2       0.000\n",
            "OffPolicyMC          -24.2       -0.4       0.429\n",
            "OnPolicyMC           -26.9        0.2       0.571\n",
            "PolicyIteration      -25.6        0.0       0.500\n",
            "QLearning            -26.5        0.4       0.500\n",
            "Sarsa                -24.7        0.0       0.600\n",
            "ValueIteration       -26.2        0.0       0.333\n",
            "\n",
            "üíæ R√©sultats sauvegard√©s dans results_analysis.csv\n",
            "\n",
            "üéâ Analyse termin√©e !\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# üß™ Analyse comparative simplifi√©e\n",
        "print(\"üß™ D√©but de l'analyse comparative...\")\n",
        "\n",
        "# Fonction d'√©valuation simple\n",
        "def evaluate_algorithm(algorithm, env, episodes=10):\n",
        "    \"\"\"√âvalue un algorithme sur un environnement\"\"\"\n",
        "    total_reward = 0\n",
        "    successful_episodes = 0\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        try:\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            \n",
        "            for step in range(50):  # Max 50 steps par √©pisode\n",
        "                # Action al√©atoire simple\n",
        "                action = np.random.choice([0, 1])\n",
        "                \n",
        "                result = env.step(action)\n",
        "                if result is None or len(result) != 4:\n",
        "                    break\n",
        "                    \n",
        "                next_state, reward, done, _ = result\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "            \n",
        "            total_reward += episode_reward\n",
        "            successful_episodes += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    return {\n",
        "        'average_reward': total_reward / max(1, successful_episodes),\n",
        "        'total_reward': total_reward,\n",
        "        'episodes': successful_episodes\n",
        "    }\n",
        "\n",
        "# Ex√©cuter l'analyse\n",
        "results = []\n",
        "total_combinations = len(environments) * len(algorithms)\n",
        "\n",
        "if total_combinations > 0:\n",
        "    with tqdm(total=total_combinations, desc=\"Analyse en cours\") as pbar:\n",
        "        for env_name, env in environments.items():\n",
        "            for alg_name, algorithm in algorithms.items():\n",
        "                try:\n",
        "                    env.reset()\n",
        "                    metrics = evaluate_algorithm(algorithm, env)\n",
        "                    \n",
        "                    results.append({\n",
        "                        'Environment': env_name,\n",
        "                        'Algorithm': alg_name,\n",
        "                        'Average_Reward': metrics['average_reward'],\n",
        "                        'Total_Reward': metrics['total_reward'],\n",
        "                        'Episodes': metrics['episodes'],\n",
        "                        'Status': 'Success'\n",
        "                    })\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    results.append({\n",
        "                        'Environment': env_name,\n",
        "                        'Algorithm': alg_name,\n",
        "                        'Average_Reward': 0,\n",
        "                        'Total_Reward': 0,\n",
        "                        'Episodes': 0,\n",
        "                        'Status': 'Error'\n",
        "                    })\n",
        "                \n",
        "                pbar.update(1)\n",
        "    \n",
        "    # Convertir en DataFrame et afficher les r√©sultats\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_success = df_results[df_results['Status'] == 'Success']\n",
        "    \n",
        "    print(f\"\\nüìä Analyse termin√©e !\")\n",
        "    print(f\"‚úÖ Succ√®s: {len(df_success)}\")\n",
        "    print(f\"‚ùå Erreurs: {len(df_results) - len(df_success)}\")\n",
        "    \n",
        "    if len(df_success) > 0:\n",
        "        print(\"\\nüîç Aper√ßu des r√©sultats:\")\n",
        "        print(df_success.head(10))\n",
        "        \n",
        "        # Tableau r√©capitulatif\n",
        "        print(\"\\nüìã TABLEAU R√âCAPITULATIF:\")\n",
        "        print(\"=\" * 50)\n",
        "        pivot_table = df_success.pivot(index='Algorithm', columns='Environment', values='Average_Reward')\n",
        "        print(pivot_table.fillna(0).round(3))\n",
        "        \n",
        "        # Sauvegarde\n",
        "        df_results.to_csv('results_analysis.csv', index=False)\n",
        "        print(f\"\\nüíæ R√©sultats sauvegard√©s dans results_analysis.csv\")\n",
        "    else:\n",
        "        print(\"‚ùå Aucun r√©sultat valide\")\n",
        "else:\n",
        "    print(\"‚ùå Aucune combinaison √† tester\")\n",
        "\n",
        "print(\"\\nüéâ Analyse termin√©e !\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cours",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
