import streamlit as st
import importlib
import sys
import os
from typing import Any, Dict
import json
from datetime import datetime
import numpy as np
import pickle
import glob
import time

# Lancer avec l'instructrion
# python -m streamlit run notebooks/main.py

# Ajout des chemins pour l'import dynamique des modules src et game
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../game')))

# Import des environnements RL personnalis√©s
from game import environments

# Ajout des environnements secrets
from game.secret_env.secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3

# Dictionnaire des environnements disponibles
env_classes = {
    'LineWorld': environments.LineWorld,
    'GridWorld': environments.GridWorld,
    'MontyHallParadox1': environments.MontyHallParadox1,
    'MontyHallParadox2': environments.MontyHallParadox2,
    'RockPaperScissors': environments.RockPaperScissors,
    'SecretEnv0': SecretEnv0,
    'SecretEnv1': SecretEnv1,
    'SecretEnv2': SecretEnv2,
    'SecretEnv3': SecretEnv3,
}

# Dictionnaire des mod√®les RL disponibles (nom: (module, classe))
model_classes = {
    'PolicyIteration': ('dp', 'PolicyIteration'),
    'ValueIteration': ('dp', 'ValueIteration'),
    'Sarsa': ('td', 'Sarsa'),
    'QLearning': ('td', 'QLearning'),
    'ExpectedSarsa': ('td', 'ExpectedSarsa'),
    'MonteCarloES': ('monte_carlo', 'MonteCarloES'),
    'OnPolicyMC': ('monte_carlo', 'OnPolicyMC'),
    'OffPolicyMC': ('monte_carlo', 'OffPolicyMC'),
    'DynaQ': ('dyna', 'DynaQ'),
    'DynaQPlus': ('dyna', 'DynaQPlus')
}

# Fonction utilitaire pour charger dynamiquement une classe de mod√®le
# √† partir de son nom de module et de classe

def get_model_class(module_name: str, class_name: str):
    module = importlib.import_module(module_name)
    return getattr(module, class_name)

# Fonction utilitaire pour rendre un objet JSON-serializable (conversion numpy ‚Üí python natif)
def to_serializable(obj):
    import numpy as np
    if isinstance(obj, dict):
        # Convertit les cl√©s tuple en string
        return {str(k): to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_serializable(v) for v in obj]
    elif hasattr(obj, 'tolist'):  # Pour les numpy arrays
        return obj.tolist()
    elif isinstance(obj, (np.integer,)):
        return int(obj)
    elif isinstance(obj, (np.floating,)):
        return float(obj)
    else:
        return obj

# Dossier o√π chaque exp√©rimentation est sauvegard√©e dans un fichier s√©par√©
EXPERIMENTS_DIR = os.path.join(os.path.dirname(__file__), 'experiments_history')
os.makedirs(EXPERIMENTS_DIR, exist_ok=True)

# Fonction pour sauvegarder un r√©sultat de test dans un fichier s√©par√©
# Le nom du fichier est bas√© sur la date, l'environnement et le mod√®le
import re
def slugify(value):
    value = str(value)
    value = re.sub(r'[^\w\-]+', '_', value)
    return value

def save_test_result(result: dict):
    dt = result.get('datetime') or datetime.now().isoformat()
    env = slugify(result.get('environnement', 'env'))
    model = slugify(result.get('modele', 'model'))
    filename = f"{dt.replace(':', '-').replace('.', '-')}_{env}_{model}.json"
    filepath = os.path.join(EXPERIMENTS_DIR, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(to_serializable(result), f, indent=2, ensure_ascii=False)

# Fonction pour charger l'historique des tests depuis tous les fichiers du dossier
def load_test_history():
    history = []
    if os.path.exists(EXPERIMENTS_DIR):
        for fname in os.listdir(EXPERIMENTS_DIR):
            if fname.endswith('.json'):
                fpath = os.path.join(EXPERIMENTS_DIR, fname)
                try:
                    with open(fpath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        history.append(data)
                except Exception:
                    pass
    # Tri par date d√©croissante si possible
    def get_dt(x):
        return x.get('datetime', '')
    history.sort(key=get_dt, reverse=True)
    return history

def save_trained_model(model, env_name, model_name, hyperparams):
    """Sauvegarde le mod√®le entra√Æn√© au format pickle dans le dossier models/ avec un nom explicite, en excluant les objets non picklables."""
    from datetime import datetime
    import hashlib
    # G√©n√©rer un hash court des hyperparam√®tres pour unicit√©
    hp_str = str(sorted(hyperparams.items()))
    hp_hash = hashlib.md5(hp_str.encode()).hexdigest()[:8]
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{model_name}_{env_name}_{timestamp}_{hp_hash}.pkl"
    path = os.path.join(os.path.dirname(__file__), '../models', filename)
    # Patch : suppression temporaire des attributs non picklables
    env = getattr(model, 'env', None)
    obs_space = None
    act_space = None
    if env is not None:
        if hasattr(env, 'observation_space'):
            obs_space = env.observation_space
            del env.observation_space
        if hasattr(env, 'action_space'):
            act_space = env.action_space
            del env.action_space
    try:
        with open(path, 'wb') as f:
            pickle.dump(model, f)
    finally:
        # On restaure les attributs pour ne pas casser l'environnement en m√©moire
        if env is not None:
            if obs_space is not None:
                env.observation_space = obs_space
            if act_space is not None:
                env.action_space = act_space
    return path

# Configuration de la page Streamlit (titre, layout)
st.set_page_config(page_title="RLProject", layout="wide")

# Cr√©ation de deux onglets : Ex√©cution et Historique
tabs = st.tabs(["Ex√©cution", "Historique des tests"])

# Onglet principal : Ex√©cution d'un test RL
with tabs[0]:
    st.title('RLProject - D√©monstrateur RL')

    # S√©lection de l'environnement et du mod√®le
    env_name = st.selectbox('Choisissez un environnement RL', list(env_classes.keys()))
    model_name = st.selectbox('Choisissez un mod√®le RL', list(model_classes.keys()))

    # Instanciation de l'environnement choisi
    env = env_classes[env_name]()
    # Patch s√©curit√© : s'assurer que observation_space et action_space existent
    if not hasattr(env, 'observation_space'):
        if hasattr(env, 'num_states'):
            nS = env.num_states() if callable(env.num_states) else env.num_states
            env.observation_space = type('obj', (), {'n': nS})()
        elif hasattr(env, 'get_state_space'):
            nS = len(env.get_state_space())
            env.observation_space = type('obj', (), {'n': nS})()
    if not hasattr(env, 'action_space'):
        if hasattr(env, 'num_actions'):
            nA = env.num_actions() if callable(env.num_actions) else env.num_actions
            env.action_space = type('obj', (), {'n': nA})()
        elif hasattr(env, 'get_action_space'):
            nA = len(env.get_action_space())
            env.action_space = type('obj', (), {'n': nA})()

    # Affichage dynamique des hyperparam√®tres selon le mod√®le choisi
    hyperparams: Dict[str, Any] = {}

    # Affichage des sliders/inputs selon le mod√®le s√©lectionn√©
    if model_name in ['PolicyIteration', 'ValueIteration']:
        gamma = st.slider('Gamma (facteur d\'actualisation)', 0.0, 1.0, 0.99, 0.01)
        theta = st.number_input('Theta (seuil de convergence)', min_value=1e-8, max_value=1e-2, value=1e-6, step=1e-8, format="%e")
        hyperparams = {'gamma': gamma, 'theta': theta}
    elif model_name in ['Sarsa', 'QLearning', 'ExpectedSarsa', 'DynaQ', 'DynaQPlus']:
        gamma = st.slider('Gamma', 0.0, 1.0, 0.99, 0.01)
        alpha = st.slider('Alpha (taux d\'apprentissage)', 0.0, 1.0, 0.1, 0.01)
        epsilon = st.slider('Epsilon (exploration)', 0.0, 1.0, 0.1, 0.01)
        episodes = st.number_input('Nombre d\'√©pisodes', min_value=1, max_value=10000, value=500)
        hyperparams = {'gamma': gamma, 'alpha': alpha, 'epsilon': epsilon, 'num_episodes': episodes}
    elif model_name in ['MonteCarloES', 'OnPolicyMC', 'OffPolicyMC']:
        gamma = st.slider('Gamma', 0.0, 1.0, 0.99, 0.01)
        epsilon = st.slider('Epsilon', 0.0, 1.0, 0.1, 0.01)
        episodes = st.number_input('Nombre d\'√©pisodes', min_value=1, max_value=10000, value=500)
        hyperparams = {'gamma': gamma, 'epsilon': epsilon, 'num_episodes': episodes}
    else:
        st.info("Aucun hyperparam√®tre sp√©cifique pour ce mod√®le.")

    # Lancement de l'entra√Ænement
    if st.button('Lancer l\'entra√Ænement'):
        with st.spinner('Entra√Ænement en cours...'):
            module_name, class_name = model_classes[model_name]
            ModelClass = get_model_class(module_name, class_name)
            # Instanciation du mod√®le avec les bons hyperparam√®tres
            try:
                if model_name in ['PolicyIteration', 'ValueIteration']:
                    model = ModelClass(env, gamma=hyperparams['gamma'], theta=hyperparams['theta'])
                    result = model.train()
                elif model_name in ['Sarsa', 'QLearning', 'ExpectedSarsa']:
                    model = ModelClass(env, gamma=hyperparams['gamma'], alpha=hyperparams['alpha'], epsilon=hyperparams['epsilon'])
                    result = model.train(num_episodes=hyperparams['num_episodes'])
                elif model_name == 'MonteCarloES':
                    model = ModelClass(env, gamma=hyperparams['gamma'])
                    result = model.train(num_episodes=hyperparams['num_episodes'])
                elif model_name in ['OnPolicyMC', 'OffPolicyMC']:
                    model = ModelClass(env, gamma=hyperparams['gamma'], epsilon=hyperparams['epsilon'])
                    result = model.train(num_episodes=hyperparams['num_episodes'])
                elif model_name in ['DynaQ', 'DynaQPlus']:
                    model = ModelClass(env, gamma=hyperparams['gamma'], alpha=hyperparams['alpha'], epsilon=hyperparams['epsilon'])
                    result = model.train(num_episodes=hyperparams['num_episodes'])
                else:
                    model = ModelClass(env)
                    result = model.train()
            except Exception as e:
                import traceback
                st.error(f"Erreur lors de l'entra√Ænement : {e}\n{traceback.format_exc()}")
                st.stop()
            except BaseException as e:
                st.error(f"Erreur inattendue (non-Exception) : {e} (type: {type(e)})")
                st.stop()
            
            # Affichage des r√©sultats d'entra√Ænement sans la cl√© 'history'
            st.success('Entra√Ænement termin√© !')
            train_display = dict(result) if result else {}
            train_display.pop('history', None)
            st.write("**R√©sultats entra√Ænement :**", train_display)
            
            # Affichage du score final si disponible
            score = None
            final_reward = None
            eval_metrics = {}
            if hasattr(model, 'evaluate'):
                st.subheader('√âvaluation du mod√®le')
                try:
                    eval_result = model.evaluate(num_episodes=100)
                    # Affichage des r√©sultats d'√©valuation sans la cl√© 'history'
                    eval_display = dict(eval_result) if eval_result else {}
                    eval_display.pop('history', None)
                    st.write("**R√©sultats √©valuation :**", eval_display)
                    st.write(eval_result)
                    # Recherche d'un score global dans le r√©sultat d'√©valuation
                    if isinstance(eval_result, dict):
                        score = eval_result.get('score')
                        if score is None:
                            for k in ['mean_reward', 'average_reward', 'reward', 'total_reward', 'avg_reward']:
                                if k in eval_result:
                                    score = eval_result[k]
                                    break
                        # Recherche du reward final explicite
                        for k in ['total_reward', 'reward', 'final_reward']:
                            if k in eval_result:
                                final_reward = eval_result[k]
                                break
                        # Affichage visuel agr√©able des m√©triques avanc√©es
                        cols = st.columns(3)
                        with cols[0]:
                            if 'avg_reward' in eval_result:
                                st.metric('üéØ R√©compense moyenne', f"{eval_result['avg_reward']:.2f}")
                            if 'min_reward' in eval_result:
                                st.metric('‚¨áÔ∏è R√©compense min.', f"{eval_result['min_reward']:.2f}")
                            if 'max_reward' in eval_result:
                                st.metric('‚¨ÜÔ∏è R√©compense max.', f"{eval_result['max_reward']:.2f}")
                        with cols[1]:
                            if 'iterations' in eval_result:
                                st.metric('üîÅ It√©rations/√©pisodes', eval_result['iterations'])
                            if 'converged' in eval_result:
                                st.metric('‚úÖ Convergence', str(eval_result['converged']))
                        with cols[2]:
                            if 'execution_time' in eval_result:
                                st.metric('‚è±Ô∏è Temps total (s)', f"{eval_result['execution_time']:.2f}")
                            if score is not None:
                                st.metric('üèÜ Score global', score)
                            if final_reward is not None:
                                st.metric('‚≠ê Reward final', final_reward)
                        # Courbe d'apprentissage
                        if 'learning_curve' in eval_result and eval_result['learning_curve']:
                            st.markdown('**Courbe d‚Äôapprentissage (valeur moyenne par it√©ration)**')
                            st.line_chart(eval_result['learning_curve'], height=250, use_container_width=True)
                        eval_metrics = {
                            'avg_reward': eval_result.get('avg_reward'),
                            'min_reward': eval_result.get('min_reward'),
                            'max_reward': eval_result.get('max_reward'),
                            'iterations': eval_result.get('iterations'),
                            'execution_time': eval_result.get('execution_time'),
                            'learning_curve': eval_result.get('learning_curve'),
                        }
                except Exception as e:
                    st.warning(f"Erreur lors de l'√©valuation : {e}")
            # Affichage d'autres m√©triques pertinentes si disponibles
            if hasattr(model, 'total_reward'):
                st.metric('R√©compense totale', getattr(model, 'total_reward'))
            if hasattr(model, 'steps_count'):
                st.metric('Nombre d\'√©tapes', getattr(model, 'steps_count'))
            if hasattr(model, 'wins'):
                st.metric('Victoires', getattr(model, 'wins'))
            if hasattr(model, 'total_games'):
                st.metric('Nombre de parties', getattr(model, 'total_games'))
            # Sauvegarde du mod√®le entra√Æn√© au format pkl
            model_path = save_trained_model(model, env_name, model_name, hyperparams)
            st.success(f"Mod√®le sauvegard√© dans : {model_path}")
            # Sauvegarde du test dans l'historique
            save_test_result({
                'datetime': datetime.now().isoformat(),
                'environnement': env_name,
                'modele': model_name,
                'hyperparametres': hyperparams,
                'resultats_train': result,
                'resultats_eval': eval_result if 'eval_result' in locals() else None,
                'score': score,
                'reward_final': final_reward,
                'eval_metrics': eval_metrics,
                'model_path': model_path
            })

    st.caption('Projet RL ESGI 2025 - Interface Streamlit')

# Onglet secondaire : Historique des tests sauvegard√©s
with tabs[1]:
    st.title('Historique des tests')
    history = load_test_history()
    if not history:
        st.info("Aucun test sauvegard√© pour le moment.")
    else:
        for test in reversed(history):
            with st.expander(f"{test['datetime']} | {test['environnement']} | {test['modele']}"):
                # Affichage des hyperparam√®tres
                st.write("**Hyperparam√®tres :**", test['hyperparametres'])
                # Affichage des r√©sultats d'entra√Ænement sans la cl√© 'history'
                train_display = dict(test['resultats_train']) if test['resultats_train'] else {}
                train_display.pop('history', None)
                st.write("**R√©sultats entra√Ænement :**", train_display)
                # Affichage des r√©sultats d'√©valuation sans la cl√© 'history'
                eval_display = dict(test['resultats_eval']) if test['resultats_eval'] else {}
                eval_display.pop('history', None)
                st.write("**R√©sultats √©valuation :**", eval_display)
                st.write("**Score :**", test['score'])
                st.write("**Reward final :**", test.get('reward_final', None))
                if 'eval_metrics' in test:
                    st.markdown('---')
                    st.markdown('**R√©sum√© des m√©triques d‚Äô√©valuation :**')
                    cols = st.columns(3)
                    with cols[0]:
                        if test['eval_metrics'].get('avg_reward') is not None:
                            st.metric('üéØ R√©compense moyenne', f"{test['eval_metrics']['avg_reward']:.2f}")
                        if test['eval_metrics'].get('min_reward') is not None:
                            st.metric('‚¨áÔ∏è R√©compense min.', f"{test['eval_metrics']['min_reward']:.2f}")
                        if test['eval_metrics'].get('max_reward') is not None:
                            st.metric('‚¨ÜÔ∏è R√©compense max.', f"{test['eval_metrics']['max_reward']:.2f}")
                    with cols[1]:
                        if test['eval_metrics'].get('iterations') is not None:
                            st.metric('üîÅ It√©rations/√©pisodes', test['eval_metrics']['iterations'])
                    with cols[2]:
                        if test['eval_metrics'].get('execution_time') is not None:
                            st.metric('‚è±Ô∏è Temps total (s)', f"{test['eval_metrics']['execution_time']:.2f}")
                    if test['eval_metrics'].get('learning_curve'):
                        st.markdown('**Courbe d‚Äôapprentissage (valeur moyenne par it√©ration)**')
                        st.line_chart(test['eval_metrics']['learning_curve'], height=250, use_container_width=True) 

# Onglet suppl√©mentaire : Ex√©cuter un mod√®le sauvegard√©
with st.tabs(["Ex√©cution", "Historique des tests", "Ex√©cuter un mod√®le sauvegard√©"])[2]:
    st.title("Ex√©cuter un mod√®le sauvegard√© sur GridWorld")
    # Lister les mod√®les disponibles
    model_files = glob.glob(os.path.join(os.path.dirname(__file__), '../models/*.pkl'))
    model_files = [f for f in model_files if os.path.isfile(f) and f.endswith('.pkl')]
    if not model_files:
        st.warning("Aucun mod√®le .pkl trouv√© dans le dossier models/.")
    else:
        model_choice = st.selectbox("Choisissez un mod√®le √† charger", model_files, format_func=lambda x: os.path.basename(x))
        if st.button("Charger et ex√©cuter sur GridWorld"):
            import pickle
            from game import environments
            # Charger le mod√®le
            with open(model_choice, 'rb') as f:
                model = pickle.load(f)
            # Cr√©er un nouvel environnement GridWorld (par d√©faut 5x5)
            env = environments.GridWorld()
            state = env.reset()
            done = False
            steps = 0
            max_steps = 100
            st.info("Ex√©cution automatique du mod√®le sur GridWorld...")
            table_placeholder = st.empty()
            info_placeholder = st.empty()
            while not done and steps < max_steps:
                action = model.policy[state]
                next_state, reward, done, info = env.step(action)
                # Construction de la grille
                grid = []
                for r in range(env.n_rows):
                    row = []
                    for c in range(env.n_cols):
                        idx = r * env.n_cols + c
                        if idx == next_state:
                            row.append('üü•')  # Agent
                        elif idx in env.terminals:
                            row.append('üèÅ')  # Terminal
                        else:
                            row.append('‚¨ú')
                    grid.append(row)
                table_placeholder.table(grid)
                info_placeholder.markdown(f"**√âtape {steps}** | Reward: {reward} | Fini: {done}")
                time.sleep(2)
                state = next_state
                steps += 1
            st.success(f"Ex√©cution termin√©e en {steps} √©tapes. Reward total: {env.total_reward}") 