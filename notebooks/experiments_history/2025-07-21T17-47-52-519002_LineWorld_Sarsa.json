{
  "datetime": "2025-07-21T17:47:52.519002",
  "environnement": "LineWorld",
  "modele": "Sarsa",
  "hyperparametres": {
    "gamma": 0.99,
    "alpha": 0.1,
    "epsilon": 0.1,
    "num_episodes": 100
  },
  "resultats_train": {
    "Q": [
      [
        0.0,
        0.0
      ],
      [
        0.0,
        0.1
      ],
      [
        -0.1,
        18.161280760595165
      ],
      [
        3.047365361469453,
        47.389519422289816
      ],
      [
        45.28375831355534,
        48.60776602101175
      ],
      [
        48.10611782587371,
        45.84050495246709
      ],
      [
        45.97989260806321,
        0.9986899794913624
      ],
      [
        0.0,
        0.0
      ]
    ],
    "policy": [
      0,
      1,
      1,
      1,
      1,
      0,
      0,
      0
    ],
    "history": [
      {
        "episode": 1,
        "reward": 24.0,
        "loss": 0.36292245403589735,
        "steps": 55,
        "epsilon": 0.09910000000000001
      },
      {
        "episode": 2,
        "reward": 7.0,
        "loss": 0.602725875519572,
        "steps": 19,
        "epsilon": 0.09820000000000001
      },
      {
        "episode": 3,
        "reward": 24.0,
        "loss": 0.3115954490649468,
        "steps": 45,
        "epsilon": 0.09730000000000001
      },
      {
        "episode": 4,
        "reward": 4.0,
        "loss": 0.6068393782310684,
        "steps": 5,
        "epsilon": 0.09640000000000001
      },
      {
        "episode": 5,
        "reward": 24.0,
        "loss": 0.3566538983487875,
        "steps": 45,
        "epsilon": 0.09550000000000002
      },
      {
        "episode": 6,
        "reward": 5.0,
        "loss": 0.8189133574738782,
        "steps": 7,
        "epsilon": 0.09460000000000002
      },
      {
        "episode": 7,
        "reward": 32.0,
        "loss": 0.5235965379390274,
        "steps": 65,
        "epsilon": 0.09370000000000002
      },
      {
        "episode": 8,
        "reward": 23.0,
        "loss": 1.335628567490826,
        "steps": 51,
        "epsilon": 0.09280000000000002
      },
      {
        "episode": 9,
        "reward": 7.0,
        "loss": 1.4644764162311106,
        "steps": 11,
        "epsilon": 0.09190000000000002
      },
      {
        "episode": 10,
        "reward": 29.0,
        "loss": 1.5215112380544829,
        "steps": 59,
        "epsilon": 0.09100000000000003
      },
      {
        "episode": 11,
        "reward": 13.0,
        "loss": 1.147810523901984,
        "steps": 23,
        "epsilon": 0.09010000000000003
      },
      {
        "episode": 12,
        "reward": 80.0,
        "loss": 3.2484530842988524,
        "steps": 183,
        "epsilon": 0.08920000000000003
      },
      {
        "episode": 13,
        "reward": 30.0,
        "loss": 1.860278245634878,
        "steps": 61,
        "epsilon": 0.08830000000000003
      },
      {
        "episode": 14,
        "reward": 3.0,
        "loss": 9.552375068361924,
        "steps": 3,
        "epsilon": 0.08740000000000003
      },
      {
        "episode": 15,
        "reward": 20.0,
        "loss": 4.064873392493264,
        "steps": 53,
        "epsilon": 0.08650000000000004
      },
      {
        "episode": 16,
        "reward": 11.0,
        "loss": 1.6916340367381792,
        "steps": 19,
        "epsilon": 0.08560000000000004
      },
      {
        "episode": 17,
        "reward": 55.0,
        "loss": 1.1662371048964042,
        "steps": 111,
        "epsilon": 0.08470000000000004
      },
      {
        "episode": 18,
        "reward": 14.0,
        "loss": 2.263895284880481,
        "steps": 25,
        "epsilon": 0.08380000000000004
      },
      {
        "episode": 19,
        "reward": 25.0,
        "loss": 3.153050616277529,
        "steps": 51,
        "epsilon": 0.08290000000000004
      },
      {
        "episode": 20,
        "reward": 19.0,
        "loss": 3.352194419272431,
        "steps": 39,
        "epsilon": 0.08200000000000005
      },
      {
        "episode": 21,
        "reward": 38.0,
        "loss": 1.148246990903114,
        "steps": 73,
        "epsilon": 0.08110000000000005
      },
      {
        "episode": 22,
        "reward": 19.0,
        "loss": 6.868063890132183,
        "steps": 45,
        "epsilon": 0.08020000000000005
      },
      {
        "episode": 23,
        "reward": 68.0,
        "loss": 2.206470753314231,
        "steps": 141,
        "epsilon": 0.07930000000000005
      },
      {
        "episode": 24,
        "reward": 3.0,
        "loss": 23.04487451142114,
        "steps": 3,
        "epsilon": 0.07840000000000005
      },
      {
        "episode": 25,
        "reward": 14.0,
        "loss": 5.448349409114825,
        "steps": 33,
        "epsilon": 0.07750000000000005
      },
      {
        "episode": 26,
        "reward": 4.0,
        "loss": 12.144993534272679,
        "steps": 5,
        "epsilon": 0.07660000000000006
      },
      {
        "episode": 27,
        "reward": 38.0,
        "loss": 2.440156306266154,
        "steps": 77,
        "epsilon": 0.07570000000000006
      },
      {
        "episode": 28,
        "reward": 28.0,
        "loss": 3.2598797896491365,
        "steps": 61,
        "epsilon": 0.07480000000000006
      },
      {
        "episode": 29,
        "reward": 33.0,
        "loss": 2.192545305770751,
        "steps": 67,
        "epsilon": 0.07390000000000006
      },
      {
        "episode": 30,
        "reward": 7.0,
        "loss": 7.33883974079574,
        "steps": 11,
        "epsilon": 0.07300000000000006
      },
      {
        "episode": 31,
        "reward": 54.0,
        "loss": 2.1465213899534636,
        "steps": 113,
        "epsilon": 0.07210000000000007
      },
      {
        "episode": 32,
        "reward": 26.0,
        "loss": 2.4255159934607766,
        "steps": 49,
        "epsilon": 0.07120000000000007
      },
      {
        "episode": 33,
        "reward": 50.0,
        "loss": 2.90345586250128,
        "steps": 109,
        "epsilon": 0.07030000000000007
      },
      {
        "episode": 34,
        "reward": 15.0,
        "loss": 5.34970268742796,
        "steps": 33,
        "epsilon": 0.06940000000000007
      },
      {
        "episode": 35,
        "reward": 104.0,
        "loss": 2.126849328510372,
        "steps": 237,
        "epsilon": 0.06850000000000007
      },
      {
        "episode": 36,
        "reward": 24.0,
        "loss": 3.966585160420471,
        "steps": 49,
        "epsilon": 0.06760000000000008
      },
      {
        "episode": 37,
        "reward": 92.0,
        "loss": 1.8373912897631517,
        "steps": 197,
        "epsilon": 0.06670000000000008
      },
      {
        "episode": 38,
        "reward": 90.0,
        "loss": 1.935923766577016,
        "steps": 185,
        "epsilon": 0.06580000000000008
      },
      {
        "episode": 39,
        "reward": 6.0,
        "loss": 24.002322695583814,
        "steps": 9,
        "epsilon": 0.06490000000000008
      },
      {
        "episode": 40,
        "reward": 31.0,
        "loss": 5.512399309267284,
        "steps": 71,
        "epsilon": 0.06400000000000008
      },
      {
        "episode": 41,
        "reward": 3.0,
        "loss": 54.76357461092448,
        "steps": 3,
        "epsilon": 0.06310000000000009
      },
      {
        "episode": 42,
        "reward": 44.0,
        "loss": 2.9917816004469766,
        "steps": 85,
        "epsilon": 0.06220000000000009
      },
      {
        "episode": 43,
        "reward": 18.0,
        "loss": 6.544820498150432,
        "steps": 33,
        "epsilon": 0.06130000000000009
      },
      {
        "episode": 44,
        "reward": 68.0,
        "loss": 2.615049618410275,
        "steps": 141,
        "epsilon": 0.06040000000000009
      },
      {
        "episode": 45,
        "reward": 32.0,
        "loss": 4.525008082218083,
        "steps": 65,
        "epsilon": 0.059500000000000094
      },
      {
        "episode": 46,
        "reward": 7.0,
        "loss": 14.947255559661528,
        "steps": 15,
        "epsilon": 0.058600000000000096
      },
      {
        "episode": 47,
        "reward": 21.0,
        "loss": 6.051682404035599,
        "steps": 39,
        "epsilon": 0.0577000000000001
      },
      {
        "episode": 48,
        "reward": 9.0,
        "loss": 11.170770487341569,
        "steps": 19,
        "epsilon": 0.0568000000000001
      },
      {
        "episode": 49,
        "reward": 84.0,
        "loss": 2.1228805384725558,
        "steps": 181,
        "epsilon": 0.0559000000000001
      },
      {
        "episode": 50,
        "reward": 34.0,
        "loss": 4.080209858783473,
        "steps": 65,
        "epsilon": 0.055000000000000104
      },
      {
        "episode": 51,
        "reward": 43.0,
        "loss": 3.6650970094404984,
        "steps": 87,
        "epsilon": 0.054100000000000106
      },
      {
        "episode": 52,
        "reward": 14.0,
        "loss": 10.266739151228965,
        "steps": 25,
        "epsilon": 0.05320000000000011
      },
      {
        "episode": 53,
        "reward": 7.0,
        "loss": 20.64905932734672,
        "steps": 11,
        "epsilon": 0.05230000000000011
      },
      {
        "episode": 54,
        "reward": 10.0,
        "loss": 12.095320241855852,
        "steps": 21,
        "epsilon": 0.05140000000000011
      },
      {
        "episode": 55,
        "reward": 74.0,
        "loss": 2.1418966190046067,
        "steps": 169,
        "epsilon": 0.050500000000000114
      },
      {
        "episode": 56,
        "reward": 23.0,
        "loss": 5.6335372065014955,
        "steps": 43,
        "epsilon": 0.049600000000000116
      },
      {
        "episode": 57,
        "reward": 16.0,
        "loss": 8.00542356638452,
        "steps": 29,
        "epsilon": 0.04870000000000012
      },
      {
        "episode": 58,
        "reward": 12.0,
        "loss": 9.10816955195602,
        "steps": 25,
        "epsilon": 0.04780000000000012
      },
      {
        "episode": 59,
        "reward": 461.0,
        "loss": 4.806410070590687,
        "steps": 1000,
        "epsilon": 0.04690000000000012
      },
      {
        "episode": 60,
        "reward": 453.0,
        "loss": 2.1107680332085317,
        "steps": 1000,
        "epsilon": 0.046000000000000124
      },
      {
        "episode": 61,
        "reward": 466.0,
        "loss": 1.5092497354714365,
        "steps": 1000,
        "epsilon": 0.045100000000000126
      },
      {
        "episode": 62,
        "reward": 234.0,
        "loss": 3.5028088699270143,
        "steps": 489,
        "epsilon": 0.04420000000000013
      },
      {
        "episode": 63,
        "reward": 479.0,
        "loss": 2.2313656295221946,
        "steps": 1000,
        "epsilon": 0.04330000000000013
      },
      {
        "episode": 64,
        "reward": 466.0,
        "loss": 1.477871339255901,
        "steps": 1000,
        "epsilon": 0.04240000000000013
      },
      {
        "episode": 65,
        "reward": 468.0,
        "loss": 0.8398841159488402,
        "steps": 1000,
        "epsilon": 0.041500000000000134
      },
      {
        "episode": 66,
        "reward": 469.0,
        "loss": 4.728899103105297,
        "steps": 1000,
        "epsilon": 0.040600000000000136
      },
      {
        "episode": 67,
        "reward": 400.0,
        "loss": 2.607574129273939,
        "steps": 825,
        "epsilon": 0.03970000000000014
      },
      {
        "episode": 68,
        "reward": 467.0,
        "loss": 1.1363753998180965,
        "steps": 1000,
        "epsilon": 0.03880000000000014
      },
      {
        "episode": 69,
        "reward": 476.0,
        "loss": 0.43853763919145966,
        "steps": 1000,
        "epsilon": 0.03790000000000014
      },
      {
        "episode": 70,
        "reward": 464.0,
        "loss": 0.5386648929130939,
        "steps": 1000,
        "epsilon": 0.037000000000000144
      },
      {
        "episode": 71,
        "reward": 465.0,
        "loss": 0.2655890787108054,
        "steps": 1000,
        "epsilon": 0.036100000000000146
      },
      {
        "episode": 72,
        "reward": 474.0,
        "loss": 0.2807673226731642,
        "steps": 1000,
        "epsilon": 0.03520000000000015
      },
      {
        "episode": 73,
        "reward": 473.0,
        "loss": 0.21866373448543203,
        "steps": 1000,
        "epsilon": 0.03430000000000015
      },
      {
        "episode": 74,
        "reward": 476.0,
        "loss": 0.22926775941175223,
        "steps": 1000,
        "epsilon": 0.03340000000000015
      },
      {
        "episode": 75,
        "reward": 322.0,
        "loss": 2.486258968383693,
        "steps": 673,
        "epsilon": 0.032500000000000154
      },
      {
        "episode": 76,
        "reward": 465.0,
        "loss": 0.40619400126827143,
        "steps": 1000,
        "epsilon": 0.031600000000000156
      },
      {
        "episode": 77,
        "reward": 487.0,
        "loss": 0.10258457390047152,
        "steps": 1000,
        "epsilon": 0.030700000000000154
      },
      {
        "episode": 78,
        "reward": 475.0,
        "loss": 0.18836986905394923,
        "steps": 1000,
        "epsilon": 0.029800000000000153
      },
      {
        "episode": 79,
        "reward": 144.0,
        "loss": 5.420082286732057,
        "steps": 305,
        "epsilon": 0.02890000000000015
      },
      {
        "episode": 80,
        "reward": 473.0,
        "loss": 0.4128313683725226,
        "steps": 1000,
        "epsilon": 0.02800000000000015
      },
      {
        "episode": 81,
        "reward": 474.0,
        "loss": 0.10065270338344132,
        "steps": 1000,
        "epsilon": 0.02710000000000015
      },
      {
        "episode": 82,
        "reward": 485.0,
        "loss": 0.10635136905925066,
        "steps": 1000,
        "epsilon": 0.026200000000000147
      },
      {
        "episode": 83,
        "reward": 477.0,
        "loss": 3.5198597163558936,
        "steps": 1000,
        "epsilon": 0.025300000000000145
      },
      {
        "episode": 84,
        "reward": 474.0,
        "loss": 0.4328319695127767,
        "steps": 1000,
        "epsilon": 0.024400000000000144
      },
      {
        "episode": 85,
        "reward": 489.0,
        "loss": 0.06299347779814933,
        "steps": 1000,
        "epsilon": 0.023500000000000142
      },
      {
        "episode": 86,
        "reward": 478.0,
        "loss": 3.521610934539402,
        "steps": 1000,
        "epsilon": 0.02260000000000014
      },
      {
        "episode": 87,
        "reward": 491.0,
        "loss": 0.10166783612477359,
        "steps": 1000,
        "epsilon": 0.02170000000000014
      },
      {
        "episode": 88,
        "reward": 474.0,
        "loss": 3.1682527815397536,
        "steps": 1000,
        "epsilon": 0.020800000000000138
      },
      {
        "episode": 89,
        "reward": 486.0,
        "loss": 0.34546210836920377,
        "steps": 1000,
        "epsilon": 0.019900000000000136
      },
      {
        "episode": 90,
        "reward": 134.0,
        "loss": 6.688933604506461,
        "steps": 289,
        "epsilon": 0.019000000000000135
      },
      {
        "episode": 91,
        "reward": 481.0,
        "loss": 0.21344053447791675,
        "steps": 1000,
        "epsilon": 0.018100000000000133
      },
      {
        "episode": 92,
        "reward": 485.0,
        "loss": 0.11391003911442756,
        "steps": 1000,
        "epsilon": 0.017200000000000132
      },
      {
        "episode": 93,
        "reward": 489.0,
        "loss": 0.09195314495619693,
        "steps": 1000,
        "epsilon": 0.01630000000000013
      },
      {
        "episode": 94,
        "reward": 492.0,
        "loss": 0.07315722823210033,
        "steps": 1000,
        "epsilon": 0.01540000000000013
      },
      {
        "episode": 95,
        "reward": 484.0,
        "loss": 0.12190681155396725,
        "steps": 1000,
        "epsilon": 0.01450000000000013
      },
      {
        "episode": 96,
        "reward": 482.0,
        "loss": 0.08988083946803692,
        "steps": 1000,
        "epsilon": 0.013600000000000131
      },
      {
        "episode": 97,
        "reward": 489.0,
        "loss": 0.05782740320396805,
        "steps": 1000,
        "epsilon": 0.012700000000000131
      },
      {
        "episode": 98,
        "reward": 498.0,
        "loss": 0.020055258441505904,
        "steps": 1000,
        "epsilon": 0.011800000000000132
      },
      {
        "episode": 99,
        "reward": 495.0,
        "loss": 0.044753038398609134,
        "steps": 1000,
        "epsilon": 0.010900000000000132
      },
      {
        "episode": 100,
        "reward": 490.0,
        "loss": 0.07118090439319863,
        "steps": 1000,
        "epsilon": 0.010000000000000132
      }
    ]
  },
  "resultats_eval": {
    "avg_reward": 500.0,
    "std_reward": 0.0,
    "success_rate": 1.0,
    "avg_steps": 1000.0,
    "learning_curve": [
      24.0,
      7.0,
      24.0,
      4.0,
      24.0,
      5.0,
      32.0,
      23.0,
      7.0,
      29.0,
      13.0,
      80.0,
      30.0,
      3.0,
      20.0,
      11.0,
      55.0,
      14.0,
      25.0,
      19.0,
      38.0,
      19.0,
      68.0,
      3.0,
      14.0,
      4.0,
      38.0,
      28.0,
      33.0,
      7.0,
      54.0,
      26.0,
      50.0,
      15.0,
      104.0,
      24.0,
      92.0,
      90.0,
      6.0,
      31.0,
      3.0,
      44.0,
      18.0,
      68.0,
      32.0,
      7.0,
      21.0,
      9.0,
      84.0,
      34.0,
      43.0,
      14.0,
      7.0,
      10.0,
      74.0,
      23.0,
      16.0,
      12.0,
      461.0,
      453.0,
      466.0,
      234.0,
      479.0,
      466.0,
      468.0,
      469.0,
      400.0,
      467.0,
      476.0,
      464.0,
      465.0,
      474.0,
      473.0,
      476.0,
      322.0,
      465.0,
      487.0,
      475.0,
      144.0,
      473.0,
      474.0,
      485.0,
      477.0,
      474.0,
      489.0,
      478.0,
      491.0,
      474.0,
      486.0,
      134.0,
      481.0,
      485.0,
      489.0,
      492.0,
      484.0,
      482.0,
      489.0,
      498.0,
      495.0,
      490.0
    ]
  },
  "score": 500.0,
  "reward_final": null,
  "eval_metrics": {
    "avg_reward": 500.0,
    "min_reward": null,
    "max_reward": null,
    "iterations": null,
    "execution_time": null,
    "learning_curve": [
      24.0,
      7.0,
      24.0,
      4.0,
      24.0,
      5.0,
      32.0,
      23.0,
      7.0,
      29.0,
      13.0,
      80.0,
      30.0,
      3.0,
      20.0,
      11.0,
      55.0,
      14.0,
      25.0,
      19.0,
      38.0,
      19.0,
      68.0,
      3.0,
      14.0,
      4.0,
      38.0,
      28.0,
      33.0,
      7.0,
      54.0,
      26.0,
      50.0,
      15.0,
      104.0,
      24.0,
      92.0,
      90.0,
      6.0,
      31.0,
      3.0,
      44.0,
      18.0,
      68.0,
      32.0,
      7.0,
      21.0,
      9.0,
      84.0,
      34.0,
      43.0,
      14.0,
      7.0,
      10.0,
      74.0,
      23.0,
      16.0,
      12.0,
      461.0,
      453.0,
      466.0,
      234.0,
      479.0,
      466.0,
      468.0,
      469.0,
      400.0,
      467.0,
      476.0,
      464.0,
      465.0,
      474.0,
      473.0,
      476.0,
      322.0,
      465.0,
      487.0,
      475.0,
      144.0,
      473.0,
      474.0,
      485.0,
      477.0,
      474.0,
      489.0,
      478.0,
      491.0,
      474.0,
      486.0,
      134.0,
      481.0,
      485.0,
      489.0,
      492.0,
      484.0,
      482.0,
      489.0,
      498.0,
      495.0,
      490.0
    ]
  }
}