{
  "datetime": "2025-07-21T17:48:58.561286",
  "environnement": "LineWorld",
  "modele": "QLearning",
  "hyperparametres": {
    "gamma": 0.99,
    "alpha": 0.1,
    "epsilon": 0.1,
    "num_episodes": 100
  },
  "resultats_train": {
    "Q": [
      [
        0.0,
        0.0
      ],
      [
        -0.1,
        0.0
      ],
      [
        -0.1,
        46.37550861561317
      ],
      [
        35.23263841667783,
        49.74874371858954
      ],
      [
        48.251256281403535,
        50.25125628140361
      ],
      [
        49.74874371858954,
        48.768743718589484
      ],
      [
        48.25125628140361,
        0.9749684449500675
      ],
      [
        0.0,
        0.0
      ]
    ],
    "policy": [
      0,
      1,
      1,
      1,
      1,
      0,
      0,
      0
    ],
    "history": [
      {
        "episode": 1,
        "reward": 80.0,
        "loss": 0.46210797841804413,
        "steps": 181,
        "epsilon": 0.09910000000000001
      },
      {
        "episode": 2,
        "reward": 4.0,
        "loss": 0.49345787174691635,
        "steps": 5,
        "epsilon": 0.09820000000000001
      },
      {
        "episode": 3,
        "reward": 3.0,
        "loss": 0.5970578066981921,
        "steps": 3,
        "epsilon": 0.09730000000000001
      },
      {
        "episode": 4,
        "reward": 22.0,
        "loss": 0.2519098243667321,
        "steps": 41,
        "epsilon": 0.09640000000000001
      },
      {
        "episode": 5,
        "reward": 10.0,
        "loss": 1.853919147565678,
        "steps": 29,
        "epsilon": 0.09550000000000002
      },
      {
        "episode": 6,
        "reward": 436.0,
        "loss": 2.710442395695825,
        "steps": 1000,
        "epsilon": 0.09460000000000002
      },
      {
        "episode": 7,
        "reward": 65.0,
        "loss": 3.997498519691331,
        "steps": 147,
        "epsilon": 0.09370000000000002
      },
      {
        "episode": 8,
        "reward": 356.0,
        "loss": 4.715634064266974,
        "steps": 843,
        "epsilon": 0.09280000000000002
      },
      {
        "episode": 9,
        "reward": 422.0,
        "loss": 3.36391720088012,
        "steps": 1000,
        "epsilon": 0.09190000000000002
      },
      {
        "episode": 10,
        "reward": 444.0,
        "loss": 1.2786574561708275,
        "steps": 1000,
        "epsilon": 0.09100000000000003
      },
      {
        "episode": 11,
        "reward": 102.0,
        "loss": 0.5481215795679889,
        "steps": 241,
        "epsilon": 0.09010000000000003
      },
      {
        "episode": 12,
        "reward": 10.0,
        "loss": 0.7036415690201022,
        "steps": 19,
        "epsilon": 0.08920000000000003
      },
      {
        "episode": 13,
        "reward": 5.0,
        "loss": 0.6024535872224942,
        "steps": 7,
        "epsilon": 0.08830000000000003
      },
      {
        "episode": 14,
        "reward": 437.0,
        "loss": 1.0798976224801855,
        "steps": 1000,
        "epsilon": 0.08740000000000003
      },
      {
        "episode": 15,
        "reward": 16.0,
        "loss": 0.151557850639236,
        "steps": 33,
        "epsilon": 0.08650000000000004
      },
      {
        "episode": 16,
        "reward": 429.0,
        "loss": 0.08162204527340428,
        "steps": 1000,
        "epsilon": 0.08560000000000004
      },
      {
        "episode": 17,
        "reward": 433.0,
        "loss": 1.9632242035634768,
        "steps": 1000,
        "epsilon": 0.08470000000000004
      },
      {
        "episode": 18,
        "reward": 384.0,
        "loss": 0.010877338316897262,
        "steps": 861,
        "epsilon": 0.08380000000000004
      },
      {
        "episode": 19,
        "reward": 178.0,
        "loss": 0.008461930526925334,
        "steps": 447,
        "epsilon": 0.08290000000000004
      },
      {
        "episode": 20,
        "reward": 288.0,
        "loss": 0.0025391590669101476,
        "steps": 651,
        "epsilon": 0.08200000000000005
      },
      {
        "episode": 21,
        "reward": 433.0,
        "loss": 1.6866726808803094,
        "steps": 1000,
        "epsilon": 0.08110000000000005
      },
      {
        "episode": 22,
        "reward": 341.0,
        "loss": 0.0007111493596026209,
        "steps": 755,
        "epsilon": 0.08020000000000005
      },
      {
        "episode": 23,
        "reward": 4.0,
        "loss": 0.01003656472096615,
        "steps": 5,
        "epsilon": 0.07930000000000005
      },
      {
        "episode": 24,
        "reward": 439.0,
        "loss": 0.732467188728005,
        "steps": 1000,
        "epsilon": 0.07840000000000005
      },
      {
        "episode": 25,
        "reward": 445.0,
        "loss": 0.00015574755273076876,
        "steps": 1000,
        "epsilon": 0.07750000000000005
      },
      {
        "episode": 26,
        "reward": 41.0,
        "loss": 0.000519969135963888,
        "steps": 91,
        "epsilon": 0.07660000000000006
      },
      {
        "episode": 27,
        "reward": 35.0,
        "loss": 0.0004792827273132504,
        "steps": 79,
        "epsilon": 0.07570000000000006
      },
      {
        "episode": 28,
        "reward": 242.0,
        "loss": 9.498643775344309e-05,
        "steps": 557,
        "epsilon": 0.07480000000000006
      },
      {
        "episode": 29,
        "reward": 87.0,
        "loss": 1.5663013607554297,
        "steps": 202,
        "epsilon": 0.07390000000000006
      },
      {
        "episode": 30,
        "reward": 460.0,
        "loss": 2.0443432502756672e-05,
        "steps": 1000,
        "epsilon": 0.07300000000000006
      },
      {
        "episode": 31,
        "reward": 376.0,
        "loss": 3.577531902829672e-05,
        "steps": 815,
        "epsilon": 0.07210000000000007
      },
      {
        "episode": 32,
        "reward": 446.0,
        "loss": 7.112214042042558e-06,
        "steps": 1000,
        "epsilon": 0.07120000000000007
      },
      {
        "episode": 33,
        "reward": 445.0,
        "loss": 1.6547055424361434,
        "steps": 1000,
        "epsilon": 0.07030000000000007
      },
      {
        "episode": 34,
        "reward": 20.0,
        "loss": 0.00039987345862807754,
        "steps": 37,
        "epsilon": 0.06940000000000007
      },
      {
        "episode": 35,
        "reward": 11.0,
        "loss": 0.0006307429328495158,
        "steps": 19,
        "epsilon": 0.06850000000000007
      },
      {
        "episode": 36,
        "reward": 448.0,
        "loss": 6.06377856775813e-07,
        "steps": 1000,
        "epsilon": 0.06760000000000008
      },
      {
        "episode": 37,
        "reward": 447.0,
        "loss": 2.2711722192596344e-07,
        "steps": 1000,
        "epsilon": 0.06670000000000008
      },
      {
        "episode": 38,
        "reward": 454.0,
        "loss": 8.481749736838366e-08,
        "steps": 1000,
        "epsilon": 0.06580000000000008
      },
      {
        "episode": 39,
        "reward": 464.0,
        "loss": 3.956654253091682e-08,
        "steps": 1000,
        "epsilon": 0.06490000000000008
      },
      {
        "episode": 40,
        "reward": 443.0,
        "loss": 2.3724682506834357e-08,
        "steps": 1000,
        "epsilon": 0.06400000000000008
      },
      {
        "episode": 41,
        "reward": 457.0,
        "loss": 5.941628112133251e-09,
        "steps": 1000,
        "epsilon": 0.06310000000000009
      },
      {
        "episode": 42,
        "reward": 153.0,
        "loss": 1.3826261958481143,
        "steps": 325,
        "epsilon": 0.06220000000000009
      },
      {
        "episode": 43,
        "reward": 133.0,
        "loss": 2.5928201528769946e-05,
        "steps": 303,
        "epsilon": 0.06130000000000009
      },
      {
        "episode": 44,
        "reward": 280.0,
        "loss": 1.0280680224065795e-05,
        "steps": 619,
        "epsilon": 0.06040000000000009
      },
      {
        "episode": 45,
        "reward": 314.0,
        "loss": 1.1177603249050565,
        "steps": 689,
        "epsilon": 0.059500000000000094
      },
      {
        "episode": 46,
        "reward": 458.0,
        "loss": 3.330118149875989e-10,
        "steps": 1000,
        "epsilon": 0.058600000000000096
      },
      {
        "episode": 47,
        "reward": 277.0,
        "loss": 6.788121040498534e-06,
        "steps": 615,
        "epsilon": 0.0577000000000001
      },
      {
        "episode": 48,
        "reward": 458.0,
        "loss": 0.32747079602853474,
        "steps": 1000,
        "epsilon": 0.0568000000000001
      },
      {
        "episode": 49,
        "reward": 358.0,
        "loss": 4.3518841346379565e-06,
        "steps": 777,
        "epsilon": 0.0559000000000001
      },
      {
        "episode": 50,
        "reward": 463.0,
        "loss": 1.3723661752913971e-11,
        "steps": 1000,
        "epsilon": 0.055000000000000104
      },
      {
        "episode": 51,
        "reward": 32.0,
        "loss": 3.651905326218928e-05,
        "steps": 75,
        "epsilon": 0.054100000000000106
      },
      {
        "episode": 52,
        "reward": 453.0,
        "loss": 0.5546975715172605,
        "steps": 1000,
        "epsilon": 0.05320000000000011
      },
      {
        "episode": 53,
        "reward": 467.0,
        "loss": 1.6870246287173623e-12,
        "steps": 1000,
        "epsilon": 0.05230000000000011
      },
      {
        "episode": 54,
        "reward": 57.0,
        "loss": 1.896180663422641e-05,
        "steps": 117,
        "epsilon": 0.05140000000000011
      },
      {
        "episode": 55,
        "reward": 466.0,
        "loss": 7.563587618804686e-13,
        "steps": 1000,
        "epsilon": 0.050500000000000114
      },
      {
        "episode": 56,
        "reward": 48.0,
        "loss": 1.8525880039471163e-05,
        "steps": 97,
        "epsilon": 0.049600000000000116
      },
      {
        "episode": 57,
        "reward": 466.0,
        "loss": 0.4391891879648134,
        "steps": 1000,
        "epsilon": 0.04870000000000012
      },
      {
        "episode": 58,
        "reward": 469.0,
        "loss": 1.4122241647186014e-13,
        "steps": 1000,
        "epsilon": 0.04780000000000012
      },
      {
        "episode": 59,
        "reward": 463.0,
        "loss": 6.660056539250104e-14,
        "steps": 1000,
        "epsilon": 0.04690000000000012
      },
      {
        "episode": 60,
        "reward": 465.0,
        "loss": 2.150467500535594e-14,
        "steps": 1000,
        "epsilon": 0.046000000000000124
      },
      {
        "episode": 61,
        "reward": 339.0,
        "loss": 2.007694274893158e-06,
        "steps": 725,
        "epsilon": 0.045100000000000126
      },
      {
        "episode": 62,
        "reward": 468.0,
        "loss": 3.2184490600018245e-15,
        "steps": 1000,
        "epsilon": 0.04420000000000013
      },
      {
        "episode": 63,
        "reward": 471.0,
        "loss": 0.18259854011992738,
        "steps": 1000,
        "epsilon": 0.04330000000000013
      },
      {
        "episode": 64,
        "reward": 204.0,
        "loss": 2.748294774063766e-06,
        "steps": 429,
        "epsilon": 0.04240000000000013
      },
      {
        "episode": 65,
        "reward": 482.0,
        "loss": 2.8612184864792984e-16,
        "steps": 1000,
        "epsilon": 0.041500000000000134
      },
      {
        "episode": 66,
        "reward": 472.0,
        "loss": 3.112280755413923e-16,
        "steps": 1000,
        "epsilon": 0.040600000000000136
      },
      {
        "episode": 67,
        "reward": 472.0,
        "loss": 1.1547282380760796e-16,
        "steps": 1000,
        "epsilon": 0.03970000000000014
      },
      {
        "episode": 68,
        "reward": 474.0,
        "loss": 4.2376898529157454e-17,
        "steps": 1000,
        "epsilon": 0.03880000000000014
      },
      {
        "episode": 69,
        "reward": 479.0,
        "loss": 1.3970122537824165e-17,
        "steps": 1000,
        "epsilon": 0.03790000000000014
      },
      {
        "episode": 70,
        "reward": 473.0,
        "loss": 0.30267669540727016,
        "steps": 1000,
        "epsilon": 0.037000000000000144
      },
      {
        "episode": 71,
        "reward": 463.0,
        "loss": 3.62070274254947e-18,
        "steps": 1000,
        "epsilon": 0.036100000000000146
      },
      {
        "episode": 72,
        "reward": 468.0,
        "loss": 8.065279964396929e-19,
        "steps": 1000,
        "epsilon": 0.03520000000000015
      },
      {
        "episode": 73,
        "reward": 470.0,
        "loss": 2.350869310046493e-19,
        "steps": 1000,
        "epsilon": 0.03430000000000015
      },
      {
        "episode": 74,
        "reward": 464.0,
        "loss": 7.884622343519971e-20,
        "steps": 1000,
        "epsilon": 0.03340000000000015
      },
      {
        "episode": 75,
        "reward": 479.0,
        "loss": 2.0259551125485347e-20,
        "steps": 1000,
        "epsilon": 0.032500000000000154
      },
      {
        "episode": 76,
        "reward": 480.0,
        "loss": 1.0907883672455565e-20,
        "steps": 1000,
        "epsilon": 0.031600000000000156
      },
      {
        "episode": 77,
        "reward": 468.0,
        "loss": 5.456968263402044e-21,
        "steps": 1000,
        "epsilon": 0.030700000000000154
      },
      {
        "episode": 78,
        "reward": 476.0,
        "loss": 2.7788203778720977e-21,
        "steps": 1000,
        "epsilon": 0.029800000000000153
      },
      {
        "episode": 79,
        "reward": 488.0,
        "loss": 4.390823348671854e-22,
        "steps": 1000,
        "epsilon": 0.02890000000000015
      },
      {
        "episode": 80,
        "reward": 480.0,
        "loss": 3.7612272694870157e-22,
        "steps": 1000,
        "epsilon": 0.02800000000000015
      },
      {
        "episode": 81,
        "reward": 479.0,
        "loss": 1.8435453980609867e-22,
        "steps": 1000,
        "epsilon": 0.02710000000000015
      },
      {
        "episode": 82,
        "reward": 489.0,
        "loss": 6.525679748118802e-23,
        "steps": 1000,
        "epsilon": 0.026200000000000147
      },
      {
        "episode": 83,
        "reward": 484.0,
        "loss": 4.5621298554655264e-23,
        "steps": 1000,
        "epsilon": 0.025300000000000145
      },
      {
        "episode": 84,
        "reward": 473.0,
        "loss": 3.3026431775808255e-23,
        "steps": 1000,
        "epsilon": 0.024400000000000144
      },
      {
        "episode": 85,
        "reward": 481.0,
        "loss": 7.745302939336986e-24,
        "steps": 1000,
        "epsilon": 0.023500000000000142
      },
      {
        "episode": 86,
        "reward": 487.0,
        "loss": 2.402154460741594e-24,
        "steps": 1000,
        "epsilon": 0.02260000000000014
      },
      {
        "episode": 87,
        "reward": 483.0,
        "loss": 1.4219379627757618e-24,
        "steps": 1000,
        "epsilon": 0.02170000000000014
      },
      {
        "episode": 88,
        "reward": 485.0,
        "loss": 5.638088096762012e-25,
        "steps": 1000,
        "epsilon": 0.020800000000000138
      },
      {
        "episode": 89,
        "reward": 487.0,
        "loss": 2.1394049113468057e-25,
        "steps": 1000,
        "epsilon": 0.019900000000000136
      },
      {
        "episode": 90,
        "reward": 485.0,
        "loss": 1.047324049514093e-25,
        "steps": 1000,
        "epsilon": 0.019000000000000135
      },
      {
        "episode": 91,
        "reward": 491.0,
        "loss": 3.512654984897502e-26,
        "steps": 1000,
        "epsilon": 0.018100000000000133
      },
      {
        "episode": 92,
        "reward": 485.0,
        "loss": 3.0796069510771686e-26,
        "steps": 1000,
        "epsilon": 0.017200000000000132
      },
      {
        "episode": 93,
        "reward": 461.0,
        "loss": 1.0235851562666917e-06,
        "steps": 933,
        "epsilon": 0.01630000000000013
      },
      {
        "episode": 94,
        "reward": 490.0,
        "loss": 6.559839121681363e-27,
        "steps": 1000,
        "epsilon": 0.01540000000000013
      },
      {
        "episode": 95,
        "reward": 492.0,
        "loss": 3.4449871275363674e-27,
        "steps": 1000,
        "epsilon": 0.01450000000000013
      },
      {
        "episode": 96,
        "reward": 488.0,
        "loss": 2.9567264034152535e-27,
        "steps": 1000,
        "epsilon": 0.013600000000000131
      },
      {
        "episode": 97,
        "reward": 485.0,
        "loss": 2.097234048184373e-27,
        "steps": 1000,
        "epsilon": 0.012700000000000131
      },
      {
        "episode": 98,
        "reward": 137.0,
        "loss": 2.792613755037651e-06,
        "steps": 277,
        "epsilon": 0.011800000000000132
      },
      {
        "episode": 99,
        "reward": 490.0,
        "loss": 0.1242412479009567,
        "steps": 1000,
        "epsilon": 0.010900000000000132
      },
      {
        "episode": 100,
        "reward": 488.0,
        "loss": 1.1625664041295513e-27,
        "steps": 1000,
        "epsilon": 0.010000000000000132
      }
    ]
  },
  "resultats_eval": {
    "avg_reward": 500.0,
    "std_reward": 0.0,
    "avg_steps": 1000.0,
    "learning_curve": [
      80.0,
      4.0,
      3.0,
      22.0,
      10.0,
      436.0,
      65.0,
      356.0,
      422.0,
      444.0,
      102.0,
      10.0,
      5.0,
      437.0,
      16.0,
      429.0,
      433.0,
      384.0,
      178.0,
      288.0,
      433.0,
      341.0,
      4.0,
      439.0,
      445.0,
      41.0,
      35.0,
      242.0,
      87.0,
      460.0,
      376.0,
      446.0,
      445.0,
      20.0,
      11.0,
      448.0,
      447.0,
      454.0,
      464.0,
      443.0,
      457.0,
      153.0,
      133.0,
      280.0,
      314.0,
      458.0,
      277.0,
      458.0,
      358.0,
      463.0,
      32.0,
      453.0,
      467.0,
      57.0,
      466.0,
      48.0,
      466.0,
      469.0,
      463.0,
      465.0,
      339.0,
      468.0,
      471.0,
      204.0,
      482.0,
      472.0,
      472.0,
      474.0,
      479.0,
      473.0,
      463.0,
      468.0,
      470.0,
      464.0,
      479.0,
      480.0,
      468.0,
      476.0,
      488.0,
      480.0,
      479.0,
      489.0,
      484.0,
      473.0,
      481.0,
      487.0,
      483.0,
      485.0,
      487.0,
      485.0,
      491.0,
      485.0,
      461.0,
      490.0,
      492.0,
      488.0,
      485.0,
      137.0,
      490.0,
      488.0
    ]
  },
  "score": 500.0,
  "reward_final": null,
  "eval_metrics": {
    "avg_reward": 500.0,
    "min_reward": null,
    "max_reward": null,
    "iterations": null,
    "execution_time": null,
    "learning_curve": [
      80.0,
      4.0,
      3.0,
      22.0,
      10.0,
      436.0,
      65.0,
      356.0,
      422.0,
      444.0,
      102.0,
      10.0,
      5.0,
      437.0,
      16.0,
      429.0,
      433.0,
      384.0,
      178.0,
      288.0,
      433.0,
      341.0,
      4.0,
      439.0,
      445.0,
      41.0,
      35.0,
      242.0,
      87.0,
      460.0,
      376.0,
      446.0,
      445.0,
      20.0,
      11.0,
      448.0,
      447.0,
      454.0,
      464.0,
      443.0,
      457.0,
      153.0,
      133.0,
      280.0,
      314.0,
      458.0,
      277.0,
      458.0,
      358.0,
      463.0,
      32.0,
      453.0,
      467.0,
      57.0,
      466.0,
      48.0,
      466.0,
      469.0,
      463.0,
      465.0,
      339.0,
      468.0,
      471.0,
      204.0,
      482.0,
      472.0,
      472.0,
      474.0,
      479.0,
      473.0,
      463.0,
      468.0,
      470.0,
      464.0,
      479.0,
      480.0,
      468.0,
      476.0,
      488.0,
      480.0,
      479.0,
      489.0,
      484.0,
      473.0,
      481.0,
      487.0,
      483.0,
      485.0,
      487.0,
      485.0,
      491.0,
      485.0,
      461.0,
      490.0,
      492.0,
      488.0,
      485.0,
      137.0,
      490.0,
      488.0
    ]
  }
}