{
  "datetime": "2025-07-21T17:06:50.986445",
  "environnement": "LineWorld",
  "modele": "Policy Iteration",
  "hyperparametres": {},
  "resultats_train": {
    "policy": [
      0,
      0,
      0,
      0,
      1,
      1,
      1,
      1
    ],
    "V": [
      9.999991791689899,
      9.99999261252091,
      8.999993351268818,
      8.099994016141936,
      8.0999917916899,
      8.999991791689899,
      9.999991791689899,
      9.999991791689899
    ],
    "history": [
      {
        "iteration": 1,
        "avg_value": 3.6249989739594284,
        "max_value": 9.999991791689899,
        "policy_stable": false
      },
      {
        "iteration": 2,
        "avg_value": 9.17374208924114,
        "max_value": 9.99999261252091,
        "policy_stable": false
      },
      {
        "iteration": 3,
        "avg_value": 9.274992367297646,
        "max_value": 9.99999261252091,
        "policy_stable": true
      }
    ],
    "iterations": 3,
    "converged": true
  },
  "resultats_eval": {
    "avg_reward": 3.0,
    "min_reward": 3.0,
    "max_reward": 3.0,
    "iterations": 3,
    "converged": true,
    "execution_time": 0.0002295970916748047,
    "learning_curve": [
      3.6249989739594284,
      9.17374208924114,
      9.274992367297646
    ],
    "rewards": [
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0
    ],
    "steps_per_episode": [
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3
    ]
  },
  "score": 3.0,
  "reward_final": null,
  "eval_metrics": {
    "avg_reward": 3.0,
    "min_reward": 3.0,
    "max_reward": 3.0,
    "iterations": 3,
    "execution_time": 0.0002295970916748047,
    "learning_curve": [
      3.6249989739594284,
      9.17374208924114,
      9.274992367297646
    ]
  }
}