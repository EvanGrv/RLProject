{
  "datetime": "2025-07-21T17:14:15.569366",
  "environnement": "LineWorld",
  "modele": "QLearning",
  "hyperparametres": {
    "gamma": 0.98,
    "alpha": 0.1,
    "epsilon": 0.1,
    "num_episodes": 100
  },
  "resultats_train": {
    "Q": [
      [
        0.0,
        0.0
      ],
      [
        -0.1,
        2.5391622785605104
      ],
      [
        -0.20287932988165283,
        24.588095577189627
      ],
      [
        20.638823964081972,
        24.74747474747376
      ],
      [
        23.252525252524265,
        25.252525252524265
      ],
      [
        24.74747474747376,
        23.787474747473766
      ],
      [
        23.252525252524265,
        0.9690968456173674
      ],
      [
        0.0,
        0.0
      ]
    ],
    "policy": [
      0,
      1,
      1,
      1,
      1,
      0,
      0,
      0
    ],
    "history": [
      {
        "episode": 1,
        "reward": 88.0,
        "loss": 0.4820879969891345,
        "steps": 213,
        "epsilon": 0.09910000000000001
      },
      {
        "episode": 2,
        "reward": 428.0,
        "loss": 2.2064853933119566,
        "steps": 1000,
        "epsilon": 0.09820000000000001
      },
      {
        "episode": 3,
        "reward": 222.0,
        "loss": 1.706994951240717,
        "steps": 513,
        "epsilon": 0.09730000000000001
      },
      {
        "episode": 4,
        "reward": 42.0,
        "loss": 0.5817492206239283,
        "steps": 87,
        "epsilon": 0.09640000000000001
      },
      {
        "episode": 5,
        "reward": 5.0,
        "loss": 1.246191617980651,
        "steps": 7,
        "epsilon": 0.09550000000000002
      },
      {
        "episode": 6,
        "reward": 199.0,
        "loss": 0.7048214020088657,
        "steps": 477,
        "epsilon": 0.09460000000000002
      },
      {
        "episode": 7,
        "reward": 21.0,
        "loss": 0.38773185697884754,
        "steps": 45,
        "epsilon": 0.09370000000000002
      },
      {
        "episode": 8,
        "reward": 162.0,
        "loss": 0.17964363419226095,
        "steps": 370,
        "epsilon": 0.09280000000000002
      },
      {
        "episode": 9,
        "reward": 429.0,
        "loss": 0.13876195305512745,
        "steps": 1000,
        "epsilon": 0.09190000000000002
      },
      {
        "episode": 10,
        "reward": 52.0,
        "loss": 0.037962191764341804,
        "steps": 113,
        "epsilon": 0.09100000000000003
      },
      {
        "episode": 11,
        "reward": 441.0,
        "loss": 0.4550952944980073,
        "steps": 1000,
        "epsilon": 0.09010000000000003
      },
      {
        "episode": 12,
        "reward": 381.0,
        "loss": 0.8684905183892354,
        "steps": 859,
        "epsilon": 0.08920000000000003
      },
      {
        "episode": 13,
        "reward": 453.0,
        "loss": 0.5409054826312323,
        "steps": 995,
        "epsilon": 0.08830000000000003
      },
      {
        "episode": 14,
        "reward": 316.0,
        "loss": 0.6038213872089848,
        "steps": 715,
        "epsilon": 0.08740000000000003
      },
      {
        "episode": 15,
        "reward": 48.0,
        "loss": 1.7575284164132756,
        "steps": 105,
        "epsilon": 0.08650000000000004
      },
      {
        "episode": 16,
        "reward": 440.0,
        "loss": 0.16699358250065033,
        "steps": 1000,
        "epsilon": 0.08560000000000004
      },
      {
        "episode": 17,
        "reward": 433.0,
        "loss": 4.359543778016856e-06,
        "steps": 1000,
        "epsilon": 0.08470000000000004
      },
      {
        "episode": 18,
        "reward": 426.0,
        "loss": 0.28738160232628,
        "steps": 1000,
        "epsilon": 0.08380000000000004
      },
      {
        "episode": 19,
        "reward": 435.0,
        "loss": 0.4787824552280854,
        "steps": 967,
        "epsilon": 0.08290000000000004
      },
      {
        "episode": 20,
        "reward": 439.0,
        "loss": 0.11038547522444615,
        "steps": 1000,
        "epsilon": 0.08200000000000005
      },
      {
        "episode": 21,
        "reward": 430.0,
        "loss": 0.2662366585911063,
        "steps": 1000,
        "epsilon": 0.08110000000000005
      },
      {
        "episode": 22,
        "reward": 450.0,
        "loss": 2.1188647985160242e-10,
        "steps": 1000,
        "epsilon": 0.08020000000000005
      },
      {
        "episode": 23,
        "reward": 93.0,
        "loss": 0.0003710067975768511,
        "steps": 215,
        "epsilon": 0.07930000000000005
      },
      {
        "episode": 24,
        "reward": 403.0,
        "loss": 7.030559226734545e-05,
        "steps": 919,
        "epsilon": 0.07840000000000005
      },
      {
        "episode": 25,
        "reward": 436.0,
        "loss": 0.13212385846253016,
        "steps": 1000,
        "epsilon": 0.07750000000000005
      },
      {
        "episode": 26,
        "reward": 230.0,
        "loss": 9.818905006645922e-05,
        "steps": 533,
        "epsilon": 0.07660000000000006
      },
      {
        "episode": 27,
        "reward": 185.0,
        "loss": 0.000107865542861682,
        "steps": 393,
        "epsilon": 0.07570000000000006
      },
      {
        "episode": 28,
        "reward": 40.0,
        "loss": 0.00040396280254063806,
        "steps": 85,
        "epsilon": 0.07480000000000006
      },
      {
        "episode": 29,
        "reward": 442.0,
        "loss": 8.858686862737039e-14,
        "steps": 1000,
        "epsilon": 0.07390000000000006
      },
      {
        "episode": 30,
        "reward": 462.0,
        "loss": 1.5022961765395403e-14,
        "steps": 1000,
        "epsilon": 0.07300000000000006
      },
      {
        "episode": 31,
        "reward": 444.0,
        "loss": 3.248414569792409e-15,
        "steps": 1000,
        "epsilon": 0.07210000000000007
      },
      {
        "episode": 32,
        "reward": 185.0,
        "loss": 0.13480299710741867,
        "steps": 407,
        "epsilon": 0.07120000000000007
      },
      {
        "episode": 33,
        "reward": 439.0,
        "loss": 2.0171085137420393e-16,
        "steps": 1000,
        "epsilon": 0.07030000000000007
      },
      {
        "episode": 34,
        "reward": 451.0,
        "loss": 2.924982992405104e-17,
        "steps": 1000,
        "epsilon": 0.06940000000000007
      },
      {
        "episode": 35,
        "reward": 335.0,
        "loss": 0.06269087126425735,
        "steps": 771,
        "epsilon": 0.06850000000000007
      },
      {
        "episode": 36,
        "reward": 448.0,
        "loss": 6.511772789459605e-19,
        "steps": 1000,
        "epsilon": 0.06760000000000008
      },
      {
        "episode": 37,
        "reward": 450.0,
        "loss": 0.0424491232079001,
        "steps": 1000,
        "epsilon": 0.06670000000000008
      },
      {
        "episode": 38,
        "reward": 134.0,
        "loss": 6.144108966801593e-05,
        "steps": 297,
        "epsilon": 0.06580000000000008
      },
      {
        "episode": 39,
        "reward": 111.0,
        "loss": 6.569281307304255e-05,
        "steps": 225,
        "epsilon": 0.06490000000000008
      },
      {
        "episode": 40,
        "reward": 466.0,
        "loss": 6.743115855940922e-21,
        "steps": 1000,
        "epsilon": 0.06400000000000008
      },
      {
        "episode": 41,
        "reward": 25.0,
        "loss": 0.00023475519965807844,
        "steps": 51,
        "epsilon": 0.06310000000000009
      },
      {
        "episode": 42,
        "reward": 94.0,
        "loss": 4.9227092882615324e-05,
        "steps": 197,
        "epsilon": 0.06220000000000009
      },
      {
        "episode": 43,
        "reward": 443.0,
        "loss": 7.863030241520455e-06,
        "steps": 999,
        "epsilon": 0.06130000000000009
      },
      {
        "episode": 44,
        "reward": 454.0,
        "loss": 1.1775610267776042e-22,
        "steps": 1000,
        "epsilon": 0.06040000000000009
      },
      {
        "episode": 45,
        "reward": 459.0,
        "loss": 1.831387509130689e-23,
        "steps": 1000,
        "epsilon": 0.059500000000000094
      },
      {
        "episode": 46,
        "reward": 389.0,
        "loss": 7.407084331939401e-06,
        "steps": 859,
        "epsilon": 0.058600000000000096
      },
      {
        "episode": 47,
        "reward": 32.0,
        "loss": 8.448811815278868e-05,
        "steps": 61,
        "epsilon": 0.0577000000000001
      },
      {
        "episode": 48,
        "reward": 455.0,
        "loss": 0.06972266341876447,
        "steps": 1000,
        "epsilon": 0.0568000000000001
      },
      {
        "episode": 49,
        "reward": 454.0,
        "loss": 7.518547459594975e-26,
        "steps": 1000,
        "epsilon": 0.0559000000000001
      },
      {
        "episode": 50,
        "reward": 456.0,
        "loss": 4.8238528809902784e-27,
        "steps": 1000,
        "epsilon": 0.055000000000000104
      },
      {
        "episode": 51,
        "reward": 17.0,
        "loss": 0.0001346631586428803,
        "steps": 31,
        "epsilon": 0.054100000000000106
      },
      {
        "episode": 52,
        "reward": 458.0,
        "loss": 1.4285955449190436e-27,
        "steps": 1000,
        "epsilon": 0.05320000000000011
      },
      {
        "episode": 53,
        "reward": 450.0,
        "loss": 4.425446569417458e-28,
        "steps": 1000,
        "epsilon": 0.05230000000000011
      },
      {
        "episode": 54,
        "reward": 469.0,
        "loss": 0.053043758081553795,
        "steps": 1000,
        "epsilon": 0.05140000000000011
      },
      {
        "episode": 55,
        "reward": 475.0,
        "loss": 3.1429480641453464e-28,
        "steps": 1000,
        "epsilon": 0.050500000000000114
      },
      {
        "episode": 56,
        "reward": 464.0,
        "loss": 3.1395401850347915e-28,
        "steps": 1000,
        "epsilon": 0.049600000000000116
      },
      {
        "episode": 57,
        "reward": 370.0,
        "loss": 4.242649828761261e-06,
        "steps": 797,
        "epsilon": 0.04870000000000012
      },
      {
        "episode": 58,
        "reward": 453.0,
        "loss": 3.1406761447383098e-28,
        "steps": 1000,
        "epsilon": 0.04780000000000012
      },
      {
        "episode": 59,
        "reward": 459.0,
        "loss": 3.1406761447383098e-28,
        "steps": 1000,
        "epsilon": 0.04690000000000012
      },
      {
        "episode": 60,
        "reward": 466.0,
        "loss": 3.141812104441828e-28,
        "steps": 1000,
        "epsilon": 0.046000000000000124
      },
      {
        "episode": 61,
        "reward": 458.0,
        "loss": 0.021430131987901054,
        "steps": 1000,
        "epsilon": 0.045100000000000126
      },
      {
        "episode": 62,
        "reward": 466.0,
        "loss": 0.03462909737296697,
        "steps": 1000,
        "epsilon": 0.04420000000000013
      },
      {
        "episode": 63,
        "reward": 462.0,
        "loss": 0.025808492914517374,
        "steps": 1000,
        "epsilon": 0.04330000000000013
      },
      {
        "episode": 64,
        "reward": 481.0,
        "loss": 3.1474919029594196e-28,
        "steps": 1000,
        "epsilon": 0.04240000000000013
      },
      {
        "episode": 65,
        "reward": 463.0,
        "loss": 3.1474919029594196e-28,
        "steps": 1000,
        "epsilon": 0.041500000000000134
      },
      {
        "episode": 66,
        "reward": 475.0,
        "loss": 3.1474919029594196e-28,
        "steps": 1000,
        "epsilon": 0.040600000000000136
      },
      {
        "episode": 67,
        "reward": 472.0,
        "loss": 3.148627862662938e-28,
        "steps": 1000,
        "epsilon": 0.03970000000000014
      },
      {
        "episode": 68,
        "reward": 472.0,
        "loss": 3.1463559432559013e-28,
        "steps": 1000,
        "epsilon": 0.03880000000000014
      },
      {
        "episode": 69,
        "reward": 210.0,
        "loss": 6.2963849424216185e-06,
        "steps": 435,
        "epsilon": 0.03790000000000014
      },
      {
        "episode": 70,
        "reward": 471.0,
        "loss": 3.145219983552383e-28,
        "steps": 1000,
        "epsilon": 0.037000000000000144
      },
      {
        "episode": 71,
        "reward": 473.0,
        "loss": 3.145219983552383e-28,
        "steps": 1000,
        "epsilon": 0.036100000000000146
      },
      {
        "episode": 72,
        "reward": 478.0,
        "loss": 0.010276097983299162,
        "steps": 1000,
        "epsilon": 0.03520000000000015
      },
      {
        "episode": 73,
        "reward": 477.0,
        "loss": 3.145219983552383e-28,
        "steps": 1000,
        "epsilon": 0.03430000000000015
      },
      {
        "episode": 74,
        "reward": 206.0,
        "loss": 5.123628717002903e-06,
        "steps": 433,
        "epsilon": 0.03340000000000015
      },
      {
        "episode": 75,
        "reward": 470.0,
        "loss": 3.1440840238488647e-28,
        "steps": 1000,
        "epsilon": 0.032500000000000154
      },
      {
        "episode": 76,
        "reward": 483.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.031600000000000156
      },
      {
        "episode": 77,
        "reward": 455.0,
        "loss": 1.909681508942002e-06,
        "steps": 941,
        "epsilon": 0.030700000000000154
      },
      {
        "episode": 78,
        "reward": 468.0,
        "loss": 0.008825734912732431,
        "steps": 1000,
        "epsilon": 0.029800000000000153
      },
      {
        "episode": 79,
        "reward": 140.0,
        "loss": 5.036603262735939e-06,
        "steps": 289,
        "epsilon": 0.02890000000000015
      },
      {
        "episode": 80,
        "reward": 482.0,
        "loss": 3.141812104441828e-28,
        "steps": 1000,
        "epsilon": 0.02800000000000015
      },
      {
        "episode": 81,
        "reward": 474.0,
        "loss": 3.1463559432559013e-28,
        "steps": 1000,
        "epsilon": 0.02710000000000015
      },
      {
        "episode": 82,
        "reward": 479.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.026200000000000147
      },
      {
        "episode": 83,
        "reward": 271.0,
        "loss": 2.139779415197562e-06,
        "steps": 551,
        "epsilon": 0.025300000000000145
      },
      {
        "episode": 84,
        "reward": 483.0,
        "loss": 3.1474919029594196e-28,
        "steps": 1000,
        "epsilon": 0.024400000000000144
      },
      {
        "episode": 85,
        "reward": 490.0,
        "loss": 3.150899782069974e-28,
        "steps": 1000,
        "epsilon": 0.023500000000000142
      },
      {
        "episode": 86,
        "reward": 480.0,
        "loss": 3.150899782069974e-28,
        "steps": 1000,
        "epsilon": 0.02260000000000014
      },
      {
        "episode": 87,
        "reward": 480.0,
        "loss": 3.150899782069974e-28,
        "steps": 1000,
        "epsilon": 0.02170000000000014
      },
      {
        "episode": 88,
        "reward": 488.0,
        "loss": 3.1531717014770106e-28,
        "steps": 1000,
        "epsilon": 0.020800000000000138
      },
      {
        "episode": 89,
        "reward": 485.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.019900000000000136
      },
      {
        "episode": 90,
        "reward": 489.0,
        "loss": 3.1520357417734923e-28,
        "steps": 1000,
        "epsilon": 0.019000000000000135
      },
      {
        "episode": 91,
        "reward": 479.0,
        "loss": 3.1474919029594196e-28,
        "steps": 1000,
        "epsilon": 0.018100000000000133
      },
      {
        "episode": 92,
        "reward": 483.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.017200000000000132
      },
      {
        "episode": 93,
        "reward": 491.0,
        "loss": 3.1520357417734923e-28,
        "steps": 1000,
        "epsilon": 0.01630000000000013
      },
      {
        "episode": 94,
        "reward": 480.0,
        "loss": 0.007568467064664441,
        "steps": 1000,
        "epsilon": 0.01540000000000013
      },
      {
        "episode": 95,
        "reward": 487.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.01450000000000013
      },
      {
        "episode": 96,
        "reward": 493.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.013600000000000131
      },
      {
        "episode": 97,
        "reward": 492.0,
        "loss": 3.150899782069974e-28,
        "steps": 1000,
        "epsilon": 0.012700000000000131
      },
      {
        "episode": 98,
        "reward": 489.0,
        "loss": 3.1497638223664557e-28,
        "steps": 1000,
        "epsilon": 0.011800000000000132
      },
      {
        "episode": 99,
        "reward": 493.0,
        "loss": 3.1520357417734923e-28,
        "steps": 1000,
        "epsilon": 0.010900000000000132
      },
      {
        "episode": 100,
        "reward": 494.0,
        "loss": 3.150899782069974e-28,
        "steps": 1000,
        "epsilon": 0.010000000000000132
      }
    ]
  },
  "resultats_eval": {
    "avg_reward": 500.0,
    "std_reward": 0.0,
    "avg_steps": 1000.0,
    "learning_curve": [
      88.0,
      428.0,
      222.0,
      42.0,
      5.0,
      199.0,
      21.0,
      162.0,
      429.0,
      52.0,
      441.0,
      381.0,
      453.0,
      316.0,
      48.0,
      440.0,
      433.0,
      426.0,
      435.0,
      439.0,
      430.0,
      450.0,
      93.0,
      403.0,
      436.0,
      230.0,
      185.0,
      40.0,
      442.0,
      462.0,
      444.0,
      185.0,
      439.0,
      451.0,
      335.0,
      448.0,
      450.0,
      134.0,
      111.0,
      466.0,
      25.0,
      94.0,
      443.0,
      454.0,
      459.0,
      389.0,
      32.0,
      455.0,
      454.0,
      456.0,
      17.0,
      458.0,
      450.0,
      469.0,
      475.0,
      464.0,
      370.0,
      453.0,
      459.0,
      466.0,
      458.0,
      466.0,
      462.0,
      481.0,
      463.0,
      475.0,
      472.0,
      472.0,
      210.0,
      471.0,
      473.0,
      478.0,
      477.0,
      206.0,
      470.0,
      483.0,
      455.0,
      468.0,
      140.0,
      482.0,
      474.0,
      479.0,
      271.0,
      483.0,
      490.0,
      480.0,
      480.0,
      488.0,
      485.0,
      489.0,
      479.0,
      483.0,
      491.0,
      480.0,
      487.0,
      493.0,
      492.0,
      489.0,
      493.0,
      494.0
    ]
  },
  "score": 500.0,
  "reward_final": null,
  "eval_metrics": {
    "avg_reward": 500.0,
    "min_reward": null,
    "max_reward": null,
    "iterations": null,
    "execution_time": null,
    "learning_curve": [
      88.0,
      428.0,
      222.0,
      42.0,
      5.0,
      199.0,
      21.0,
      162.0,
      429.0,
      52.0,
      441.0,
      381.0,
      453.0,
      316.0,
      48.0,
      440.0,
      433.0,
      426.0,
      435.0,
      439.0,
      430.0,
      450.0,
      93.0,
      403.0,
      436.0,
      230.0,
      185.0,
      40.0,
      442.0,
      462.0,
      444.0,
      185.0,
      439.0,
      451.0,
      335.0,
      448.0,
      450.0,
      134.0,
      111.0,
      466.0,
      25.0,
      94.0,
      443.0,
      454.0,
      459.0,
      389.0,
      32.0,
      455.0,
      454.0,
      456.0,
      17.0,
      458.0,
      450.0,
      469.0,
      475.0,
      464.0,
      370.0,
      453.0,
      459.0,
      466.0,
      458.0,
      466.0,
      462.0,
      481.0,
      463.0,
      475.0,
      472.0,
      472.0,
      210.0,
      471.0,
      473.0,
      478.0,
      477.0,
      206.0,
      470.0,
      483.0,
      455.0,
      468.0,
      140.0,
      482.0,
      474.0,
      479.0,
      271.0,
      483.0,
      490.0,
      480.0,
      480.0,
      488.0,
      485.0,
      489.0,
      479.0,
      483.0,
      491.0,
      480.0,
      487.0,
      493.0,
      492.0,
      489.0,
      493.0,
      494.0
    ]
  }
}