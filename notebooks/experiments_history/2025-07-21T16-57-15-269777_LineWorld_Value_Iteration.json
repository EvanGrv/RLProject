{
  "datetime": "2025-07-21T16:57:15.269777",
  "environnement": "LineWorld",
  "modele": "Value Iteration",
  "hyperparametres": {},
  "resultats_train": {
    "V": [
      0.0,
      1.0,
      0.999999,
      0.9999980000009999,
      0.9999980000009999,
      0.999999,
      1.0,
      0.0
    ],
    "policy": [
      [
        0.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        0.0,
        1.0
      ],
      [
        0.0,
        1.0
      ],
      [
        0.0,
        1.0
      ],
      [
        0.0,
        0.0
      ]
    ],
    "history": [
      {
        "iteration": 1,
        "delta": 1.0,
        "avg_value": 0.0,
        "max_value": 0.0,
        "converged": false
      },
      {
        "iteration": 2,
        "delta": 0.999999,
        "avg_value": 0.25,
        "max_value": 1.0,
        "converged": false
      },
      {
        "iteration": 3,
        "delta": 0.9999980000009999,
        "avg_value": 0.49999974999999997,
        "max_value": 1.0,
        "converged": false
      },
      {
        "iteration": 4,
        "delta": 0.0,
        "avg_value": 0.7499992500002499,
        "max_value": 1.0,
        "converged": true
      }
    ],
    "iterations": 4,
    "converged": true,
    "final_delta": 0.0
  },
  "resultats_eval": {
    "avg_reward": 3.0,
    "min_reward": 3.0,
    "max_reward": 3.0,
    "iterations": 4,
    "converged": true,
    "execution_time": 0.00023293495178222656,
    "learning_curve": [
      0.0,
      0.25,
      0.49999974999999997,
      0.7499992500002499
    ],
    "rewards": [
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0
    ],
    "steps_per_episode": [
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3,
      3
    ]
  },
  "score": 3.0,
  "reward_final": null,
  "eval_metrics": {
    "avg_reward": 3.0,
    "min_reward": 3.0,
    "max_reward": 3.0,
    "iterations": 4,
    "execution_time": 0.00023293495178222656,
    "learning_curve": [
      0.0,
      0.25,
      0.49999974999999997,
      0.7499992500002499
    ]
  }
}