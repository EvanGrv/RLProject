{
  "datetime": "2025-07-21T17:48:27.381258",
  "environnement": "LineWorld",
  "modele": "QLearning",
  "hyperparametres": {
    "gamma": 0.99,
    "alpha": 0.11,
    "epsilon": 0.1,
    "num_episodes": 100
  },
  "resultats_train": {
    "Q": [
      [
        0.0,
        0.0
      ],
      [
        0.0,
        0.0
      ],
      [
        0.0,
        0.0
      ],
      [
        0.0,
        0.587388658191994
      ],
      [
        -0.2561186723348985,
        2.969305358068025
      ],
      [
        0.8464448507423132,
        1.9898773377486672
      ],
      [
        -0.098021,
        0.9999913103824115
      ],
      [
        0.0,
        0.0
      ]
    ],
    "policy": [
      0,
      0,
      0,
      1,
      1,
      1,
      1,
      0
    ],
    "history": [
      {
        "episode": 1,
        "reward": 3.0,
        "loss": 0.917231842,
        "steps": 5,
        "epsilon": 0.09910000000000001
      },
      {
        "episode": 2,
        "reward": 3.0,
        "loss": 0.9349082213469999,
        "steps": 3,
        "epsilon": 0.09820000000000001
      },
      {
        "episode": 3,
        "reward": 3.0,
        "loss": 0.8624901011777227,
        "steps": 3,
        "epsilon": 0.09730000000000001
      },
      {
        "episode": 4,
        "reward": 3.0,
        "loss": 0.7971925182774168,
        "steps": 3,
        "epsilon": 0.09640000000000001
      },
      {
        "episode": 5,
        "reward": 3.0,
        "loss": 0.737200258062315,
        "steps": 3,
        "epsilon": 0.09550000000000002
      },
      {
        "episode": 6,
        "reward": 2.0,
        "loss": 0.6743449174001146,
        "steps": 5,
        "epsilon": 0.09460000000000002
      },
      {
        "episode": 7,
        "reward": 3.0,
        "loss": 0.6289393966008737,
        "steps": 3,
        "epsilon": 0.09370000000000002
      },
      {
        "episode": 8,
        "reward": 3.0,
        "loss": 0.579503531634913,
        "steps": 3,
        "epsilon": 0.09280000000000002
      },
      {
        "episode": 9,
        "reward": 3.0,
        "loss": 0.5327861963134207,
        "steps": 3,
        "epsilon": 0.09190000000000002
      },
      {
        "episode": 10,
        "reward": 3.0,
        "loss": 0.48863592679618845,
        "steps": 3,
        "epsilon": 0.09100000000000003
      },
      {
        "episode": 11,
        "reward": 3.0,
        "loss": 0.4469677383612167,
        "steps": 3,
        "epsilon": 0.09010000000000003
      },
      {
        "episode": 12,
        "reward": 3.0,
        "loss": 0.4077326996774571,
        "steps": 3,
        "epsilon": 0.08920000000000003
      },
      {
        "episode": 13,
        "reward": 3.0,
        "loss": 0.37089781831373286,
        "steps": 3,
        "epsilon": 0.08830000000000003
      },
      {
        "episode": 14,
        "reward": 3.0,
        "loss": 0.336433054859038,
        "steps": 3,
        "epsilon": 0.08740000000000003
      },
      {
        "episode": 15,
        "reward": 3.0,
        "loss": 0.30430330649434845,
        "steps": 3,
        "epsilon": 0.08650000000000004
      },
      {
        "episode": 16,
        "reward": 3.0,
        "loss": 0.2744638678580597,
        "steps": 3,
        "epsilon": 0.08560000000000004
      },
      {
        "episode": 17,
        "reward": 3.0,
        "loss": 0.24685831725302598,
        "steps": 3,
        "epsilon": 0.08470000000000004
      },
      {
        "episode": 18,
        "reward": 3.0,
        "loss": 0.2214180723587987,
        "steps": 3,
        "epsilon": 0.08380000000000004
      },
      {
        "episode": 19,
        "reward": 3.0,
        "loss": 0.19806306386120295,
        "steps": 3,
        "epsilon": 0.08290000000000004
      },
      {
        "episode": 20,
        "reward": 3.0,
        "loss": 0.17670312046069367,
        "steps": 3,
        "epsilon": 0.08200000000000005
      },
      {
        "episode": 21,
        "reward": 3.0,
        "loss": 0.15723976485459168,
        "steps": 3,
        "epsilon": 0.08110000000000005
      },
      {
        "episode": 22,
        "reward": 3.0,
        "loss": 0.13956820005701612,
        "steps": 3,
        "epsilon": 0.08020000000000005
      },
      {
        "episode": 23,
        "reward": 3.0,
        "loss": 0.12357932661844202,
        "steps": 3,
        "epsilon": 0.07930000000000005
      },
      {
        "episode": 24,
        "reward": 3.0,
        "loss": 0.10916167884819328,
        "steps": 3,
        "epsilon": 0.07840000000000005
      },
      {
        "episode": 25,
        "reward": 3.0,
        "loss": 0.09620320522211549,
        "steps": 3,
        "epsilon": 0.07750000000000005
      },
      {
        "episode": 26,
        "reward": 2.0,
        "loss": 1.1265269251106695,
        "steps": 5,
        "epsilon": 0.07660000000000006
      },
      {
        "episode": 27,
        "reward": 3.0,
        "loss": 0.0742218907915241,
        "steps": 3,
        "epsilon": 0.07570000000000006
      },
      {
        "episode": 28,
        "reward": 3.0,
        "loss": 0.06498508873374632,
        "steps": 3,
        "epsilon": 0.07480000000000006
      },
      {
        "episode": 29,
        "reward": 3.0,
        "loss": 0.05678154941978886,
        "steps": 3,
        "epsilon": 0.07390000000000006
      },
      {
        "episode": 30,
        "reward": 3.0,
        "loss": 0.04951541447189633,
        "steps": 3,
        "epsilon": 0.07300000000000006
      },
      {
        "episode": 31,
        "reward": 3.0,
        "loss": 0.04309633853575421,
        "steps": 3,
        "epsilon": 0.07210000000000007
      },
      {
        "episode": 32,
        "reward": 3.0,
        "loss": 0.037439794993846025,
        "steps": 3,
        "epsilon": 0.07120000000000007
      },
      {
        "episode": 33,
        "reward": 3.0,
        "loss": 0.03246723073447607,
        "steps": 3,
        "epsilon": 0.07030000000000007
      },
      {
        "episode": 34,
        "reward": 3.0,
        "loss": 0.028106093438449633,
        "steps": 3,
        "epsilon": 0.06940000000000007
      },
      {
        "episode": 35,
        "reward": 3.0,
        "loss": 0.02428975396152729,
        "steps": 3,
        "epsilon": 0.06850000000000007
      },
      {
        "episode": 36,
        "reward": 3.0,
        "loss": 0.020957344851753157,
        "steps": 3,
        "epsilon": 0.06760000000000008
      },
      {
        "episode": 37,
        "reward": 3.0,
        "loss": 0.018053534092587517,
        "steps": 3,
        "epsilon": 0.06670000000000008
      },
      {
        "episode": 38,
        "reward": 3.0,
        "loss": 0.015528250997792668,
        "steps": 3,
        "epsilon": 0.06580000000000008
      },
      {
        "episode": 39,
        "reward": 3.0,
        "loss": 0.013336378948394104,
        "steps": 3,
        "epsilon": 0.06490000000000008
      },
      {
        "episode": 40,
        "reward": 3.0,
        "loss": 0.011437427462977177,
        "steps": 3,
        "epsilon": 0.06400000000000008
      },
      {
        "episode": 41,
        "reward": 3.0,
        "loss": 0.009795194005178967,
        "steps": 3,
        "epsilon": 0.06310000000000009
      },
      {
        "episode": 42,
        "reward": 3.0,
        "loss": 0.008377424005668271,
        "steps": 3,
        "epsilon": 0.06220000000000009
      },
      {
        "episode": 43,
        "reward": 3.0,
        "loss": 0.007155475838941064,
        "steps": 3,
        "epsilon": 0.06130000000000009
      },
      {
        "episode": 44,
        "reward": 3.0,
        "loss": 0.006103995961011031,
        "steps": 3,
        "epsilon": 0.06040000000000009
      },
      {
        "episode": 45,
        "reward": 3.0,
        "loss": 0.005200608084044216,
        "steps": 3,
        "epsilon": 0.059500000000000094
      },
      {
        "episode": 46,
        "reward": 3.0,
        "loss": 0.004425619131375502,
        "steps": 3,
        "epsilon": 0.058600000000000096
      },
      {
        "episode": 47,
        "reward": 4.0,
        "loss": 1.5901934090866197,
        "steps": 5,
        "epsilon": 0.0577000000000001
      },
      {
        "episode": 48,
        "reward": 3.0,
        "loss": 0.002604953273215379,
        "steps": 3,
        "epsilon": 0.0568000000000001
      },
      {
        "episode": 49,
        "reward": 3.0,
        "loss": 0.002224402910413984,
        "steps": 3,
        "epsilon": 0.0559000000000001
      },
      {
        "episode": 50,
        "reward": 3.0,
        "loss": 0.0018968053303081607,
        "steps": 3,
        "epsilon": 0.055000000000000104
      },
      {
        "episode": 51,
        "reward": 4.0,
        "loss": 1.3011325725945329,
        "steps": 5,
        "epsilon": 0.054100000000000106
      },
      {
        "episode": 52,
        "reward": 3.0,
        "loss": 0.001121920857180344,
        "steps": 3,
        "epsilon": 0.05320000000000011
      },
      {
        "episode": 53,
        "reward": 3.0,
        "loss": 0.0009595582588306674,
        "steps": 3,
        "epsilon": 0.05230000000000011
      },
      {
        "episode": 54,
        "reward": 3.0,
        "loss": 0.0008194253086334091,
        "steps": 3,
        "epsilon": 0.05140000000000011
      },
      {
        "episode": 55,
        "reward": 3.0,
        "loss": 0.0006987198709822282,
        "steps": 3,
        "epsilon": 0.050500000000000114
      },
      {
        "episode": 56,
        "reward": 3.0,
        "loss": 0.0005949463423881891,
        "steps": 3,
        "epsilon": 0.049600000000000116
      },
      {
        "episode": 57,
        "reward": 3.0,
        "loss": 0.0005058914821262612,
        "steps": 3,
        "epsilon": 0.04870000000000012
      },
      {
        "episode": 58,
        "reward": 3.0,
        "loss": 0.0004296000906466305,
        "steps": 3,
        "epsilon": 0.04780000000000012
      },
      {
        "episode": 59,
        "reward": 3.0,
        "loss": 0.0003643511363488503,
        "steps": 3,
        "epsilon": 0.04690000000000012
      },
      {
        "episode": 60,
        "reward": 2.0,
        "loss": 1.4033242783957218,
        "steps": 5,
        "epsilon": 0.046000000000000124
      },
      {
        "episode": 61,
        "reward": 3.0,
        "loss": 0.00026113053078036583,
        "steps": 3,
        "epsilon": 0.045100000000000126
      },
      {
        "episode": 62,
        "reward": 3.0,
        "loss": 0.00022068694934472828,
        "steps": 3,
        "epsilon": 0.04420000000000013
      },
      {
        "episode": 63,
        "reward": 3.0,
        "loss": 0.0001863026509110257,
        "steps": 3,
        "epsilon": 0.04330000000000013
      },
      {
        "episode": 64,
        "reward": 3.0,
        "loss": 0.00015710901695532113,
        "steps": 3,
        "epsilon": 0.04240000000000013
      },
      {
        "episode": 65,
        "reward": 3.0,
        "loss": 0.0001323543965904515,
        "steps": 3,
        "epsilon": 0.041500000000000134
      },
      {
        "episode": 66,
        "reward": 4.0,
        "loss": 1.0855944518236498,
        "steps": 5,
        "epsilon": 0.040600000000000136
      },
      {
        "episode": 67,
        "reward": 3.0,
        "loss": 7.588719561347751e-05,
        "steps": 3,
        "epsilon": 0.03970000000000014
      },
      {
        "episode": 68,
        "reward": 3.0,
        "loss": 6.413943672366456e-05,
        "steps": 3,
        "epsilon": 0.03880000000000014
      },
      {
        "episode": 69,
        "reward": 3.0,
        "loss": 5.414712621884931e-05,
        "steps": 3,
        "epsilon": 0.03790000000000014
      },
      {
        "episode": 70,
        "reward": 3.0,
        "loss": 4.5660225904192555e-05,
        "steps": 3,
        "epsilon": 0.037000000000000144
      },
      {
        "episode": 71,
        "reward": 3.0,
        "loss": 3.8461905805315266e-05,
        "steps": 3,
        "epsilon": 0.036100000000000146
      },
      {
        "episode": 72,
        "reward": 3.0,
        "loss": 3.236461355684747e-05,
        "steps": 3,
        "epsilon": 0.03520000000000015
      },
      {
        "episode": 73,
        "reward": 3.0,
        "loss": 2.7206509867403933e-05,
        "steps": 3,
        "epsilon": 0.03430000000000015
      },
      {
        "episode": 74,
        "reward": 3.0,
        "loss": 2.2848258131312493e-05,
        "steps": 3,
        "epsilon": 0.03340000000000015
      },
      {
        "episode": 75,
        "reward": 3.0,
        "loss": 1.9170150552718412e-05,
        "steps": 3,
        "epsilon": 0.032500000000000154
      },
      {
        "episode": 76,
        "reward": 3.0,
        "loss": 1.606954945155556e-05,
        "steps": 3,
        "epsilon": 0.031600000000000156
      },
      {
        "episode": 77,
        "reward": 3.0,
        "loss": 1.3458620290079194e-05,
        "steps": 3,
        "epsilon": 0.030700000000000154
      },
      {
        "episode": 78,
        "reward": 3.0,
        "loss": 1.1262332024649145e-05,
        "steps": 3,
        "epsilon": 0.029800000000000153
      },
      {
        "episode": 79,
        "reward": 3.0,
        "loss": 9.41670034746847e-06,
        "steps": 3,
        "epsilon": 0.02890000000000015
      },
      {
        "episode": 80,
        "reward": 3.0,
        "loss": 7.867249991736196e-06,
        "steps": 3,
        "epsilon": 0.02800000000000015
      },
      {
        "episode": 81,
        "reward": 3.0,
        "loss": 6.567673335055399e-06,
        "steps": 3,
        "epsilon": 0.02710000000000015
      },
      {
        "episode": 82,
        "reward": 3.0,
        "loss": 5.478663895504633e-06,
        "steps": 3,
        "epsilon": 0.026200000000000147
      },
      {
        "episode": 83,
        "reward": 3.0,
        "loss": 4.566904852742741e-06,
        "steps": 3,
        "epsilon": 0.025300000000000145
      },
      {
        "episode": 84,
        "reward": 3.0,
        "loss": 3.80419435139505e-06,
        "steps": 3,
        "epsilon": 0.024400000000000144
      },
      {
        "episode": 85,
        "reward": 3.0,
        "loss": 3.1666909879811175e-06,
        "steps": 3,
        "epsilon": 0.023500000000000142
      },
      {
        "episode": 86,
        "reward": 3.0,
        "loss": 2.634264496039158e-06,
        "steps": 3,
        "epsilon": 0.02260000000000014
      },
      {
        "episode": 87,
        "reward": 3.0,
        "loss": 2.189938192541694e-06,
        "steps": 3,
        "epsilon": 0.02170000000000014
      },
      {
        "episode": 88,
        "reward": 3.0,
        "loss": 1.8194112089969824e-06,
        "steps": 3,
        "epsilon": 0.020800000000000138
      },
      {
        "episode": 89,
        "reward": 3.0,
        "loss": 1.5106498888578304e-06,
        "steps": 3,
        "epsilon": 0.019900000000000136
      },
      {
        "episode": 90,
        "reward": 3.0,
        "loss": 1.2535389818021482e-06,
        "steps": 3,
        "epsilon": 0.019000000000000135
      },
      {
        "episode": 91,
        "reward": 3.0,
        "loss": 1.0395844028568834e-06,
        "steps": 3,
        "epsilon": 0.018100000000000133
      },
      {
        "episode": 92,
        "reward": 3.0,
        "loss": 8.616603518306119e-07,
        "steps": 3,
        "epsilon": 0.017200000000000132
      },
      {
        "episode": 93,
        "reward": 3.0,
        "loss": 7.137945101556284e-07,
        "steps": 3,
        "epsilon": 0.01630000000000013
      },
      {
        "episode": 94,
        "reward": 3.0,
        "loss": 5.909858537770102e-07,
        "steps": 3,
        "epsilon": 0.01540000000000013
      },
      {
        "episode": 95,
        "reward": 3.0,
        "loss": 4.890503491287196e-07,
        "steps": 3,
        "epsilon": 0.01450000000000013
      },
      {
        "episode": 96,
        "reward": 3.0,
        "loss": 4.044904418459058e-07,
        "steps": 3,
        "epsilon": 0.013600000000000131
      },
      {
        "episode": 97,
        "reward": 3.0,
        "loss": 3.343848123398703e-07,
        "steps": 3,
        "epsilon": 0.012700000000000131
      },
      {
        "episode": 98,
        "reward": 3.0,
        "loss": 2.762953662537392e-07,
        "steps": 3,
        "epsilon": 0.011800000000000132
      },
      {
        "episode": 99,
        "reward": 3.0,
        "loss": 2.2818885833674488e-07,
        "steps": 3,
        "epsilon": 0.010900000000000132
      },
      {
        "episode": 100,
        "reward": 3.0,
        "loss": 1.883709224097144e-07,
        "steps": 3,
        "epsilon": 0.010000000000000132
      }
    ]
  },
  "resultats_eval": {
    "avg_reward": 3.0,
    "std_reward": 0.0,
    "avg_steps": 3.0,
    "learning_curve": [
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      2.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      2.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      4.0,
      3.0,
      3.0,
      3.0,
      4.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      2.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      4.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0
    ]
  },
  "score": 3.0,
  "reward_final": null,
  "eval_metrics": {
    "avg_reward": 3.0,
    "min_reward": null,
    "max_reward": null,
    "iterations": null,
    "execution_time": null,
    "learning_curve": [
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      2.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      2.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      4.0,
      3.0,
      3.0,
      3.0,
      4.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      2.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      4.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0,
      3.0
    ]
  }
}