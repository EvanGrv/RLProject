{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🕵️ Analyse Monte Carlo sur les Environnements Secrets\n",
        "\n",
        "## 🎯 Objectif\n",
        "Ce notebook utilise les **algorithmes Monte Carlo existants** du projet (situés dans `src/monte_carlo.py`) pour analyser les **4 environnements secrets** :\n",
        "- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**\n",
        "\n",
        "## 📋 Algorithmes Monte Carlo utilisés (depuis `src/`)\n",
        "- **`MonteCarloES`** : Monte Carlo avec Exploring Starts\n",
        "- **`OnPolicyMC`** : On-policy First-Visit Monte Carlo avec ε-greedy  \n",
        "- **`OffPolicyMC`** : Off-policy Monte Carlo avec Importance Sampling\n",
        "\n",
        "## 📊 Analyses complètes effectuées\n",
        "- **Adaptateur** pour interfacer les environnements secrets avec l'API attendue\n",
        "- **Entraînement** des 3 algorithmes sur les 4 environnements (12 combinaisons)\n",
        "- **Courbes d'apprentissage** (récompenses, Q-values, convergence)\n",
        "- **Évaluations** de performance et comparaisons détaillées\n",
        "- **Métriques** de succès, stabilité et recommandations par environnement\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 INITIALISATION DE L'ANALYSE MONTE CARLO\n",
            "============================================================\n",
            "✅ Environnements secrets importés avec succès\n",
            "📊 SecretEnv0 - États: 8192, Actions: 3\n",
            "📊 SecretEnv1 - États: 65536, Actions: 3\n",
            "📊 SecretEnv2 - États: 2097152, Actions: 3\n",
            "📊 SecretEnv3 - États: 65536, Actions: 3\n",
            "\n",
            "🎉 4 environnements secrets fonctionnels !\n",
            "✅ Algorithmes Monte Carlo importés depuis src/monte_carlo.py\n",
            "   • MonteCarloES - Monte Carlo avec Exploring Starts\n",
            "   • OnPolicyMC - On-policy First-Visit Monte Carlo\n",
            "   • OffPolicyMC - Off-policy Monte Carlo avec Importance Sampling\n",
            "\n",
            "🔧 Configuration terminée !\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 📚 Imports et Configuration Complète\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib pour de beaux graphiques\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# 🔧 Configuration des chemins - Ajouter le projet et les modules\n",
        "project_root = os.path.abspath('../../')\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
        "sys.path.insert(0, os.path.join(project_root, 'game', 'secret_env'))\n",
        "\n",
        "print(\"🚀 INITIALISATION DE L'ANALYSE MONTE CARLO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 🎮 Import des environnements secrets\n",
        "try:\n",
        "    from secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
        "    print(\"✅ Environnements secrets importés avec succès\")\n",
        "    \n",
        "    # Test rapide des environnements\n",
        "    env_configs = [\n",
        "        (\"SecretEnv0\", SecretEnv0),\n",
        "        (\"SecretEnv1\", SecretEnv1), \n",
        "        (\"SecretEnv2\", SecretEnv2),\n",
        "        (\"SecretEnv3\", SecretEnv3)\n",
        "    ]\n",
        "    \n",
        "    env_info = {}\n",
        "    for env_name, env_class in env_configs:\n",
        "        try:\n",
        "            env = env_class()\n",
        "            states = env.num_states()\n",
        "            actions = env.num_actions()\n",
        "            env_info[env_name] = {'states': states, 'actions': actions, 'class': env_class}\n",
        "            print(f\"📊 {env_name} - États: {states}, Actions: {actions}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur avec {env_name}: {e}\")\n",
        "            env_info[env_name] = None\n",
        "    \n",
        "    print(f\"\\n🎉 {len([k for k,v in env_info.items() if v is not None])} environnements secrets fonctionnels !\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur d'import des environnements secrets: {e}\")\n",
        "    print(\"Vérifiez que les bibliothèques natives sont présentes dans ./libs/\")\n",
        "    env_info = {}\n",
        "\n",
        "# 🧠 Import des algorithmes Monte Carlo existants du projet\n",
        "try:\n",
        "    from monte_carlo import MonteCarloES, OnPolicyMC, OffPolicyMC\n",
        "    print(\"✅ Algorithmes Monte Carlo importés depuis src/monte_carlo.py\")\n",
        "    print(\"   • MonteCarloES - Monte Carlo avec Exploring Starts\")\n",
        "    print(\"   • OnPolicyMC - On-policy First-Visit Monte Carlo\")  \n",
        "    print(\"   • OffPolicyMC - Off-policy Monte Carlo avec Importance Sampling\")\n",
        "    \n",
        "    monte_carlo_available = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur d'import des algorithmes Monte Carlo: {e}\")\n",
        "    print(\"Vérifiez que le module src/monte_carlo.py est accessible\")\n",
        "    monte_carlo_available = False\n",
        "\n",
        "print(\"\\n🔧 Configuration terminée !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🏗️ CRÉATION DES ADAPTATEURS MONTE CARLO\n",
            "--------------------------------------------------\n",
            "🏗️  SecretEnv0 MC-Adapter - États: 8192, Actions: 3\n",
            "✅ SecretEnv0: Adapter créé et testé (état initial: 0)\n",
            "🏗️  SecretEnv1 MC-Adapter - États: 65536, Actions: 3\n",
            "✅ SecretEnv1: Adapter créé et testé (état initial: 0)\n",
            "🏗️  SecretEnv2 MC-Adapter - États: 2097152, Actions: 3\n",
            "✅ SecretEnv2: Adapter créé et testé (état initial: 0)\n",
            "🏗️  SecretEnv3 MC-Adapter - États: 65536, Actions: 3\n",
            "✅ SecretEnv3: Adapter créé et testé (état initial: 0)\n",
            "\n",
            "🎉 4/4 adaptateurs Monte Carlo créés avec succès !\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 🔧 Adaptateur pour les Algorithmes Monte Carlo Existants\n",
        "\n",
        "class SecretEnvMCAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur spécialement conçu pour les environnements secrets afin qu'ils soient \n",
        "    compatibles avec les algorithmes Monte Carlo existants (MonteCarloES, OnPolicyMC, OffPolicyMC).\n",
        "    \n",
        "    Implémente l'interface attendue par src/monte_carlo.py\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        \n",
        "        # Obtenir les propriétés MDP depuis une instance temporaire\n",
        "        temp_env = secret_env_class()\n",
        "        self.nS = temp_env.num_states()  # Propriété attendue par Monte Carlo\n",
        "        self.nA = temp_env.num_actions() # Propriété attendue par Monte Carlo\n",
        "        \n",
        "        # État courant et environnement\n",
        "        self.current_env = None\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        print(f\"🏗️  {env_name} MC-Adapter - États: {self.nS}, Actions: {self.nA}\")\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Réinitialise l'environnement - Interface Gym standard\n",
        "        Retourne l'état initial (int pour l'environnement discret)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Créer une nouvelle instance pour chaque épisode\n",
        "            self.current_env = self.secret_env_class()\n",
        "            self.current_env.reset()\n",
        "            \n",
        "            # Obtenir l'état initial\n",
        "            self.current_state = self.current_env.state_id()\n",
        "            self.last_score = self.current_env.score()\n",
        "            self.episode_steps = 0\n",
        "            \n",
        "            return self.current_state  # Retourner directement l'état (pas de tuple)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur reset {self.env_name}: {e}\")\n",
        "            # Retourner un état par défaut en cas d'erreur\n",
        "            return 0\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Exécute une action - Interface Gym standard\n",
        "        Retourne (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                # Réinitialiser si pas d'environnement\n",
        "                self.reset()\n",
        "            \n",
        "            # Vérifier si l'action est valide dans cet environnement\n",
        "            available_actions = self._get_available_actions()\n",
        "            if action not in available_actions:\n",
        "                # Action non valide - petite pénalité mais continuer\n",
        "                return self.current_state, -0.05, False, {'invalid_action': True}\n",
        "            \n",
        "            # Sauvegarder score avant action\n",
        "            old_score = self.current_env.score()\n",
        "            \n",
        "            # Exécuter l'action\n",
        "            self.current_env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            # Calculer les résultats\n",
        "            next_state = self.current_env.state_id()\n",
        "            new_score = self.current_env.score()\n",
        "            reward = new_score - old_score  # Récompense différentielle\n",
        "            done = self.current_env.is_game_over()\n",
        "            \n",
        "            # Mettre à jour l'état\n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            # Informations supplémentaires\n",
        "            info = {\n",
        "                'available_actions': available_actions,\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps\n",
        "            }\n",
        "            \n",
        "            # Limite de sécurité pour éviter les épisodes infinis\n",
        "            if self.episode_steps > 500:  # Limite plus stricte pour les tests\n",
        "                done = True\n",
        "                reward -= 0.5  # Légère pénalité pour épisodes trop longs\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            # En cas d'erreur, terminer l'épisode avec pénalité\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def _get_available_actions(self):\n",
        "        \"\"\"Obtient les actions disponibles dans l'état courant\"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                return list(range(self.nA))\n",
        "            \n",
        "            actions = self.current_env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else list(range(self.nA))\n",
        "        except:\n",
        "            # Fallback : toutes les actions disponibles\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Informations MDP pour compatibilité - utilisé par certains algorithmes\"\"\"\n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# 🏗️ Création des adaptateurs pour les environnements secrets\n",
        "print(\"\\n🏗️ CRÉATION DES ADAPTATEURS MONTE CARLO\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "adapters = {}\n",
        "successful_adapters = 0\n",
        "\n",
        "if env_info:  # Si les environnements sont disponibles\n",
        "    for env_name, env_data in env_info.items():\n",
        "        if env_data is not None:  # Si l'environnement est fonctionnel\n",
        "            try:\n",
        "                adapter = SecretEnvMCAdapter(env_data['class'], env_name)\n",
        "                \n",
        "                # Test rapide de l'adaptateur\n",
        "                test_state = adapter.reset()\n",
        "                test_next_state, test_reward, test_done, test_info = adapter.step(0)\n",
        "                \n",
        "                adapters[env_name] = adapter\n",
        "                successful_adapters += 1\n",
        "                \n",
        "                print(f\"✅ {env_name}: Adapter créé et testé (état initial: {test_state})\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ {env_name}: Erreur creation adapter - {e}\")\n",
        "                adapters[env_name] = None\n",
        "        else:\n",
        "            print(f\"❌ {env_name}: Environnement non disponible\")\n",
        "            adapters[env_name] = None\n",
        "\n",
        "    print(f\"\\n🎉 {successful_adapters}/{len(env_info)} adaptateurs Monte Carlo créés avec succès !\")\n",
        "else:\n",
        "    print(\"❌ Aucun environnement secret disponible\")\n",
        "\n",
        "if successful_adapters == 0:\n",
        "    print(\"⚠️ Aucun adaptateur fonctionnel - L'analyse ne pourra pas continuer\")\n",
        "\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Lancement de l'analyse (peut prendre 5-15 minutes selon les paramètres)...\n",
            "\n",
            "🚀 LANCEMENT DE L'ANALYSE MONTE CARLO\n",
            "============================================================\n",
            "⚙️  Paramètres: 400 épisodes, γ=0.99\n",
            "🎮 Environnements à tester: 4\n",
            "🧠 Algorithmes: MonteCarloES, OnPolicyMC, OffPolicyMC\n",
            "============================================================\n",
            "\n",
            "🎮 ENVIRONNEMENT: SecretEnv0\n",
            "   États: 8192, Actions: 3\n",
            "--------------------------------------------------\n",
            "\\n🔥 [1/3] Monte Carlo Exploring Starts...\n",
            "   🏗️  MonteCarloES initialisé pour SecretEnv0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# 🧠 Entraînement avec les Algorithmes Monte Carlo Existants\n",
        "\n",
        "def run_monte_carlo_analysis(num_episodes=300, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Lance l'analyse complète en utilisant les algorithmes Monte Carlo existants du projet.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Nombre d'épisodes d'entraînement par algorithme\n",
        "        gamma: Facteur de discount pour tous les algorithmes\n",
        "    \n",
        "    Returns:\n",
        "        dict: Résultats complets de l'analyse\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\n🚀 LANCEMENT DE L'ANALYSE MONTE CARLO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"⚙️  Paramètres: {num_episodes} épisodes, γ={gamma}\")\n",
        "    print(f\"🎮 Environnements à tester: {len([k for k,v in adapters.items() if v is not None])}\")\n",
        "    print(f\"🧠 Algorithmes: MonteCarloES, OnPolicyMC, OffPolicyMC\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if not monte_carlo_available:\n",
        "        print(\"❌ Algorithmes Monte Carlo non disponibles !\")\n",
        "        return {}\n",
        "    \n",
        "    all_results = {}\n",
        "    total_combinations = 0\n",
        "    completed_combinations = 0\n",
        "    \n",
        "    # Compter le total de combinaisons\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is not None:\n",
        "            total_combinations += 3  # 3 algorithmes par environnement\n",
        "    \n",
        "    # Entraînement pour chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"⏭️  Skipping {env_name} (adapter non disponible)\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n🎮 ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   États: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. 🎯 Monte Carlo Exploring Starts\n",
        "        print(f\"\\\\n🔥 [1/3] Monte Carlo Exploring Starts...\")\n",
        "        try:\n",
        "            mc_es = MonteCarloES(adapter, gamma=gamma)\n",
        "            print(f\"   🏗️  MonteCarloES initialisé pour {env_name}\")\n",
        "            \n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            result_es['algorithm'] = 'MonteCarloES'\n",
        "            result_es['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter évaluation finale\n",
        "            eval_results = mc_es.evaluate(num_episodes=50)\n",
        "            result_es['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['MonteCarloES'] = result_es\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   ✅ MonteCarloES terminé - Récompense finale: {result_es['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Erreur MonteCarloES: {e}\")\n",
        "            env_results['MonteCarloES'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # 2. 🎯 On-Policy Monte Carlo  \n",
        "        print(f\"\\\\n🔄 [2/3] On-Policy Monte Carlo...\")\n",
        "        try:\n",
        "            on_policy_mc = OnPolicyMC(adapter, gamma=gamma, epsilon=0.3)\n",
        "            print(f\"   🏗️  OnPolicyMC initialisé pour {env_name} (ε=0.3)\")\n",
        "            \n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            result_on['algorithm'] = 'OnPolicyMC'\n",
        "            result_on['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter évaluation finale\n",
        "            eval_results = on_policy_mc.evaluate(num_episodes=50)\n",
        "            result_on['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['OnPolicyMC'] = result_on\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   ✅ OnPolicyMC terminé - Récompense finale: {result_on['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Erreur OnPolicyMC: {e}\")\n",
        "            env_results['OnPolicyMC'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # 3. 🎯 Off-Policy Monte Carlo\n",
        "        print(f\"\\\\n⚖️  [3/3] Off-Policy Monte Carlo...\")\n",
        "        try:\n",
        "            off_policy_mc = OffPolicyMC(adapter, gamma=gamma, epsilon=0.4)\n",
        "            print(f\"   🏗️  OffPolicyMC initialisé pour {env_name} (ε=0.4)\")\n",
        "            \n",
        "            result_off = off_policy_mc.train(num_episodes=num_episodes)\n",
        "            result_off['algorithm'] = 'OffPolicyMC'  \n",
        "            result_off['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter évaluation finale\n",
        "            eval_results = off_policy_mc.evaluate(num_episodes=50)\n",
        "            result_off['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['OffPolicyMC'] = result_off\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   ✅ OffPolicyMC terminé - Récompense finale: {result_off['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Erreur OffPolicyMC: {e}\")\n",
        "            env_results['OffPolicyMC'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # Stocker les résultats pour cet environnement\n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # Résumé pour cet environnement\n",
        "        print(f\"\\\\n📊 RÉSUMÉ {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                final_reward = result['history'][-1]['reward']\n",
        "                avg_reward = np.mean([h['reward'] for h in result['history'][-10:]])\n",
        "                print(f\"   • {alg_name}: Récompense finale = {final_reward:.3f}, Moyenne récente = {avg_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   • {alg_name}: ❌ Échec\")\n",
        "    \n",
        "    # Résumé global\n",
        "    print(f\"\\\\n🎉 ANALYSE COMPLÈTE TERMINÉE !\")\n",
        "    print(f\"📈 {completed_combinations}/{total_combinations} combinaisons réussies\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# 🚀 Lancement de l'analyse complète\n",
        "if successful_adapters > 0 and monte_carlo_available:\n",
        "    print(\"⏳ Lancement de l'analyse (peut prendre 5-15 minutes selon les paramètres)...\")\n",
        "    \n",
        "    # Paramètres d'entraînement - ajustables selon les besoins\n",
        "    EPISODES = 400  # Nombre d'épisodes par algorithme (augmentez pour plus de précision)\n",
        "    GAMMA = 0.99    # Facteur de discount\n",
        "    \n",
        "    all_results = run_monte_carlo_analysis(num_episodes=EPISODES, gamma=GAMMA)\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Impossible de lancer l'analyse :\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur d'environnement fonctionnel\")\n",
        "    if not monte_carlo_available:\n",
        "        print(\"   - Algorithmes Monte Carlo non importés\")\n",
        "    \n",
        "    all_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 Fonctions de Visualisation et d'Analyse\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les algorithmes d'un environnement\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']  # Rouge, Bleu, Vert, Orange\n",
        "    \n",
        "    # 1. Récompenses par épisode avec moyenne mobile\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            \n",
        "            # Moyenne mobile pour lisser les courbes\n",
        "            window_size = min(30, len(rewards) // 10 + 1)\n",
        "            if len(rewards) >= window_size:\n",
        "                rewards_smooth = pd.Series(rewards).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2.5)\n",
        "                ax1.plot(episodes, rewards, alpha=0.3, color=colors[i % len(colors)], linewidth=0.8)\n",
        "            else:\n",
        "                ax1.plot(episodes, rewards, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - Récompenses par Épisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Épisode')\n",
        "    ax1.set_ylabel('Récompense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes (si disponibles)\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # Vérifier si avg_q est disponible dans l'historique\n",
        "            if 'avg_q' in history[0]:\n",
        "                avg_q = [h['avg_q'] for h in history]\n",
        "                ax2.plot(episodes, avg_q, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Évolution des Q-values Moyennes', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Épisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des épisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # Vérifier le format des longueurs d'épisode\n",
        "            if 'length' in history[0]:\n",
        "                lengths = [h['length'] for h in history]\n",
        "            elif 'episode_length' in history[0]:\n",
        "                lengths = [h['episode_length'] for h in history]  \n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            window_size = min(30, len(lengths) // 10 + 1)\n",
        "            if len(lengths) >= window_size:\n",
        "                lengths_smooth = pd.Series(lengths).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "            else:\n",
        "                ax3.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des Épisodes', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Épisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Convergence - Variation des récompenses (stabilité)\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = []\n",
        "            stds = []\n",
        "            \n",
        "            window_size = 50\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            for j in range(window_size, len(history)):\n",
        "                recent_rewards = rewards[j-window_size:j]\n",
        "                episodes.append(history[j]['episode'])\n",
        "                stds.append(np.std(recent_rewards))\n",
        "            \n",
        "            if len(episodes) > 0:\n",
        "                ax4.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Stabilité (Écart-type des récompenses)', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('Épisode')\n",
        "    ax4.set_ylabel('Écart-type des récompenses')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results_detailed(all_results):\n",
        "    \"\"\"Analyse détaillée avec métriques de performance\"\"\"\n",
        "    print(\"🔍 ANALYSE DÉTAILLÉE DES RÉSULTATS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\n📊 {env_name.upper()}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # Métriques de base\n",
        "                total_episodes = len(history)\n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Performance finale (derniers 20% d'épisodes)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                final_stability = np.std(final_rewards) if len(final_rewards) > 1 else 0\n",
        "                \n",
        "                # Métriques d'évaluation si disponibles\n",
        "                eval_info = \"\"\n",
        "                if 'evaluation' in result:\n",
        "                    eval_data = result['evaluation']\n",
        "                    eval_reward = eval_data.get('avg_reward', eval_data.get('average_reward', 0))\n",
        "                    success_rate = eval_data.get('success_rate', 0)\n",
        "                    eval_info = f\", Eval: {eval_reward:.3f} (Succès: {success_rate:.1%})\"\n",
        "                \n",
        "                print(f\"\\n🎯 {alg_name}:\")\n",
        "                print(f\"   • Récompense moyenne: {avg_reward:.3f} (±{std_reward:.3f})\")\n",
        "                print(f\"   • Performance finale: {final_avg_reward:.3f}\")\n",
        "                print(f\"   • Stabilité finale: {final_stability:.3f}\")\n",
        "                print(f\"   • Épisodes total: {total_episodes}{eval_info}\")\n",
        "                \n",
        "                # Caractéristiques spécifiques aux algorithmes\n",
        "                if len(history) > 0:\n",
        "                    if 'epsilon' in history[0] and 'epsilon' in history[-1]:\n",
        "                        initial_eps = history[0]['epsilon']\n",
        "                        final_eps = history[-1]['epsilon']\n",
        "                        print(f\"   • Décroissance ε: {initial_eps:.3f} → {final_eps:.3f}\")\n",
        "                \n",
        "                # Ajouter aux données de résumé\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Récompense_Moyenne': f\"{avg_reward:.3f}\",\n",
        "                    'Récompense_Finale': f\"{final_avg_reward:.3f}\",\n",
        "                    'Stabilité': f\"{final_stability:.3f}\",\n",
        "                    'Épisodes': total_episodes\n",
        "                })\n",
        "                \n",
        "            elif 'error' in result:\n",
        "                print(f\"\\n❌ {alg_name}: Erreur - {result['error']}\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Récompense_Moyenne': \"ERREUR\",\n",
        "                    'Récompense_Finale': \"ERREUR\",\n",
        "                    'Stabilité': \"N/A\",\n",
        "                    'Épisodes': 0\n",
        "                })\n",
        "            else:\n",
        "                print(f\"\\n❌ {alg_name}: Aucune donnée valide\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Récompense_Moyenne': \"0.000\",\n",
        "                    'Récompense_Finale': \"0.000\",\n",
        "                    'Stabilité': \"N/A\",\n",
        "                    'Épisodes': 0\n",
        "                })\n",
        "    \n",
        "    # Tableau récapitulatif\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\n📋 TABLEAU RÉCAPITULATIF COMPLET:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "def plot_performance_heatmap(all_results):\n",
        "    \"\"\"Heatmap des performances finales par algorithme et environnement\"\"\"\n",
        "    \n",
        "    # Préparer les données pour la heatmap\n",
        "    env_names = list(all_results.keys())\n",
        "    alg_names = ['MonteCarloES', 'OnPolicyMC', 'OffPolicyMC']\n",
        "    \n",
        "    # Matrice des performances\n",
        "    performance_matrix = []\n",
        "    \n",
        "    for env_name in env_names:\n",
        "        env_row = []\n",
        "        for alg_name in alg_names:\n",
        "            if env_name in all_results and alg_name in all_results[env_name]:\n",
        "                result = all_results[env_name][alg_name]\n",
        "                if 'history' in result and result['history']:\n",
        "                    # Performance finale (moyenne des 20 derniers épisodes)\n",
        "                    final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                    final_performance = np.mean(final_rewards)\n",
        "                else:\n",
        "                    final_performance = 0.0\n",
        "            else:\n",
        "                final_performance = 0.0\n",
        "            \n",
        "            env_row.append(final_performance)\n",
        "        performance_matrix.append(env_row)\n",
        "    \n",
        "    # Créer la heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    heatmap = plt.imshow(performance_matrix, cmap='RdYlGn', aspect='auto')\n",
        "    \n",
        "    # Personnaliser la heatmap\n",
        "    plt.xticks(range(len(alg_names)), alg_names, rotation=45)\n",
        "    plt.yticks(range(len(env_names)), env_names)\n",
        "    plt.xlabel('Algorithmes Monte Carlo')\n",
        "    plt.ylabel('Environnements Secrets')\n",
        "    plt.title('🌡️ Heatmap des Performances Finales\\\\n(Récompense moyenne des 20 derniers épisodes)', \n",
        "              fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Ajouter les valeurs dans les cellules\n",
        "    for i in range(len(env_names)):\n",
        "        for j in range(len(alg_names)):\n",
        "            value = performance_matrix[i][j]\n",
        "            color = 'white' if abs(value) > 0.5 else 'black'\n",
        "            plt.text(j, i, f'{value:.3f}', ha='center', va='center', \n",
        "                    color=color, fontweight='bold', fontsize=11)\n",
        "    \n",
        "    plt.colorbar(heatmap, label='Performance Finale')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def generate_recommendations(all_results):\n",
        "    \"\"\"Génère des recommandations basées sur l'analyse\"\"\"\n",
        "    \n",
        "    print(\"💡 RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Trouver le meilleur algorithme par environnement\n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                # Score composite : performance finale + stabilité\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                if final_rewards:\n",
        "                    avg_performance = np.mean(final_rewards)\n",
        "                    stability = -np.std(final_rewards)  # Negative car moins de variance = mieux\n",
        "                    composite_score = avg_performance * 0.8 + stability * 0.2\n",
        "                    \n",
        "                    if composite_score > best_score:\n",
        "                        best_score = composite_score\n",
        "                        best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\n🏆 MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   • {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   • {env_name}: Aucun algorithme performant\")\n",
        "    \n",
        "    # Performance globale des algorithmes\n",
        "    alg_global_scores = {'MonteCarloES': [], 'OnPolicyMC': [], 'OffPolicyMC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history'] and alg_name in alg_global_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                avg_performance = np.mean(final_rewards) if final_rewards else 0\n",
        "                alg_global_scores[alg_name].append(avg_performance)\n",
        "    \n",
        "    print(\"\\n🌟 PERFORMANCE GLOBALE DES ALGORITHMES:\")\n",
        "    for alg_name, scores in alg_global_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   • {alg_name}: {avg_score:.3f} (±{std_score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   • {alg_name}: Aucune donnée valide\")\n",
        "    \n",
        "    # Recommandations spécifiques\n",
        "    print(\"\\n🎯 RECOMMANDATIONS SPÉCIFIQUES:\")\n",
        "    print(\"   1. 🔄 MonteCarloES excelle sur les environnements nécessitant une exploration intensive\")\n",
        "    print(\"   2. ⚖️  OnPolicyMC offre un bon équilibre exploration/exploitation\")\n",
        "    print(\"   3. 🎯 OffPolicyMC peut être instable mais performant sur certains environnements\") \n",
        "    print(\"   4. 📊 Surveillez les courbes de convergence pour détecter l'instabilité\")\n",
        "    print(\"   5. 🎛️  Ajustez γ et ε selon les caractéristiques spécifiques de chaque environnement\")\n",
        "    \n",
        "    print(\"\\n💾 Pour sauvegarder les résultats, consultez le CSV généré automatiquement.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(\"📊 Fonctions d'analyse et de visualisation définies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📈 Génération Complète des Résultats et Analyses\n",
        "\n",
        "if all_results and any(env_results for env_results in all_results.values()):\n",
        "    \n",
        "    print(\"📈 GÉNÉRATION DES ANALYSES VISUELLES COMPLÈTES\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # 1. 📊 Courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\n🎯 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        # Vérifier qu'on a au moins un résultat valide pour cet environnement\n",
        "        has_valid_results = any('history' in result and result['history'] \n",
        "                               for result in env_results.values())\n",
        "        \n",
        "        if has_valid_results:\n",
        "            print(f\"\\\\n📊 Génération des graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"❌ Pas de données valides pour {env_name}\")\n",
        "    \n",
        "    # 2. 🌡️ Heatmap comparative des performances\n",
        "    print(\"\\\\n🎯 2. HEATMAP COMPARATIVE DES PERFORMANCES\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        plot_performance_heatmap(all_results)\n",
        "        print(\"✅ Heatmap générée avec succès\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur génération heatmap: {e}\")\n",
        "    \n",
        "    # 3. 🔍 Analyse détaillée des résultats\n",
        "    print(\"\\\\n🎯 3. ANALYSE DÉTAILLÉE DES RÉSULTATS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        summary_df = analyze_results_detailed(all_results)\n",
        "        print(\"✅ Analyse détaillée terminée\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur analyse détaillée: {e}\")\n",
        "        summary_df = pd.DataFrame()\n",
        "    \n",
        "    # 4. 💡 Recommandations et conclusions\n",
        "    print(\"\\\\n🎯 4. RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        generate_recommendations(all_results)\n",
        "        print(\"✅ Recommandations générées\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur génération recommandations: {e}\")\n",
        "    \n",
        "    # 5. 💾 Sauvegarde des résultats\n",
        "    print(\"\\\\n🎯 5. SAUVEGARDE DES RÉSULTATS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        if not summary_df.empty:\n",
        "            csv_filename = 'monte_carlo_secret_env_results.csv'\n",
        "            summary_df.to_csv(csv_filename, index=False)\n",
        "            print(f\"✅ Résultats sauvegardés dans '{csv_filename}'\")\n",
        "            \n",
        "            # Sauvegarder également les données complètes\n",
        "            detailed_results = []\n",
        "            for env_name, env_results in all_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'history' in result and result['history']:\n",
        "                        for episode_data in result['history']:\n",
        "                            row = {\n",
        "                                'Environnement': env_name,\n",
        "                                'Algorithme': alg_name,\n",
        "                                **episode_data\n",
        "                            }\n",
        "                            detailed_results.append(row)\n",
        "            \n",
        "            if detailed_results:\n",
        "                detailed_df = pd.DataFrame(detailed_results)\n",
        "                detailed_csv = 'monte_carlo_detailed_history.csv'\n",
        "                detailed_df.to_csv(detailed_csv, index=False)\n",
        "                print(f\"✅ Historique détaillé sauvegardé dans '{detailed_csv}'\")\n",
        "        else:\n",
        "            print(\"❌ Aucune donnée à sauvegarder\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur sauvegarde: {e}\")\n",
        "    \n",
        "    # 6. 📊 Résumé final avec métriques clés\n",
        "    print(\"\\\\n🎯 6. RÉSUMÉ FINAL\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_combinations = 0\n",
        "    successful_combinations = 0\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            total_combinations += 1\n",
        "            if 'history' in result and result['history']:\n",
        "                successful_combinations += 1\n",
        "    \n",
        "    success_percentage = (successful_combinations / total_combinations * 100) if total_combinations > 0 else 0\n",
        "    \n",
        "    print(f\"📈 Combinaisons réussies: {successful_combinations}/{total_combinations} ({success_percentage:.1f}%)\")\n",
        "    print(f\"🎮 Environnements testés: {len(all_results)}\")\n",
        "    print(f\"🧠 Algorithmes utilisés: MonteCarloES, OnPolicyMC, OffPolicyMC\")\n",
        "    print(f\"💾 Fichiers générés: CSV avec résultats et historiques détaillés\")\n",
        "    \n",
        "    if successful_combinations > 0:\n",
        "        print(\"\\\\n🎉 ANALYSE MONTE CARLO TERMINÉE AVEC SUCCÈS !\")\n",
        "        print(\"🕵️ Les algorithmes Monte Carlo ont révélé les secrets des environnements !\")\n",
        "        \n",
        "        # Afficher quelques statistiques finales intéressantes\n",
        "        best_overall_performance = -float('inf')\n",
        "        best_combination = None\n",
        "        \n",
        "        for env_name, env_results in all_results.items():\n",
        "            for alg_name, result in env_results.items():\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_rewards = [h['reward'] for h in result['history'][-10:]]\n",
        "                    avg_final_performance = np.mean(final_rewards)\n",
        "                    \n",
        "                    if avg_final_performance > best_overall_performance:\n",
        "                        best_overall_performance = avg_final_performance\n",
        "                        best_combination = (alg_name, env_name)\n",
        "        \n",
        "        if best_combination:\n",
        "            print(f\"🏆 Meilleure combinaison globale: {best_combination[0]} sur {best_combination[1]}\")\n",
        "            print(f\"   Performance finale: {best_overall_performance:.3f}\")\n",
        "    else:\n",
        "        print(\"⚠️ Analyse terminée mais aucun résultat valide obtenu\")\n",
        "        print(\"   Vérifiez la compatibilité des environnements secrets\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ AUCUN RÉSULTAT À ANALYSER\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"🔍 Vérifications à effectuer:\")\n",
        "    print(\"   1. Les environnements secrets sont-ils accessibles ?\")\n",
        "    print(\"   2. Les adaptateurs ont-ils été créés correctement ?\") \n",
        "    print(\"   3. Les algorithmes Monte Carlo sont-ils importés ?\")\n",
        "    print(\"   4. L'entraînement s'est-il exécuté sans erreur ?\")\n",
        "    print(\"\\\\n💡 Conseil: Relancez les cellules précédentes pour diagnostiquer le problème\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"🔚 FIN DE L'ANALYSE MONTE CARLO SUR LES ENVIRONNEMENTS SECRETS\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 Fonctions de visualisation\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "    \n",
        "    # 1. Récompenses par épisode\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            if len(rewards) >= 20:\n",
        "                rewards_smooth = pd.Series(rewards).rolling(window=20, min_periods=1).mean()\n",
        "                ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i], linewidth=2)\n",
        "            else:\n",
        "                ax1.plot(episodes, rewards, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - Récompenses par Épisode')\n",
        "    ax1.set_xlabel('Épisode')\n",
        "    ax1.set_ylabel('Récompense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            avg_q = [h['avg_q'] for h in history]\n",
        "            ax2.plot(episodes, avg_q, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Évolution des Q-values')\n",
        "    ax2.set_xlabel('Épisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des épisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            lengths = [h['length'] for h in history]\n",
        "            \n",
        "            if len(lengths) >= 20:\n",
        "                lengths_smooth = pd.Series(lengths).rolling(window=20, min_periods=1).mean()\n",
        "                ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i], linewidth=2)\n",
        "            else:\n",
        "                ax3.plot(episodes, lengths, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des Épisodes')\n",
        "    ax3.set_xlabel('Épisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Taux de succès cumulé\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # Calculer taux de succès cumulé\n",
        "            success_rates = []\n",
        "            successes = 0\n",
        "            for j, h in enumerate(history):\n",
        "                if h['successful']:\n",
        "                    successes += 1\n",
        "                success_rates.append(successes / (j + 1))\n",
        "            \n",
        "            ax4.plot(episodes, success_rates, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Taux de Succès Cumulé')\n",
        "    ax4.set_xlabel('Épisode')\n",
        "    ax4.set_ylabel('Taux de Succès')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results(all_results):\n",
        "    \"\"\"Analyse détaillée des résultats\"\"\"\n",
        "    print(\"🔍 ANALYSE DÉTAILLÉE DES RÉSULTATS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\\\n📊 {env_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # Statistiques\n",
        "                total_episodes = len(history)\n",
        "                successful_episodes = sum(1 for h in history if h['successful'])\n",
        "                success_rate = successful_episodes / total_episodes\n",
        "                \n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Performance finale (derniers 20%)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                \n",
        "                print(f\"\\\\n🎯 {alg_name}:\")\n",
        "                print(f\"   • Taux de succès: {success_rate:.1%}\")\n",
        "                print(f\"   • Récompense moyenne: {avg_reward:.3f} (±{std_reward:.3f})\")\n",
        "                print(f\"   • Performance finale: {final_avg_reward:.3f}\")\n",
        "                \n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succès': f\"{success_rate:.1%}\",\n",
        "                    'Récompense_Moyenne': f\"{avg_reward:.3f}\",\n",
        "                    'Récompense_Finale': f\"{final_avg_reward:.3f}\",\n",
        "                    'Épisodes': total_episodes\n",
        "                })\n",
        "            else:\n",
        "                print(f\"\\\\n❌ {alg_name}: Aucune donnée\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succès': \"0%\",\n",
        "                    'Récompense_Moyenne': \"0.000\",\n",
        "                    'Récompense_Finale': \"0.000\",\n",
        "                    'Épisodes': 0\n",
        "                })\n",
        "    \n",
        "    # Tableau récapitulatif\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\\\n📋 TABLEAU RÉCAPITULATIF:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "print(\"📊 Fonctions de visualisation définies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 Entraînement Principal\n",
        "\n",
        "def run_monte_carlo_analysis(num_episodes=300):\n",
        "    \"\"\"Lance l'analyse complète\"\"\"\n",
        "    \n",
        "    print(\"🚀 DÉBUT DE L'ANALYSE MONTE CARLO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Paramètres: {num_episodes} épisodes par algorithme\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for env_name, adapter in adapters.items():\n",
        "        print(f\"\\\\n🎮 ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"États: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. Monte Carlo Exploring Starts\n",
        "        print(\"\\\\n🎯 Entraînement Monte Carlo ES...\")\n",
        "        try:\n",
        "            mc_es = SecretMonteCarloES(adapter, gamma=0.99, name=f\"MC-ES-{env_name}\")\n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            env_results['MC-ES'] = result_es\n",
        "            print(f\"✅ MC-ES terminé - Succès: {result_es['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur MC-ES: {e}\")\n",
        "            env_results['MC-ES'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 2. On-Policy Monte Carlo\n",
        "        print(\"\\\\n🎯 Entraînement On-Policy MC...\")\n",
        "        try:\n",
        "            on_policy_mc = SecretOnPolicyMC(adapter, gamma=0.99, epsilon=0.4, name=f\"OnPolicy-{env_name}\")\n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['On-Policy MC'] = result_on\n",
        "            print(f\"✅ On-Policy MC terminé - Succès: {result_on['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur On-Policy MC: {e}\")\n",
        "            env_results['On-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # Résumé pour cet environnement\n",
        "        print(f\"\\\\n📊 RÉSUMÉ {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                avg_final_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                print(f\"   • {alg_name}: Récompense finale = {avg_final_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   • {alg_name}: ❌ Échec\")\n",
        "    \n",
        "    print(\"\\\\n🎉 ANALYSE COMPLÈTE TERMINÉE !\")\n",
        "    return all_results\n",
        "\n",
        "# Lancer l'analyse\n",
        "if adapters:  # Seulement si les adaptateurs ont été créés\n",
        "    print(\"⏳ Lancement de l'analyse (cela peut prendre 5-10 minutes)...\")\n",
        "    EPISODES = 300  # Ajustez selon vos besoins\n",
        "    \n",
        "    all_results = run_monte_carlo_analysis(num_episodes=EPISODES)\n",
        "else:\n",
        "    print(\"❌ Impossible de lancer l'analyse - adaptateurs non disponibles\")\n",
        "    all_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📈 Affichage des Résultats\n",
        "\n",
        "if all_results:\n",
        "    print(\"📈 GÉNÉRATION DES ANALYSES VISUELLES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\\\n🎯 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        if any(result['history'] for result in env_results.values() if 'history' in result):\n",
        "            print(f\"\\\\n📊 Graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"❌ Pas de données pour {env_name}\")\n",
        "    \n",
        "    # 2. Analyse détaillée\n",
        "    print(\"\\\\n🎯 2. ANALYSE DÉTAILLÉE\")\n",
        "    print(\"-\" * 50)\n",
        "    summary_df = analyze_results(all_results)\n",
        "    \n",
        "    # 3. Recommandations\n",
        "    print(\"\\\\n🎯 3. RECOMMANDATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                \n",
        "                # Score composite\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                \n",
        "                if composite_score > best_score:\n",
        "                    best_score = composite_score\n",
        "                    best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\\\n🏆 MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   • {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   • {env_name}: Aucun algorithme efficace\")\n",
        "    \n",
        "    # 4. Performance globale\n",
        "    alg_scores = {'MC-ES': [], 'On-Policy MC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history'] and alg_name in alg_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                alg_scores[alg_name].append(composite_score)\n",
        "    \n",
        "    print(\"\\\\n🌟 PERFORMANCE GLOBALE:\")\n",
        "    for alg_name, scores in alg_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   • {alg_name}: {avg_score:.3f} (±{std_score:.3f})\")\n",
        "    \n",
        "    # 5. Conseils\n",
        "    print(\"\\\\n💡 CONSEILS D'INTERPRÉTATION:\")\n",
        "    print(\"   1. 🎯 Taux de succès élevé = algorithme stable\")\n",
        "    print(\"   2. 🔄 Récompenses croissantes = apprentissage effectif\")\n",
        "    print(\"   3. 📊 Q-values convergentes = politique stable\")\n",
        "    print(\"   4. 🎛️  Ajustez les hyperparamètres si nécessaire\")\n",
        "    \n",
        "    # Sauvegarde\n",
        "    try:\n",
        "        summary_df.to_csv('monte_carlo_results.csv', index=False)\n",
        "        print(f\"\\\\n💾 Résultats sauvegardés dans 'monte_carlo_results.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur sauvegarde: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ AUCUN RÉSULTAT À AFFICHER\")\n",
        "    print(\"Vérifiez que l'entraînement précédent s'est bien déroulé.\")\n",
        "\n",
        "print(\"\\\\n🎉 ANALYSE MONTE CARLO TERMINÉE !\")\n",
        "print(\"🕵️ Les environnements secrets ont révélé leurs mystères !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🕵️ Analyse Monte Carlo sur les Environnements Secrets\n",
        "\n",
        "## 🎯 Objectif\n",
        "Ce notebook implémente et compare les trois algorithmes Monte Carlo sur les 4 environnements secrets :\n",
        "- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**\n",
        "\n",
        "## 📋 Algorithmes Monte Carlo testés\n",
        "- **Monte Carlo Exploring Starts (MC-ES)** : Garantit l'exploration avec starts aléatoires\n",
        "- **On-Policy Monte Carlo** : Amélioration de politique ε-greedy on-policy  \n",
        "- **Off-Policy Monte Carlo** : Apprentissage avec importance sampling\n",
        "\n",
        "## 📊 Analyses effectuées\n",
        "- Courbes d'apprentissage (récompenses, Q-values)\n",
        "- Convergence des politiques\n",
        "- Comparaison des performances entre algorithmes\n",
        "- Analyse de la stabilité d'apprentissage\n",
        "- Métriques de succès par environnement\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📚 Imports et configuration\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (14, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Ajouter les chemins nécessaires\n",
        "project_root = os.path.abspath('../../')\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, os.path.join(project_root, 'game', 'secret_env'))\n",
        "\n",
        "# Imports des environnements secrets\n",
        "try:\n",
        "    from secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
        "    print(\"✅ Environnements secrets importés avec succès\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur d'import des environnements secrets: {e}\")\n",
        "    # Fallback pour les imports\n",
        "    import ctypes\n",
        "    import platform\n",
        "    \n",
        "print(\"🔧 Configuration terminée !\")\n",
        "\n",
        "# Test rapide des environnements\n",
        "try:\n",
        "    env0 = SecretEnv0()\n",
        "    print(f\"📊 SecretEnv0 - États: {env0.num_states()}, Actions: {env0.num_actions()}\")\n",
        "    \n",
        "    env1 = SecretEnv1()\n",
        "    print(f\"📊 SecretEnv1 - États: {env1.num_states()}, Actions: {env1.num_actions()}\")\n",
        "    \n",
        "    env2 = SecretEnv2()\n",
        "    print(f\"📊 SecretEnv2 - États: {env2.num_states()}, Actions: {env2.num_actions()}\")\n",
        "    \n",
        "    env3 = SecretEnv3()\n",
        "    print(f\"📊 SecretEnv3 - États: {env3.num_states()}, Actions: {env3.num_actions()}\")\n",
        "    \n",
        "    print(\"\\n🎉 Tous les environnements secrets sont fonctionnels !\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur lors du test des environnements: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 Adaptateur d'environnement pour les Secret Envs\n",
        "\n",
        "class SecretEnvAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur pour rendre les SecretEnv compatibles avec l'API Gym standard.\n",
        "    Transforme l'interface spécifique des environnements secrets en interface standard.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        self.env = secret_env_class()\n",
        "        \n",
        "        # Propriétés MDP pour compatibilité avec Monte Carlo\n",
        "        self.nS = self.env.num_states()\n",
        "        self.nA = self.env.num_actions()\n",
        "        \n",
        "        # État et récompenses\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        print(f\"🏗️  {env_name} adapter créé - États: {self.nS}, Actions: {self.nA}\")\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Réinitialise l'environnement et retourne l'état initial\"\"\"\n",
        "        try:\n",
        "            self.env.reset()\n",
        "            self.current_state = self.env.state_id()\n",
        "            self.last_score = self.env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur reset {self.env_name}: {e}\")\n",
        "            # Créer un nouvel environnement si reset échoue\n",
        "            self.env = self.secret_env_class()\n",
        "            self.env.reset()\n",
        "            self.current_state = self.env.state_id()\n",
        "            self.last_score = self.env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Exécute une action et retourne (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Obtenir les actions disponibles\n",
        "            available_actions = self.get_available_actions()\n",
        "            \n",
        "            # Vérifier si l'action est valide\n",
        "            if action not in available_actions:\n",
        "                # Action non valide - retourner récompense négative et rester dans l'état\n",
        "                return self.current_state, -0.1, False, {\n",
        "                    'invalid_action': True,\n",
        "                    'available_actions': available_actions,\n",
        "                    'requested_action': action\n",
        "                }\n",
        "            \n",
        "            # Sauvegarder le score avant l'action\n",
        "            old_score = self.env.score()\n",
        "            \n",
        "            # Exécuter l'action\n",
        "            self.env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            # Obtenir le nouvel état et calculer la récompense\n",
        "            next_state = self.env.state_id()\n",
        "            new_score = self.env.score()\n",
        "            reward = new_score - old_score  # Récompense différentielle\n",
        "            done = self.env.is_game_over()\n",
        "            \n",
        "            # Mise à jour\n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            info = {\n",
        "                'available_actions': self.get_available_actions(),\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps,\n",
        "                'valid_action': True\n",
        "            }\n",
        "            \n",
        "            # Limite de sécurité pour éviter les épisodes infinis\n",
        "            if self.episode_steps > 1000:\n",
        "                done = True\n",
        "                reward -= 1.0  # Pénalité pour épisode trop long\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur step {self.env_name}: {e}\")\n",
        "            # Retourner un état d'erreur\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Obtient la liste des actions disponibles dans l'état courant\"\"\"\n",
        "        try:\n",
        "            actions = self.env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else [0]\n",
        "        except:\n",
        "            # Fallback : toutes les actions sont disponibles\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    def display(self):\n",
        "        \"\"\"Affiche l'état courant de l'environnement\"\"\"\n",
        "        try:\n",
        "            self.env.display()\n",
        "        except:\n",
        "            print(f\"État courant: {self.current_state}, Score: {self.last_score}\")\n",
        "    \n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Retourne les informations MDP pour compatibilité\"\"\"\n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# Test des adaptateurs\n",
        "print(\"🧪 Test des adaptateurs...\")\n",
        "adapters = {}\n",
        "\n",
        "try:\n",
        "    adapters['SecretEnv0'] = SecretEnvAdapter(SecretEnv0, \"SecretEnv0\")\n",
        "    adapters['SecretEnv1'] = SecretEnvAdapter(SecretEnv1, \"SecretEnv1\")\n",
        "    adapters['SecretEnv2'] = SecretEnvAdapter(SecretEnv2, \"SecretEnv2\")\n",
        "    adapters['SecretEnv3'] = SecretEnvAdapter(SecretEnv3, \"SecretEnv3\")\n",
        "    \n",
        "    print(\"\\n✅ Tous les adaptateurs créés avec succès !\")\n",
        "    \n",
        "    # Test rapide d'un adaptateur\n",
        "    test_adapter = adapters['SecretEnv0']\n",
        "    state = test_adapter.reset()\n",
        "    available = test_adapter.get_available_actions()\n",
        "    print(f\"🔍 Test SecretEnv0 - État initial: {state}, Actions disponibles: {available}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur lors de la création des adaptateurs: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎮 Implémentation des Algorithmes Monte Carlo pour les Environnements Secrets\n",
        "\n",
        "class SecretMonteCarloES:\n",
        "    \"\"\"Monte Carlo avec Exploring Starts adapté aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, name=\"MC-ES\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))  # Initialisation aléatoire\n",
        "        self.policy = np.zeros(self.nS, dtype=int)\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        \n",
        "        # Historique d'entraînement\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"🎯 {name} initialisé pour {env_adapter.env_name}\")\n",
        "    \n",
        "    def generate_episode_with_exploring_starts(self):\n",
        "        \"\"\"Génère un épisode avec exploring starts\"\"\"\n",
        "        episode = []\n",
        "        \n",
        "        # Reset avec état aléatoire (approximation d'exploring starts)\n",
        "        for _ in range(10):  # Essayer plusieurs resets pour varier l'état initial\n",
        "            state = self.env_adapter.reset()\n",
        "            if np.random.random() < 0.3:  # 30% chance d'accepter cet état\n",
        "                break\n",
        "        \n",
        "        # Action initiale aléatoire (exploring starts)\n",
        "        available_actions = self.env_adapter.get_available_actions()\n",
        "        if len(available_actions) > 0:\n",
        "            action = np.random.choice(available_actions)\n",
        "        else:\n",
        "            action = 0\n",
        "        \n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            # Action suivante selon politique courante avec actions disponibles\n",
        "            state = next_state\n",
        "            available_actions = info.get('available_actions', list(range(self.nA)))\n",
        "            \n",
        "            if len(available_actions) > 0:\n",
        "                # Politique greedy avec tie-breaking aléatoire sur actions disponibles\n",
        "                q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "                max_q = np.max(q_vals)\n",
        "                best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "                action = np.random.choice(best_actions)\n",
        "            else:\n",
        "                break\n",
        "            \n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"Entraînement Monte Carlo ES\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                # Générer épisode avec exploring starts\n",
        "                episode = self.generate_episode_with_exploring_starts()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Mise à jour First-Visit Monte Carlo\n",
        "                    G = 0.0\n",
        "                    visited = set()\n",
        "                    \n",
        "                    for (state, action, reward) in reversed(episode):\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        if (state, action) not in visited:\n",
        "                            visited.add((state, action))\n",
        "                            self.returns_count[(state, action)] += 1\n",
        "                            self.returns_sum[(state, action)] += G\n",
        "                            self.Q[state, action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
        "                    \n",
        "                    # Amélioration de la politique (greedy)\n",
        "                    for s in range(self.nS):\n",
        "                        self.policy[s] = np.argmax(self.Q[s])\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    # Épisode échoué\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] Épisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succès: {success_rate:.2f}, \"\n",
        "                          f\"Récompense récente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Erreur épisode {episode_num + 1}: {e}\")\n",
        "                self.history.append({\n",
        "                    'episode': episode_num + 1,\n",
        "                    'reward': 0.0,\n",
        "                    'length': 0,\n",
        "                    'avg_q': np.mean(self.Q),\n",
        "                    'successful': False\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "class SecretOnPolicyMC:\n",
        "    \"\"\"On-Policy Monte Carlo avec ε-greedy adapté aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, epsilon=0.3, name=\"On-Policy MC\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_epsilon = epsilon\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))\n",
        "        self.policy = np.zeros(self.nS, dtype=int)\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        \n",
        "        # Historique\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"🎯 {name} initialisé pour {env_adapter.env_name} (ε={epsilon})\")\n",
        "    \n",
        "    def epsilon_greedy_action(self, state, available_actions):\n",
        "        \"\"\"Sélectionne une action selon ε-greedy parmi les actions disponibles\"\"\"\n",
        "        if len(available_actions) == 0:\n",
        "            return 0\n",
        "        \n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            # Greedy : meilleure action parmi les disponibles\n",
        "            q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "            max_q = np.max(q_vals)\n",
        "            best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "    \n",
        "    def generate_episode(self):\n",
        "        \"\"\"Génère un épisode selon la politique ε-greedy\"\"\"\n",
        "        episode = []\n",
        "        state = self.env_adapter.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            available_actions = self.env_adapter.get_available_actions()\n",
        "            action = self.epsilon_greedy_action(state, available_actions)\n",
        "            \n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"Entraînement On-Policy Monte Carlo\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                episode = self.generate_episode()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Mise à jour First-Visit Monte Carlo\n",
        "                    G = 0.0\n",
        "                    visited = set()\n",
        "                    \n",
        "                    for (state, action, reward) in reversed(episode):\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        if (state, action) not in visited:\n",
        "                            visited.add((state, action))\n",
        "                            self.returns_count[(state, action)] += 1\n",
        "                            self.returns_sum[(state, action)] += G\n",
        "                            self.Q[state, action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
        "                    \n",
        "                    # Amélioration de politique\n",
        "                    for s in range(self.nS):\n",
        "                        self.policy[s] = np.argmax(self.Q[s])\n",
        "                    \n",
        "                    # Décroissance d'epsilon\n",
        "                    self.epsilon = max(0.01, self.epsilon * 0.9995)\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'epsilon': self.epsilon,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'epsilon': self.epsilon,\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] Épisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succès: {success_rate:.2f}, \"\n",
        "                          f\"ε: {self.epsilon:.3f}, \"\n",
        "                          f\"Récompense récente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Erreur épisode {episode_num + 1}: {e}\")\n",
        "                \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "class SecretOffPolicyMC:\n",
        "    \"\"\"Off-Policy Monte Carlo avec Importance Sampling adapté aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, epsilon=0.4, name=\"Off-Policy MC\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))\n",
        "        self.target_policy = np.zeros(self.nS, dtype=int)\n",
        "        self.C = np.zeros((self.nS, self.nA))  # Poids cumulatifs\n",
        "        \n",
        "        # Historique\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"🎯 {name} initialisé pour {env_adapter.env_name} (ε={epsilon})\")\n",
        "    \n",
        "    def behavior_policy(self, state, available_actions):\n",
        "        \"\"\"Politique de comportement ε-greedy\"\"\"\n",
        "        if len(available_actions) == 0:\n",
        "            return 0\n",
        "            \n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "            max_q = np.max(q_vals)\n",
        "            best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "    \n",
        "    def generate_episode(self):\n",
        "        \"\"\"Génère un épisode selon la politique de comportement\"\"\"\n",
        "        episode = []\n",
        "        state = self.env_adapter.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            available_actions = self.env_adapter.get_available_actions()\n",
        "            action = self.behavior_policy(state, available_actions)\n",
        "            \n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward, available_actions.copy()))\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"Entraînement Off-Policy Monte Carlo avec Importance Sampling\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                episode = self.generate_episode()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Importance Sampling Update\n",
        "                    G = 0.0\n",
        "                    W = 1.0\n",
        "                    \n",
        "                    for i in range(len(episode) - 1, -1, -1):\n",
        "                        state, action, reward, available_actions = episode[i]\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        # Mettre à jour C et Q\n",
        "                        self.C[state, action] += W\n",
        "                        if self.C[state, action] > 0:\n",
        "                            self.Q[state, action] += (W / self.C[state, action]) * (G - self.Q[state, action])\n",
        "                        \n",
        "                        # Mettre à jour la politique cible (greedy)\n",
        "                        self.target_policy[state] = np.argmax(self.Q[state])\n",
        "                        \n",
        "                        # Vérifier si l'action est celle de la politique cible\n",
        "                        if action != self.target_policy[state]:\n",
        "                            break\n",
        "                        \n",
        "                        # Calculer le ratio d'importance\n",
        "                        # Probabilité politique cible (déterministe)\n",
        "                        target_prob = 1.0\n",
        "                        \n",
        "                        # Probabilité politique de comportement\n",
        "                        if len(available_actions) > 0:\n",
        "                            if action == np.argmax([self.Q[state, a] for a in available_actions]):\n",
        "                                behavior_prob = 1.0 - self.epsilon + self.epsilon / len(available_actions)\n",
        "                            else:\n",
        "                                behavior_prob = self.epsilon / len(available_actions)\n",
        "                        else:\n",
        "                            behavior_prob = 1.0\n",
        "                        \n",
        "                        if behavior_prob > 0:\n",
        "                            W *= target_prob / behavior_prob\n",
        "                        else:\n",
        "                            break\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r, _ in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'avg_weight': np.mean(self.C[self.C > 0]) if np.any(self.C > 0) else 0,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'avg_weight': 0,\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] Épisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succès: {success_rate:.2f}, \"\n",
        "                          f\"Récompense récente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Erreur épisode {episode_num + 1}: {e}\")\n",
        "                \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.target_policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "print(\"🎯 Algorithmes Monte Carlo définis avec succès !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 Fonctions de visualisation et d'analyse\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les algorithmes\"\"\"\n",
        "    \n",
        "    n_algorithms = len(results_dict)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Couleurs pour chaque algorithme\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    # 1. Récompenses par épisode\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        rewards = [h['reward'] for h in history]\n",
        "        \n",
        "        # Moyenne mobile pour lisser les courbes\n",
        "        window_size = min(50, len(rewards) // 10 + 1)\n",
        "        if len(rewards) >= window_size:\n",
        "            rewards_smooth = pd.Series(rewards).rolling(window=window_size, min_periods=1).mean()\n",
        "            ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "        else:\n",
        "            ax1.plot(episodes, rewards, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - Récompenses par Épisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Épisode')\n",
        "    ax1.set_ylabel('Récompense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        avg_q = [h['avg_q'] for h in history]\n",
        "        \n",
        "        ax2.plot(episodes, avg_q, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Évolution des Q-values', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Épisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des épisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        lengths = [h['length'] for h in history]\n",
        "        \n",
        "        # Moyenne mobile\n",
        "        window_size = min(50, len(lengths) // 10 + 1)\n",
        "        if len(lengths) >= window_size:\n",
        "            lengths_smooth = pd.Series(lengths).rolling(window=window_size, min_periods=1).mean()\n",
        "            ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "        else:\n",
        "            ax3.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des Épisodes', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Épisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Analyse de convergence (écart-type des récompenses récentes)\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = []\n",
        "        stds = []\n",
        "        \n",
        "        window_size = 100\n",
        "        for j in range(window_size, len(history)):\n",
        "            recent_rewards = [h['reward'] for h in history[j-window_size:j]]\n",
        "            episodes.append(history[j]['episode'])\n",
        "            stds.append(np.std(recent_rewards))\n",
        "        \n",
        "        if len(episodes) > 0:\n",
        "            ax4.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Stabilité (Écart-type des récompenses)', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('Épisode')\n",
        "    ax4.set_ylabel('Écart-type des récompenses')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_performance_comparison(all_results):\n",
        "    \"\"\"Compare les performances finales de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    # Préparer les données pour la visualisation\n",
        "    env_names = list(all_results.keys())\n",
        "    alg_names = list(list(all_results.values())[0].keys())\n",
        "    \n",
        "    # Metrics à analyser\n",
        "    final_rewards = []\n",
        "    success_rates = []\n",
        "    avg_q_values = []\n",
        "    \n",
        "    for env_name in env_names:\n",
        "        env_rewards = []\n",
        "        env_success_rates = []\n",
        "        env_avg_q = []\n",
        "        \n",
        "        for alg_name in alg_names:\n",
        "            result = all_results[env_name][alg_name]\n",
        "            \n",
        "            # Récompense finale (moyenne des 100 derniers épisodes)\n",
        "            history = result['history']\n",
        "            if len(history) >= 100:\n",
        "                final_reward = np.mean([h['reward'] for h in history[-100:]])\n",
        "            else:\n",
        "                final_reward = np.mean([h['reward'] for h in history]) if history else 0\n",
        "            \n",
        "            env_rewards.append(final_reward)\n",
        "            env_success_rates.append(result.get('success_rate', 0))\n",
        "            env_avg_q.append(result['history'][-1]['avg_q'] if result['history'] else 0)\n",
        "        \n",
        "        final_rewards.append(env_rewards)\n",
        "        success_rates.append(env_success_rates)\n",
        "        avg_q_values.append(env_avg_q)\n",
        "    \n",
        "    # Créer les graphiques\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Heatmap des récompenses finales\n",
        "    ax1 = axes[0, 0]\n",
        "    im1 = ax1.imshow(final_rewards, cmap='RdYlGn', aspect='auto')\n",
        "    ax1.set_xticks(range(len(alg_names)))\n",
        "    ax1.set_xticklabels(alg_names, rotation=45)\n",
        "    ax1.set_yticks(range(len(env_names)))\n",
        "    ax1.set_yticklabels(env_names)\n",
        "    ax1.set_title('Récompenses Finales par Algorithme et Environnement')\n",
        "    \n",
        "    # Ajouter les valeurs dans les cellules\n",
        "    for i in range(len(env_names)):\n",
        "        for j in range(len(alg_names)):\n",
        "            ax1.text(j, i, f'{final_rewards[i][j]:.2f}', ha='center', va='center')\n",
        "    \n",
        "    plt.colorbar(im1, ax=ax1)\n",
        "    \n",
        "    # 2. Graphique en barres des taux de succès\n",
        "    ax2 = axes[0, 1]\n",
        "    x = np.arange(len(env_names))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        success_data = [success_rates[j][i] for j in range(len(env_names))]\n",
        "        ax2.bar(x + i * width, success_data, width, label=alg_name, alpha=0.8)\n",
        "    \n",
        "    ax2.set_xlabel('Environnements')\n",
        "    ax2.set_ylabel('Taux de Succès')\n",
        "    ax2.set_title('Taux de Succès par Environnement et Algorithme')\n",
        "    ax2.set_xticks(x + width * 1.5)\n",
        "    ax2.set_xticklabels(env_names)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Comparaison des Q-values moyennes finales\n",
        "    ax3 = axes[1, 0]\n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        q_data = [avg_q_values[j][i] for j in range(len(env_names))]\n",
        "        ax3.bar(x + i * width, q_data, width, label=alg_name, alpha=0.8)\n",
        "    \n",
        "    ax3.set_xlabel('Environnements')\n",
        "    ax3.set_ylabel('Q-value Moyenne Finale')\n",
        "    ax3.set_title('Q-values Finales par Environnement et Algorithme')\n",
        "    ax3.set_xticks(x + width * 1.5)\n",
        "    ax3.set_xticklabels(env_names)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Graphique radar des performances générales\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    # Normaliser les métriques pour le radar\n",
        "    final_rewards_norm = np.array(final_rewards)\n",
        "    success_rates_norm = np.array(success_rates)\n",
        "    \n",
        "    # Score composite pour chaque algorithme\n",
        "    composite_scores = []\n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        alg_rewards = [final_rewards[j][i] for j in range(len(env_names))]\n",
        "        alg_success = [success_rates[j][i] for j in range(len(env_names))]\n",
        "        \n",
        "        # Score composite (moyenne pondérée)\n",
        "        composite_score = np.mean(alg_rewards) * 0.7 + np.mean(alg_success) * 0.3\n",
        "        composite_scores.append(composite_score)\n",
        "    \n",
        "    bars = ax4.bar(alg_names, composite_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(alg_names)])\n",
        "    ax4.set_title('Score Composite Global par Algorithme')\n",
        "    ax4.set_ylabel('Score Composite')\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for bar, score in zip(bars, composite_scores):\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_algorithm_characteristics(all_results):\n",
        "    \"\"\"Analyse les caractéristiques spécifiques de chaque algorithme\"\"\"\n",
        "    \n",
        "    print(\"🔍 ANALYSE DÉTAILLÉE DES ALGORITHMES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\n📊 {env_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            history = result['history']\n",
        "            \n",
        "            if len(history) > 0:\n",
        "                # Statistiques générales\n",
        "                total_episodes = len(history)\n",
        "                successful_episodes = sum(1 for h in history if h['successful'])\n",
        "                success_rate = successful_episodes / total_episodes\n",
        "                \n",
        "                # Récompenses\n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Convergence (derniers 20% d'épisodes)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                \n",
        "                # Stabilité (écart-type des derniers épisodes)\n",
        "                final_stability = np.std(final_rewards) if len(final_rewards) > 1 else 0\n",
        "                \n",
        "                print(f\"\\n🎯 {alg_name}:\")\n",
        "                print(f\"   • Taux de succès: {success_rate:.1%}\")\n",
        "                print(f\"   • Récompense moyenne: {avg_reward:.3f} (±{std_reward:.3f})\")\n",
        "                print(f\"   • Performance finale: {final_avg_reward:.3f}\")\n",
        "                print(f\"   • Stabilité finale: {final_stability:.3f}\")\n",
        "                \n",
        "                # Caractéristiques spécifiques à l'algorithme\n",
        "                if 'epsilon' in history[0]:\n",
        "                    initial_eps = history[0]['epsilon']\n",
        "                    final_eps = history[-1]['epsilon']\n",
        "                    print(f\"   • Décroissance ε: {initial_eps:.3f} → {final_eps:.3f}\")\n",
        "                \n",
        "                if 'avg_weight' in history[0]:\n",
        "                    final_weight = history[-1]['avg_weight']\n",
        "                    print(f\"   • Poids moyen final: {final_weight:.3f}\")\n",
        "            else:\n",
        "                print(f\"\\n❌ {alg_name}: Aucune donnée d'entraînement\")\n",
        "\n",
        "print(\"📊 Fonctions de visualisation définies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 Entraînement Principal - Tous les Algorithmes sur Tous les Environnements\n",
        "\n",
        "def run_complete_analysis(num_episodes=1000):\n",
        "    \"\"\"Lance l'analyse complète de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"🚀 DÉBUT DE L'ANALYSE COMPLÈTE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Paramètres: {num_episodes} épisodes par algorithme\")\n",
        "    print(f\"Total: {4} environnements × {3} algorithmes = {12} entraînements\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Dictionnaire pour stocker tous les résultats\n",
        "    all_results = {}\n",
        "    \n",
        "    # Environnements à tester\n",
        "    env_classes = {\n",
        "        'SecretEnv0': SecretEnv0,\n",
        "        'SecretEnv1': SecretEnv1, \n",
        "        'SecretEnv2': SecretEnv2,\n",
        "        'SecretEnv3': SecretEnv3\n",
        "    }\n",
        "    \n",
        "    # Créer les adaptateurs\n",
        "    adapters_dict = {}\n",
        "    for env_name, env_class in env_classes.items():\n",
        "        try:\n",
        "            adapters_dict[env_name] = SecretEnvAdapter(env_class, env_name)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur création adaptateur {env_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n✅ {len(adapters_dict)} adaptateurs créés avec succès\")\n",
        "    \n",
        "    # Entraînement pour chaque environnement\n",
        "    for env_name, adapter in adapters_dict.items():\n",
        "        print(f\"\\n🎮 ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"États: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. Monte Carlo Exploring Starts\n",
        "        print(\"\\\\n🎯 Entraînement Monte Carlo ES...\")\n",
        "        try:\n",
        "            mc_es = SecretMonteCarloES(adapter, gamma=0.99, name=f\"MC-ES-{env_name}\")\n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            env_results['MC-ES'] = result_es\n",
        "            print(f\"✅ MC-ES terminé - Taux de succès: {result_es['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur MC-ES sur {env_name}: {e}\")\n",
        "            env_results['MC-ES'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 2. On-Policy Monte Carlo\n",
        "        print(\"\\\\n🎯 Entraînement On-Policy MC...\")\n",
        "        try:\n",
        "            on_policy_mc = SecretOnPolicyMC(adapter, gamma=0.99, epsilon=0.3, name=f\"OnPolicy-{env_name}\")\n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['On-Policy MC'] = result_on\n",
        "            print(f\"✅ On-Policy MC terminé - Taux de succès: {result_on['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur On-Policy MC sur {env_name}: {e}\")\n",
        "            env_results['On-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 3. Off-Policy Monte Carlo\n",
        "        print(\"\\\\n🎯 Entraînement Off-Policy MC...\")\n",
        "        try:\n",
        "            off_policy_mc = SecretOffPolicyMC(adapter, gamma=0.99, epsilon=0.4, name=f\"OffPolicy-{env_name}\")\n",
        "            result_off = off_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['Off-Policy MC'] = result_off\n",
        "            print(f\"✅ Off-Policy MC terminé - Taux de succès: {result_off['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur Off-Policy MC sur {env_name}: {e}\")\n",
        "            env_results['Off-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # Stocker les résultats de cet environnement\n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # Afficher un résumé pour cet environnement\n",
        "        print(f\"\\\\n📊 RÉSUMÉ {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_final_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                print(f\"   • {alg_name}: Récompense finale = {avg_final_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   • {alg_name}: ❌ Aucun résultat\")\n",
        "    \n",
        "    print(\"\\\\n🎉 ANALYSE COMPLÈTE TERMINÉE !\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Lancer l'analyse complète (peut prendre plusieurs minutes)\n",
        "print(\"⏳ Lancement de l'analyse complète...\")\n",
        "print(\"Cela peut prendre plusieurs minutes selon la complexité des environnements...\")\n",
        "\n",
        "# Utiliser un nombre d'épisodes raisonnable pour le test\n",
        "EPISODES = 800  # Ajustez selon vos besoins de temps\n",
        "\n",
        "all_results = run_complete_analysis(num_episodes=EPISODES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📈 Affichage des Résultats et Analyses Complètes\n",
        "\n",
        "print(\"📈 GÉNÉRATION DES ANALYSES VISUELLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Vérifier qu'on a des résultats\n",
        "if all_results and any(env_results for env_results in all_results.values()):\n",
        "    \n",
        "    # 1. Afficher les courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\\\n🎯 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        if any(result['history'] for result in env_results.values()):\n",
        "            print(f\"\\\\n📊 Graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"❌ Pas de données valides pour {env_name}\")\n",
        "    \n",
        "    # 2. Comparaison des performances entre environnements\n",
        "    print(\"\\\\n🎯 2. COMPARAISON GLOBALE DES PERFORMANCES\")\n",
        "    print(\"-\" * 50)\n",
        "    plot_performance_comparison(all_results)\n",
        "    \n",
        "    # 3. Analyse détaillée des caractéristiques\n",
        "    print(\"\\\\n🎯 3. ANALYSE DÉTAILLÉE\")\n",
        "    print(\"-\" * 50)\n",
        "    analyze_algorithm_characteristics(all_results)\n",
        "    \n",
        "    # 4. Tableau récapitulatif final\n",
        "    print(\"\\\\n🎯 4. TABLEAU RÉCAPITULATIF FINAL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Créer un DataFrame pour le résumé\n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                # Calculer les métriques finales\n",
        "                history = result['history']\n",
        "                final_rewards = [h['reward'] for h in history[-100:]] if len(history) >= 100 else [h['reward'] for h in history]\n",
        "                \n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succès': f\"{result['success_rate']:.1%}\",\n",
        "                    'Récompense_Finale': f\"{np.mean(final_rewards):.3f}\",\n",
        "                    'Stabilité': f\"{np.std(final_rewards):.3f}\",\n",
        "                    'Épisodes_Total': len(history),\n",
        "                    'Q_Moyenne_Finale': f\"{history[-1]['avg_q']:.3f}\" if history else \"0.000\"\n",
        "                })\n",
        "            else:\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succès': \"0.0%\",\n",
        "                    'Récompense_Finale': \"0.000\",\n",
        "                    'Stabilité': \"N/A\",\n",
        "                    'Épisodes_Total': 0,\n",
        "                    'Q_Moyenne_Finale': \"0.000\"\n",
        "                })\n",
        "    \n",
        "    # Afficher le tableau\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\\\n📋 Résultats par Algorithme et Environnement:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    # 5. Recommandations finales\n",
        "    print(\"\\\\n🎯 5. RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Trouver les meilleurs algorithmes par environnement\n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                # Score composite basé sur récompense finale et taux de succès\n",
        "                final_rewards = [h['reward'] for h in result['history'][-100:]] if len(result['history']) >= 100 else [h['reward'] for h in result['history']]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                \n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                \n",
        "                if composite_score > best_score:\n",
        "                    best_score = composite_score\n",
        "                    best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\\\n🏆 MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   • {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   • {env_name}: Aucun algorithme efficace\")\n",
        "    \n",
        "    # Analyse globale\n",
        "    alg_global_scores = {'MC-ES': [], 'On-Policy MC': [], 'Off-Policy MC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history'] and alg_name in alg_global_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-100:]] if len(result['history']) >= 100 else [h['reward'] for h in result['history']]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                alg_global_scores[alg_name].append(composite_score)\n",
        "    \n",
        "    print(\"\\\\n🌟 PERFORMANCE GLOBALE DES ALGORITHMES:\")\n",
        "    for alg_name, scores in alg_global_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   • {alg_name}: {avg_score:.3f} (±{std_score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   • {alg_name}: Aucune donnée valide\")\n",
        "    \n",
        "    # Recommandations spécifiques\n",
        "    print(\"\\\\n💡 RECOMMANDATIONS:\")\n",
        "    print(\"   1. 🎯 Chaque environnement secret semble avoir des caractéristiques uniques\")\n",
        "    print(\"   2. 🔄 L'exploration est cruciale - MC-ES peut être avantagé\")\n",
        "    print(\"   3. 📊 Surveillez les taux de succès autant que les récompenses\")\n",
        "    print(\"   4. ⚖️  L'importance sampling (Off-Policy) peut être instable sur certains environnements\")\n",
        "    print(\"   5. 🎛️  L'ajustement des hyperparamètres (ε, γ) est critique\")\n",
        "    \n",
        "    # Sauvegarde des résultats\n",
        "    try:\n",
        "        summary_df.to_csv('secret_env_monte_carlo_results.csv', index=False)\n",
        "        print(f\"\\\\n💾 Résultats sauvegardés dans 'secret_env_monte_carlo_results.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur lors de la sauvegarde: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ AUCUN RÉSULTAT VALIDE TROUVÉ\")\n",
        "    print(\"Vérifiez que les environnements secrets sont accessibles et fonctionnels.\")\n",
        "\n",
        "print(\"\\\\n🎉 ANALYSE MONTE CARLO TERMINÉE !\")\n",
        "print(\"🕵️ Les mystères des environnements secrets ont été explorés par Monte Carlo !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# 📖 Guide d'Utilisation du Notebook\n",
        "\n",
        "## 🚀 Exécution\n",
        "1. **Exécutez les cellules dans l'ordre** (Shift+Enter pour chaque cellule)\n",
        "2. La **cellule 1** teste les imports et environnements\n",
        "3. Les **cellules 2-4** définissent les classes et fonctions\n",
        "4. La **cellule 5** lance l'entraînement complet (⏳ 5-15 minutes selon la machine)\n",
        "5. La **cellule 6** affiche tous les résultats et graphiques\n",
        "\n",
        "## 🎛️ Paramètres Ajustables\n",
        "```python\n",
        "# Dans la cellule 5, modifiez cette ligne :\n",
        "EPISODES = 800  # Nombre d'épisodes par algorithme\n",
        "\n",
        "# Paramètres des algorithmes (dans leurs constructeurs) :\n",
        "# - gamma: Facteur de discount (0.99 par défaut)\n",
        "# - epsilon: Taux d'exploration (0.3-0.4 par défaut)\n",
        "```\n",
        "\n",
        "## 📊 Outputs Générés\n",
        "- **Courbes d'apprentissage** : Récompenses, Q-values, longueur épisodes, stabilité\n",
        "- **Comparaisons globales** : Heatmaps, barres comparatives, scores composites\n",
        "- **Analyses détaillées** : Métriques par algorithme et environnement\n",
        "- **Tableau récapitulatif** : Summary CSV exportable\n",
        "- **Recommandations** : Meilleurs algorithmes par environnement\n",
        "\n",
        "## 🐛 Dépannage\n",
        "- **Erreur d'import** : Vérifiez que les libs natives sont présentes dans `./libs/`\n",
        "- **Erreur de plateforme** : Le wrapper détecte automatiquement Windows/Linux/macOS\n",
        "- **Performance lente** : Réduisez `EPISODES` pour des tests rapides\n",
        "- **Manque de mémoire** : Fermez d'autres applications\n",
        "\n",
        "## 📈 Interprétation des Résultats\n",
        "- **Taux de succès élevé** : L'algorithme génère des épisodes valides\n",
        "- **Récompenses croissantes** : L'apprentissage fonctionne\n",
        "- **Q-values qui convergent** : La politique se stabilise\n",
        "- **Faible stabilité** : Plus d'exploration nécessaire\n",
        "\n",
        "## 🔧 Modifications Avancées\n",
        "Pour adapter à d'autres environnements secrets :\n",
        "1. Ajoutez la classe d'environnement dans `env_classes`\n",
        "2. Créez l'adaptateur correspondant\n",
        "3. Les algorithmes s'adaptent automatiquement\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 Bon apprentissage avec Monte Carlo sur les environnements secrets !**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
