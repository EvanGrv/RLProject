{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ•µï¸ Analyse Monte Carlo sur les Environnements Secrets\n",
        "\n",
        "## ğŸ¯ Objectif\n",
        "Ce notebook utilise les **algorithmes Monte Carlo existants** du projet (situÃ©s dans `src/monte_carlo.py`) pour analyser les **4 environnements secrets** :\n",
        "- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**\n",
        "\n",
        "## ğŸ“‹ Algorithmes Monte Carlo utilisÃ©s (depuis `src/`)\n",
        "- **`MonteCarloES`** : Monte Carlo avec Exploring Starts\n",
        "- **`OnPolicyMC`** : On-policy First-Visit Monte Carlo avec Îµ-greedy  \n",
        "- **`OffPolicyMC`** : Off-policy Monte Carlo avec Importance Sampling\n",
        "\n",
        "## ğŸ“Š Analyses complÃ¨tes effectuÃ©es\n",
        "- **Adaptateur** pour interfacer les environnements secrets avec l'API attendue\n",
        "- **EntraÃ®nement** des 3 algorithmes sur les 4 environnements (12 combinaisons)\n",
        "- **Courbes d'apprentissage** (rÃ©compenses, Q-values, convergence)\n",
        "- **Ã‰valuations** de performance et comparaisons dÃ©taillÃ©es\n",
        "- **MÃ©triques** de succÃ¨s, stabilitÃ© et recommandations par environnement\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ INITIALISATION DE L'ANALYSE MONTE CARLO\n",
            "============================================================\n",
            "âœ… Environnements secrets importÃ©s avec succÃ¨s\n",
            "ğŸ“Š SecretEnv0 - Ã‰tats: 8192, Actions: 3\n",
            "ğŸ“Š SecretEnv1 - Ã‰tats: 65536, Actions: 3\n",
            "ğŸ“Š SecretEnv2 - Ã‰tats: 2097152, Actions: 3\n",
            "ğŸ“Š SecretEnv3 - Ã‰tats: 65536, Actions: 3\n",
            "\n",
            "ğŸ‰ 4 environnements secrets fonctionnels !\n",
            "âœ… Algorithmes Monte Carlo importÃ©s depuis src/monte_carlo.py\n",
            "   â€¢ MonteCarloES - Monte Carlo avec Exploring Starts\n",
            "   â€¢ OnPolicyMC - On-policy First-Visit Monte Carlo\n",
            "   â€¢ OffPolicyMC - Off-policy Monte Carlo avec Importance Sampling\n",
            "\n",
            "ğŸ”§ Configuration terminÃ©e !\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“š Imports et Configuration ComplÃ¨te\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib pour de beaux graphiques\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# ğŸ”§ Configuration des chemins - Ajouter le projet et les modules\n",
        "project_root = os.path.abspath('../../')\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
        "sys.path.insert(0, os.path.join(project_root, 'game', 'secret_env'))\n",
        "\n",
        "print(\"ğŸš€ INITIALISATION DE L'ANALYSE MONTE CARLO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ® Import des environnements secrets\n",
        "try:\n",
        "    from secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
        "    print(\"âœ… Environnements secrets importÃ©s avec succÃ¨s\")\n",
        "    \n",
        "    # Test rapide des environnements\n",
        "    env_configs = [\n",
        "        (\"SecretEnv0\", SecretEnv0),\n",
        "        (\"SecretEnv1\", SecretEnv1), \n",
        "        (\"SecretEnv2\", SecretEnv2),\n",
        "        (\"SecretEnv3\", SecretEnv3)\n",
        "    ]\n",
        "    \n",
        "    env_info = {}\n",
        "    for env_name, env_class in env_configs:\n",
        "        try:\n",
        "            env = env_class()\n",
        "            states = env.num_states()\n",
        "            actions = env.num_actions()\n",
        "            env_info[env_name] = {'states': states, 'actions': actions, 'class': env_class}\n",
        "            print(f\"ğŸ“Š {env_name} - Ã‰tats: {states}, Actions: {actions}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur avec {env_name}: {e}\")\n",
        "            env_info[env_name] = None\n",
        "    \n",
        "    print(f\"\\nğŸ‰ {len([k for k,v in env_info.items() if v is not None])} environnements secrets fonctionnels !\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur d'import des environnements secrets: {e}\")\n",
        "    print(\"VÃ©rifiez que les bibliothÃ¨ques natives sont prÃ©sentes dans ./libs/\")\n",
        "    env_info = {}\n",
        "\n",
        "# ğŸ§  Import des algorithmes Monte Carlo existants du projet\n",
        "try:\n",
        "    from monte_carlo import MonteCarloES, OnPolicyMC, OffPolicyMC\n",
        "    print(\"âœ… Algorithmes Monte Carlo importÃ©s depuis src/monte_carlo.py\")\n",
        "    print(\"   â€¢ MonteCarloES - Monte Carlo avec Exploring Starts\")\n",
        "    print(\"   â€¢ OnPolicyMC - On-policy First-Visit Monte Carlo\")  \n",
        "    print(\"   â€¢ OffPolicyMC - Off-policy Monte Carlo avec Importance Sampling\")\n",
        "    \n",
        "    monte_carlo_available = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur d'import des algorithmes Monte Carlo: {e}\")\n",
        "    print(\"VÃ©rifiez que le module src/monte_carlo.py est accessible\")\n",
        "    monte_carlo_available = False\n",
        "\n",
        "print(\"\\nğŸ”§ Configuration terminÃ©e !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ—ï¸ CRÃ‰ATION DES ADAPTATEURS MONTE CARLO\n",
            "--------------------------------------------------\n",
            "ğŸ—ï¸  SecretEnv0 MC-Adapter - Ã‰tats: 8192, Actions: 3\n",
            "âœ… SecretEnv0: Adapter crÃ©Ã© et testÃ© (Ã©tat initial: 0)\n",
            "ğŸ—ï¸  SecretEnv1 MC-Adapter - Ã‰tats: 65536, Actions: 3\n",
            "âœ… SecretEnv1: Adapter crÃ©Ã© et testÃ© (Ã©tat initial: 0)\n",
            "ğŸ—ï¸  SecretEnv2 MC-Adapter - Ã‰tats: 2097152, Actions: 3\n",
            "âœ… SecretEnv2: Adapter crÃ©Ã© et testÃ© (Ã©tat initial: 0)\n",
            "ğŸ—ï¸  SecretEnv3 MC-Adapter - Ã‰tats: 65536, Actions: 3\n",
            "âœ… SecretEnv3: Adapter crÃ©Ã© et testÃ© (Ã©tat initial: 0)\n",
            "\n",
            "ğŸ‰ 4/4 adaptateurs Monte Carlo crÃ©Ã©s avec succÃ¨s !\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”§ Adaptateur pour les Algorithmes Monte Carlo Existants\n",
        "\n",
        "class SecretEnvMCAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur spÃ©cialement conÃ§u pour les environnements secrets afin qu'ils soient \n",
        "    compatibles avec les algorithmes Monte Carlo existants (MonteCarloES, OnPolicyMC, OffPolicyMC).\n",
        "    \n",
        "    ImplÃ©mente l'interface attendue par src/monte_carlo.py\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        \n",
        "        # Obtenir les propriÃ©tÃ©s MDP depuis une instance temporaire\n",
        "        temp_env = secret_env_class()\n",
        "        self.nS = temp_env.num_states()  # PropriÃ©tÃ© attendue par Monte Carlo\n",
        "        self.nA = temp_env.num_actions() # PropriÃ©tÃ© attendue par Monte Carlo\n",
        "        \n",
        "        # Ã‰tat courant et environnement\n",
        "        self.current_env = None\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        print(f\"ğŸ—ï¸  {env_name} MC-Adapter - Ã‰tats: {self.nS}, Actions: {self.nA}\")\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        RÃ©initialise l'environnement - Interface Gym standard\n",
        "        Retourne l'Ã©tat initial (int pour l'environnement discret)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # CrÃ©er une nouvelle instance pour chaque Ã©pisode\n",
        "            self.current_env = self.secret_env_class()\n",
        "            self.current_env.reset()\n",
        "            \n",
        "            # Obtenir l'Ã©tat initial\n",
        "            self.current_state = self.current_env.state_id()\n",
        "            self.last_score = self.current_env.score()\n",
        "            self.episode_steps = 0\n",
        "            \n",
        "            return self.current_state  # Retourner directement l'Ã©tat (pas de tuple)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur reset {self.env_name}: {e}\")\n",
        "            # Retourner un Ã©tat par dÃ©faut en cas d'erreur\n",
        "            return 0\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        ExÃ©cute une action - Interface Gym standard\n",
        "        Retourne (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                # RÃ©initialiser si pas d'environnement\n",
        "                self.reset()\n",
        "            \n",
        "            # VÃ©rifier si l'action est valide dans cet environnement\n",
        "            available_actions = self._get_available_actions()\n",
        "            if action not in available_actions:\n",
        "                # Action non valide - petite pÃ©nalitÃ© mais continuer\n",
        "                return self.current_state, -0.05, False, {'invalid_action': True}\n",
        "            \n",
        "            # Sauvegarder score avant action\n",
        "            old_score = self.current_env.score()\n",
        "            \n",
        "            # ExÃ©cuter l'action\n",
        "            self.current_env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            # Calculer les rÃ©sultats\n",
        "            next_state = self.current_env.state_id()\n",
        "            new_score = self.current_env.score()\n",
        "            reward = new_score - old_score  # RÃ©compense diffÃ©rentielle\n",
        "            done = self.current_env.is_game_over()\n",
        "            \n",
        "            # Mettre Ã  jour l'Ã©tat\n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            # Informations supplÃ©mentaires\n",
        "            info = {\n",
        "                'available_actions': available_actions,\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps\n",
        "            }\n",
        "            \n",
        "            # Limite de sÃ©curitÃ© pour Ã©viter les Ã©pisodes infinis\n",
        "            if self.episode_steps > 500:  # Limite plus stricte pour les tests\n",
        "                done = True\n",
        "                reward -= 0.5  # LÃ©gÃ¨re pÃ©nalitÃ© pour Ã©pisodes trop longs\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            # En cas d'erreur, terminer l'Ã©pisode avec pÃ©nalitÃ©\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def _get_available_actions(self):\n",
        "        \"\"\"Obtient les actions disponibles dans l'Ã©tat courant\"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                return list(range(self.nA))\n",
        "            \n",
        "            actions = self.current_env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else list(range(self.nA))\n",
        "        except:\n",
        "            # Fallback : toutes les actions disponibles\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Informations MDP pour compatibilitÃ© - utilisÃ© par certains algorithmes\"\"\"\n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# ğŸ—ï¸ CrÃ©ation des adaptateurs pour les environnements secrets\n",
        "print(\"\\nğŸ—ï¸ CRÃ‰ATION DES ADAPTATEURS MONTE CARLO\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "adapters = {}\n",
        "successful_adapters = 0\n",
        "\n",
        "if env_info:  # Si les environnements sont disponibles\n",
        "    for env_name, env_data in env_info.items():\n",
        "        if env_data is not None:  # Si l'environnement est fonctionnel\n",
        "            try:\n",
        "                adapter = SecretEnvMCAdapter(env_data['class'], env_name)\n",
        "                \n",
        "                # Test rapide de l'adaptateur\n",
        "                test_state = adapter.reset()\n",
        "                test_next_state, test_reward, test_done, test_info = adapter.step(0)\n",
        "                \n",
        "                adapters[env_name] = adapter\n",
        "                successful_adapters += 1\n",
        "                \n",
        "                print(f\"âœ… {env_name}: Adapter crÃ©Ã© et testÃ© (Ã©tat initial: {test_state})\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ {env_name}: Erreur creation adapter - {e}\")\n",
        "                adapters[env_name] = None\n",
        "        else:\n",
        "            print(f\"âŒ {env_name}: Environnement non disponible\")\n",
        "            adapters[env_name] = None\n",
        "\n",
        "    print(f\"\\nğŸ‰ {successful_adapters}/{len(env_info)} adaptateurs Monte Carlo crÃ©Ã©s avec succÃ¨s !\")\n",
        "else:\n",
        "    print(\"âŒ Aucun environnement secret disponible\")\n",
        "\n",
        "if successful_adapters == 0:\n",
        "    print(\"âš ï¸ Aucun adaptateur fonctionnel - L'analyse ne pourra pas continuer\")\n",
        "\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ Lancement de l'analyse (peut prendre 5-15 minutes selon les paramÃ¨tres)...\n",
            "\n",
            "ğŸš€ LANCEMENT DE L'ANALYSE MONTE CARLO\n",
            "============================================================\n",
            "âš™ï¸  ParamÃ¨tres: 400 Ã©pisodes, Î³=0.99\n",
            "ğŸ® Environnements Ã  tester: 4\n",
            "ğŸ§  Algorithmes: MonteCarloES, OnPolicyMC, OffPolicyMC\n",
            "============================================================\n",
            "\n",
            "ğŸ® ENVIRONNEMENT: SecretEnv0\n",
            "   Ã‰tats: 8192, Actions: 3\n",
            "--------------------------------------------------\n",
            "\\nğŸ”¥ [1/3] Monte Carlo Exploring Starts...\n",
            "   ğŸ—ï¸  MonteCarloES initialisÃ© pour SecretEnv0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ğŸ§  EntraÃ®nement avec les Algorithmes Monte Carlo Existants\n",
        "\n",
        "def run_monte_carlo_analysis(num_episodes=300, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Lance l'analyse complÃ¨te en utilisant les algorithmes Monte Carlo existants du projet.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Nombre d'Ã©pisodes d'entraÃ®nement par algorithme\n",
        "        gamma: Facteur de discount pour tous les algorithmes\n",
        "    \n",
        "    Returns:\n",
        "        dict: RÃ©sultats complets de l'analyse\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\nğŸš€ LANCEMENT DE L'ANALYSE MONTE CARLO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"âš™ï¸  ParamÃ¨tres: {num_episodes} Ã©pisodes, Î³={gamma}\")\n",
        "    print(f\"ğŸ® Environnements Ã  tester: {len([k for k,v in adapters.items() if v is not None])}\")\n",
        "    print(f\"ğŸ§  Algorithmes: MonteCarloES, OnPolicyMC, OffPolicyMC\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if not monte_carlo_available:\n",
        "        print(\"âŒ Algorithmes Monte Carlo non disponibles !\")\n",
        "        return {}\n",
        "    \n",
        "    all_results = {}\n",
        "    total_combinations = 0\n",
        "    completed_combinations = 0\n",
        "    \n",
        "    # Compter le total de combinaisons\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is not None:\n",
        "            total_combinations += 3  # 3 algorithmes par environnement\n",
        "    \n",
        "    # EntraÃ®nement pour chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"â­ï¸  Skipping {env_name} (adapter non disponible)\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nğŸ® ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   Ã‰tats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. ğŸ¯ Monte Carlo Exploring Starts\n",
        "        print(f\"\\\\nğŸ”¥ [1/3] Monte Carlo Exploring Starts...\")\n",
        "        try:\n",
        "            mc_es = MonteCarloES(adapter, gamma=gamma)\n",
        "            print(f\"   ğŸ—ï¸  MonteCarloES initialisÃ© pour {env_name}\")\n",
        "            \n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            result_es['algorithm'] = 'MonteCarloES'\n",
        "            result_es['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter Ã©valuation finale\n",
        "            eval_results = mc_es.evaluate(num_episodes=50)\n",
        "            result_es['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['MonteCarloES'] = result_es\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   âœ… MonteCarloES terminÃ© - RÃ©compense finale: {result_es['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Erreur MonteCarloES: {e}\")\n",
        "            env_results['MonteCarloES'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # 2. ğŸ¯ On-Policy Monte Carlo  \n",
        "        print(f\"\\\\nğŸ”„ [2/3] On-Policy Monte Carlo...\")\n",
        "        try:\n",
        "            on_policy_mc = OnPolicyMC(adapter, gamma=gamma, epsilon=0.3)\n",
        "            print(f\"   ğŸ—ï¸  OnPolicyMC initialisÃ© pour {env_name} (Îµ=0.3)\")\n",
        "            \n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            result_on['algorithm'] = 'OnPolicyMC'\n",
        "            result_on['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter Ã©valuation finale\n",
        "            eval_results = on_policy_mc.evaluate(num_episodes=50)\n",
        "            result_on['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['OnPolicyMC'] = result_on\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   âœ… OnPolicyMC terminÃ© - RÃ©compense finale: {result_on['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Erreur OnPolicyMC: {e}\")\n",
        "            env_results['OnPolicyMC'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # 3. ğŸ¯ Off-Policy Monte Carlo\n",
        "        print(f\"\\\\nâš–ï¸  [3/3] Off-Policy Monte Carlo...\")\n",
        "        try:\n",
        "            off_policy_mc = OffPolicyMC(adapter, gamma=gamma, epsilon=0.4)\n",
        "            print(f\"   ğŸ—ï¸  OffPolicyMC initialisÃ© pour {env_name} (Îµ=0.4)\")\n",
        "            \n",
        "            result_off = off_policy_mc.train(num_episodes=num_episodes)\n",
        "            result_off['algorithm'] = 'OffPolicyMC'  \n",
        "            result_off['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter Ã©valuation finale\n",
        "            eval_results = off_policy_mc.evaluate(num_episodes=50)\n",
        "            result_off['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['OffPolicyMC'] = result_off\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   âœ… OffPolicyMC terminÃ© - RÃ©compense finale: {result_off['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Erreur OffPolicyMC: {e}\")\n",
        "            env_results['OffPolicyMC'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # Stocker les rÃ©sultats pour cet environnement\n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # RÃ©sumÃ© pour cet environnement\n",
        "        print(f\"\\\\nğŸ“Š RÃ‰SUMÃ‰ {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                final_reward = result['history'][-1]['reward']\n",
        "                avg_reward = np.mean([h['reward'] for h in result['history'][-10:]])\n",
        "                print(f\"   â€¢ {alg_name}: RÃ©compense finale = {final_reward:.3f}, Moyenne rÃ©cente = {avg_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   â€¢ {alg_name}: âŒ Ã‰chec\")\n",
        "    \n",
        "    # RÃ©sumÃ© global\n",
        "    print(f\"\\\\nğŸ‰ ANALYSE COMPLÃˆTE TERMINÃ‰E !\")\n",
        "    print(f\"ğŸ“ˆ {completed_combinations}/{total_combinations} combinaisons rÃ©ussies\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# ğŸš€ Lancement de l'analyse complÃ¨te\n",
        "if successful_adapters > 0 and monte_carlo_available:\n",
        "    print(\"â³ Lancement de l'analyse (peut prendre 5-15 minutes selon les paramÃ¨tres)...\")\n",
        "    \n",
        "    # ParamÃ¨tres d'entraÃ®nement - ajustables selon les besoins\n",
        "    EPISODES = 400  # Nombre d'Ã©pisodes par algorithme (augmentez pour plus de prÃ©cision)\n",
        "    GAMMA = 0.99    # Facteur de discount\n",
        "    \n",
        "    all_results = run_monte_carlo_analysis(num_episodes=EPISODES, gamma=GAMMA)\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ Impossible de lancer l'analyse :\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur d'environnement fonctionnel\")\n",
        "    if not monte_carlo_available:\n",
        "        print(\"   - Algorithmes Monte Carlo non importÃ©s\")\n",
        "    \n",
        "    all_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š Fonctions de Visualisation et d'Analyse\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les algorithmes d'un environnement\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']  # Rouge, Bleu, Vert, Orange\n",
        "    \n",
        "    # 1. RÃ©compenses par Ã©pisode avec moyenne mobile\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            \n",
        "            # Moyenne mobile pour lisser les courbes\n",
        "            window_size = min(30, len(rewards) // 10 + 1)\n",
        "            if len(rewards) >= window_size:\n",
        "                rewards_smooth = pd.Series(rewards).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2.5)\n",
        "                ax1.plot(episodes, rewards, alpha=0.3, color=colors[i % len(colors)], linewidth=0.8)\n",
        "            else:\n",
        "                ax1.plot(episodes, rewards, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - RÃ©compenses par Ã‰pisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Ã‰pisode')\n",
        "    ax1.set_ylabel('RÃ©compense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes (si disponibles)\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # VÃ©rifier si avg_q est disponible dans l'historique\n",
        "            if 'avg_q' in history[0]:\n",
        "                avg_q = [h['avg_q'] for h in history]\n",
        "                ax2.plot(episodes, avg_q, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Ã‰volution des Q-values Moyennes', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Ã‰pisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des Ã©pisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # VÃ©rifier le format des longueurs d'Ã©pisode\n",
        "            if 'length' in history[0]:\n",
        "                lengths = [h['length'] for h in history]\n",
        "            elif 'episode_length' in history[0]:\n",
        "                lengths = [h['episode_length'] for h in history]  \n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            window_size = min(30, len(lengths) // 10 + 1)\n",
        "            if len(lengths) >= window_size:\n",
        "                lengths_smooth = pd.Series(lengths).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "            else:\n",
        "                ax3.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des Ã‰pisodes', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Ã‰pisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Convergence - Variation des rÃ©compenses (stabilitÃ©)\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = []\n",
        "            stds = []\n",
        "            \n",
        "            window_size = 50\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            for j in range(window_size, len(history)):\n",
        "                recent_rewards = rewards[j-window_size:j]\n",
        "                episodes.append(history[j]['episode'])\n",
        "                stds.append(np.std(recent_rewards))\n",
        "            \n",
        "            if len(episodes) > 0:\n",
        "                ax4.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - StabilitÃ© (Ã‰cart-type des rÃ©compenses)', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('Ã‰pisode')\n",
        "    ax4.set_ylabel('Ã‰cart-type des rÃ©compenses')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results_detailed(all_results):\n",
        "    \"\"\"Analyse dÃ©taillÃ©e avec mÃ©triques de performance\"\"\"\n",
        "    print(\"ğŸ” ANALYSE DÃ‰TAILLÃ‰E DES RÃ‰SULTATS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\nğŸ“Š {env_name.upper()}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # MÃ©triques de base\n",
        "                total_episodes = len(history)\n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Performance finale (derniers 20% d'Ã©pisodes)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                final_stability = np.std(final_rewards) if len(final_rewards) > 1 else 0\n",
        "                \n",
        "                # MÃ©triques d'Ã©valuation si disponibles\n",
        "                eval_info = \"\"\n",
        "                if 'evaluation' in result:\n",
        "                    eval_data = result['evaluation']\n",
        "                    eval_reward = eval_data.get('avg_reward', eval_data.get('average_reward', 0))\n",
        "                    success_rate = eval_data.get('success_rate', 0)\n",
        "                    eval_info = f\", Eval: {eval_reward:.3f} (SuccÃ¨s: {success_rate:.1%})\"\n",
        "                \n",
        "                print(f\"\\nğŸ¯ {alg_name}:\")\n",
        "                print(f\"   â€¢ RÃ©compense moyenne: {avg_reward:.3f} (Â±{std_reward:.3f})\")\n",
        "                print(f\"   â€¢ Performance finale: {final_avg_reward:.3f}\")\n",
        "                print(f\"   â€¢ StabilitÃ© finale: {final_stability:.3f}\")\n",
        "                print(f\"   â€¢ Ã‰pisodes total: {total_episodes}{eval_info}\")\n",
        "                \n",
        "                # CaractÃ©ristiques spÃ©cifiques aux algorithmes\n",
        "                if len(history) > 0:\n",
        "                    if 'epsilon' in history[0] and 'epsilon' in history[-1]:\n",
        "                        initial_eps = history[0]['epsilon']\n",
        "                        final_eps = history[-1]['epsilon']\n",
        "                        print(f\"   â€¢ DÃ©croissance Îµ: {initial_eps:.3f} â†’ {final_eps:.3f}\")\n",
        "                \n",
        "                # Ajouter aux donnÃ©es de rÃ©sumÃ©\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'RÃ©compense_Moyenne': f\"{avg_reward:.3f}\",\n",
        "                    'RÃ©compense_Finale': f\"{final_avg_reward:.3f}\",\n",
        "                    'StabilitÃ©': f\"{final_stability:.3f}\",\n",
        "                    'Ã‰pisodes': total_episodes\n",
        "                })\n",
        "                \n",
        "            elif 'error' in result:\n",
        "                print(f\"\\nâŒ {alg_name}: Erreur - {result['error']}\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'RÃ©compense_Moyenne': \"ERREUR\",\n",
        "                    'RÃ©compense_Finale': \"ERREUR\",\n",
        "                    'StabilitÃ©': \"N/A\",\n",
        "                    'Ã‰pisodes': 0\n",
        "                })\n",
        "            else:\n",
        "                print(f\"\\nâŒ {alg_name}: Aucune donnÃ©e valide\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'RÃ©compense_Moyenne': \"0.000\",\n",
        "                    'RÃ©compense_Finale': \"0.000\",\n",
        "                    'StabilitÃ©': \"N/A\",\n",
        "                    'Ã‰pisodes': 0\n",
        "                })\n",
        "    \n",
        "    # Tableau rÃ©capitulatif\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\nğŸ“‹ TABLEAU RÃ‰CAPITULATIF COMPLET:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "def plot_performance_heatmap(all_results):\n",
        "    \"\"\"Heatmap des performances finales par algorithme et environnement\"\"\"\n",
        "    \n",
        "    # PrÃ©parer les donnÃ©es pour la heatmap\n",
        "    env_names = list(all_results.keys())\n",
        "    alg_names = ['MonteCarloES', 'OnPolicyMC', 'OffPolicyMC']\n",
        "    \n",
        "    # Matrice des performances\n",
        "    performance_matrix = []\n",
        "    \n",
        "    for env_name in env_names:\n",
        "        env_row = []\n",
        "        for alg_name in alg_names:\n",
        "            if env_name in all_results and alg_name in all_results[env_name]:\n",
        "                result = all_results[env_name][alg_name]\n",
        "                if 'history' in result and result['history']:\n",
        "                    # Performance finale (moyenne des 20 derniers Ã©pisodes)\n",
        "                    final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                    final_performance = np.mean(final_rewards)\n",
        "                else:\n",
        "                    final_performance = 0.0\n",
        "            else:\n",
        "                final_performance = 0.0\n",
        "            \n",
        "            env_row.append(final_performance)\n",
        "        performance_matrix.append(env_row)\n",
        "    \n",
        "    # CrÃ©er la heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    heatmap = plt.imshow(performance_matrix, cmap='RdYlGn', aspect='auto')\n",
        "    \n",
        "    # Personnaliser la heatmap\n",
        "    plt.xticks(range(len(alg_names)), alg_names, rotation=45)\n",
        "    plt.yticks(range(len(env_names)), env_names)\n",
        "    plt.xlabel('Algorithmes Monte Carlo')\n",
        "    plt.ylabel('Environnements Secrets')\n",
        "    plt.title('ğŸŒ¡ï¸ Heatmap des Performances Finales\\\\n(RÃ©compense moyenne des 20 derniers Ã©pisodes)', \n",
        "              fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Ajouter les valeurs dans les cellules\n",
        "    for i in range(len(env_names)):\n",
        "        for j in range(len(alg_names)):\n",
        "            value = performance_matrix[i][j]\n",
        "            color = 'white' if abs(value) > 0.5 else 'black'\n",
        "            plt.text(j, i, f'{value:.3f}', ha='center', va='center', \n",
        "                    color=color, fontweight='bold', fontsize=11)\n",
        "    \n",
        "    plt.colorbar(heatmap, label='Performance Finale')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def generate_recommendations(all_results):\n",
        "    \"\"\"GÃ©nÃ¨re des recommandations basÃ©es sur l'analyse\"\"\"\n",
        "    \n",
        "    print(\"ğŸ’¡ RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Trouver le meilleur algorithme par environnement\n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                # Score composite : performance finale + stabilitÃ©\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                if final_rewards:\n",
        "                    avg_performance = np.mean(final_rewards)\n",
        "                    stability = -np.std(final_rewards)  # Negative car moins de variance = mieux\n",
        "                    composite_score = avg_performance * 0.8 + stability * 0.2\n",
        "                    \n",
        "                    if composite_score > best_score:\n",
        "                        best_score = composite_score\n",
        "                        best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\nğŸ† MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   â€¢ {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   â€¢ {env_name}: Aucun algorithme performant\")\n",
        "    \n",
        "    # Performance globale des algorithmes\n",
        "    alg_global_scores = {'MonteCarloES': [], 'OnPolicyMC': [], 'OffPolicyMC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history'] and alg_name in alg_global_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                avg_performance = np.mean(final_rewards) if final_rewards else 0\n",
        "                alg_global_scores[alg_name].append(avg_performance)\n",
        "    \n",
        "    print(\"\\nğŸŒŸ PERFORMANCE GLOBALE DES ALGORITHMES:\")\n",
        "    for alg_name, scores in alg_global_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   â€¢ {alg_name}: {avg_score:.3f} (Â±{std_score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   â€¢ {alg_name}: Aucune donnÃ©e valide\")\n",
        "    \n",
        "    # Recommandations spÃ©cifiques\n",
        "    print(\"\\nğŸ¯ RECOMMANDATIONS SPÃ‰CIFIQUES:\")\n",
        "    print(\"   1. ğŸ”„ MonteCarloES excelle sur les environnements nÃ©cessitant une exploration intensive\")\n",
        "    print(\"   2. âš–ï¸  OnPolicyMC offre un bon Ã©quilibre exploration/exploitation\")\n",
        "    print(\"   3. ğŸ¯ OffPolicyMC peut Ãªtre instable mais performant sur certains environnements\") \n",
        "    print(\"   4. ğŸ“Š Surveillez les courbes de convergence pour dÃ©tecter l'instabilitÃ©\")\n",
        "    print(\"   5. ğŸ›ï¸  Ajustez Î³ et Îµ selon les caractÃ©ristiques spÃ©cifiques de chaque environnement\")\n",
        "    \n",
        "    print(\"\\nğŸ’¾ Pour sauvegarder les rÃ©sultats, consultez le CSV gÃ©nÃ©rÃ© automatiquement.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(\"ğŸ“Š Fonctions d'analyse et de visualisation dÃ©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ˆ GÃ©nÃ©ration ComplÃ¨te des RÃ©sultats et Analyses\n",
        "\n",
        "if all_results and any(env_results for env_results in all_results.values()):\n",
        "    \n",
        "    print(\"ğŸ“ˆ GÃ‰NÃ‰RATION DES ANALYSES VISUELLES COMPLÃˆTES\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # 1. ğŸ“Š Courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\nğŸ¯ 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        # VÃ©rifier qu'on a au moins un rÃ©sultat valide pour cet environnement\n",
        "        has_valid_results = any('history' in result and result['history'] \n",
        "                               for result in env_results.values())\n",
        "        \n",
        "        if has_valid_results:\n",
        "            print(f\"\\\\nğŸ“Š GÃ©nÃ©ration des graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"âŒ Pas de donnÃ©es valides pour {env_name}\")\n",
        "    \n",
        "    # 2. ğŸŒ¡ï¸ Heatmap comparative des performances\n",
        "    print(\"\\\\nğŸ¯ 2. HEATMAP COMPARATIVE DES PERFORMANCES\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        plot_performance_heatmap(all_results)\n",
        "        print(\"âœ… Heatmap gÃ©nÃ©rÃ©e avec succÃ¨s\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur gÃ©nÃ©ration heatmap: {e}\")\n",
        "    \n",
        "    # 3. ğŸ” Analyse dÃ©taillÃ©e des rÃ©sultats\n",
        "    print(\"\\\\nğŸ¯ 3. ANALYSE DÃ‰TAILLÃ‰E DES RÃ‰SULTATS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        summary_df = analyze_results_detailed(all_results)\n",
        "        print(\"âœ… Analyse dÃ©taillÃ©e terminÃ©e\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur analyse dÃ©taillÃ©e: {e}\")\n",
        "        summary_df = pd.DataFrame()\n",
        "    \n",
        "    # 4. ğŸ’¡ Recommandations et conclusions\n",
        "    print(\"\\\\nğŸ¯ 4. RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        generate_recommendations(all_results)\n",
        "        print(\"âœ… Recommandations gÃ©nÃ©rÃ©es\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur gÃ©nÃ©ration recommandations: {e}\")\n",
        "    \n",
        "    # 5. ğŸ’¾ Sauvegarde des rÃ©sultats\n",
        "    print(\"\\\\nğŸ¯ 5. SAUVEGARDE DES RÃ‰SULTATS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        if not summary_df.empty:\n",
        "            csv_filename = 'monte_carlo_secret_env_results.csv'\n",
        "            summary_df.to_csv(csv_filename, index=False)\n",
        "            print(f\"âœ… RÃ©sultats sauvegardÃ©s dans '{csv_filename}'\")\n",
        "            \n",
        "            # Sauvegarder Ã©galement les donnÃ©es complÃ¨tes\n",
        "            detailed_results = []\n",
        "            for env_name, env_results in all_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'history' in result and result['history']:\n",
        "                        for episode_data in result['history']:\n",
        "                            row = {\n",
        "                                'Environnement': env_name,\n",
        "                                'Algorithme': alg_name,\n",
        "                                **episode_data\n",
        "                            }\n",
        "                            detailed_results.append(row)\n",
        "            \n",
        "            if detailed_results:\n",
        "                detailed_df = pd.DataFrame(detailed_results)\n",
        "                detailed_csv = 'monte_carlo_detailed_history.csv'\n",
        "                detailed_df.to_csv(detailed_csv, index=False)\n",
        "                print(f\"âœ… Historique dÃ©taillÃ© sauvegardÃ© dans '{detailed_csv}'\")\n",
        "        else:\n",
        "            print(\"âŒ Aucune donnÃ©e Ã  sauvegarder\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur sauvegarde: {e}\")\n",
        "    \n",
        "    # 6. ğŸ“Š RÃ©sumÃ© final avec mÃ©triques clÃ©s\n",
        "    print(\"\\\\nğŸ¯ 6. RÃ‰SUMÃ‰ FINAL\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_combinations = 0\n",
        "    successful_combinations = 0\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            total_combinations += 1\n",
        "            if 'history' in result and result['history']:\n",
        "                successful_combinations += 1\n",
        "    \n",
        "    success_percentage = (successful_combinations / total_combinations * 100) if total_combinations > 0 else 0\n",
        "    \n",
        "    print(f\"ğŸ“ˆ Combinaisons rÃ©ussies: {successful_combinations}/{total_combinations} ({success_percentage:.1f}%)\")\n",
        "    print(f\"ğŸ® Environnements testÃ©s: {len(all_results)}\")\n",
        "    print(f\"ğŸ§  Algorithmes utilisÃ©s: MonteCarloES, OnPolicyMC, OffPolicyMC\")\n",
        "    print(f\"ğŸ’¾ Fichiers gÃ©nÃ©rÃ©s: CSV avec rÃ©sultats et historiques dÃ©taillÃ©s\")\n",
        "    \n",
        "    if successful_combinations > 0:\n",
        "        print(\"\\\\nğŸ‰ ANALYSE MONTE CARLO TERMINÃ‰E AVEC SUCCÃˆS !\")\n",
        "        print(\"ğŸ•µï¸ Les algorithmes Monte Carlo ont rÃ©vÃ©lÃ© les secrets des environnements !\")\n",
        "        \n",
        "        # Afficher quelques statistiques finales intÃ©ressantes\n",
        "        best_overall_performance = -float('inf')\n",
        "        best_combination = None\n",
        "        \n",
        "        for env_name, env_results in all_results.items():\n",
        "            for alg_name, result in env_results.items():\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_rewards = [h['reward'] for h in result['history'][-10:]]\n",
        "                    avg_final_performance = np.mean(final_rewards)\n",
        "                    \n",
        "                    if avg_final_performance > best_overall_performance:\n",
        "                        best_overall_performance = avg_final_performance\n",
        "                        best_combination = (alg_name, env_name)\n",
        "        \n",
        "        if best_combination:\n",
        "            print(f\"ğŸ† Meilleure combinaison globale: {best_combination[0]} sur {best_combination[1]}\")\n",
        "            print(f\"   Performance finale: {best_overall_performance:.3f}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Analyse terminÃ©e mais aucun rÃ©sultat valide obtenu\")\n",
        "        print(\"   VÃ©rifiez la compatibilitÃ© des environnements secrets\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ AUCUN RÃ‰SULTAT Ã€ ANALYSER\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ğŸ” VÃ©rifications Ã  effectuer:\")\n",
        "    print(\"   1. Les environnements secrets sont-ils accessibles ?\")\n",
        "    print(\"   2. Les adaptateurs ont-ils Ã©tÃ© crÃ©Ã©s correctement ?\") \n",
        "    print(\"   3. Les algorithmes Monte Carlo sont-ils importÃ©s ?\")\n",
        "    print(\"   4. L'entraÃ®nement s'est-il exÃ©cutÃ© sans erreur ?\")\n",
        "    print(\"\\\\nğŸ’¡ Conseil: Relancez les cellules prÃ©cÃ©dentes pour diagnostiquer le problÃ¨me\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ”š FIN DE L'ANALYSE MONTE CARLO SUR LES ENVIRONNEMENTS SECRETS\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š Fonctions de visualisation\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "    \n",
        "    # 1. RÃ©compenses par Ã©pisode\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            if len(rewards) >= 20:\n",
        "                rewards_smooth = pd.Series(rewards).rolling(window=20, min_periods=1).mean()\n",
        "                ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i], linewidth=2)\n",
        "            else:\n",
        "                ax1.plot(episodes, rewards, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - RÃ©compenses par Ã‰pisode')\n",
        "    ax1.set_xlabel('Ã‰pisode')\n",
        "    ax1.set_ylabel('RÃ©compense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            avg_q = [h['avg_q'] for h in history]\n",
        "            ax2.plot(episodes, avg_q, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Ã‰volution des Q-values')\n",
        "    ax2.set_xlabel('Ã‰pisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des Ã©pisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            lengths = [h['length'] for h in history]\n",
        "            \n",
        "            if len(lengths) >= 20:\n",
        "                lengths_smooth = pd.Series(lengths).rolling(window=20, min_periods=1).mean()\n",
        "                ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i], linewidth=2)\n",
        "            else:\n",
        "                ax3.plot(episodes, lengths, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des Ã‰pisodes')\n",
        "    ax3.set_xlabel('Ã‰pisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Taux de succÃ¨s cumulÃ©\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # Calculer taux de succÃ¨s cumulÃ©\n",
        "            success_rates = []\n",
        "            successes = 0\n",
        "            for j, h in enumerate(history):\n",
        "                if h['successful']:\n",
        "                    successes += 1\n",
        "                success_rates.append(successes / (j + 1))\n",
        "            \n",
        "            ax4.plot(episodes, success_rates, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Taux de SuccÃ¨s CumulÃ©')\n",
        "    ax4.set_xlabel('Ã‰pisode')\n",
        "    ax4.set_ylabel('Taux de SuccÃ¨s')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results(all_results):\n",
        "    \"\"\"Analyse dÃ©taillÃ©e des rÃ©sultats\"\"\"\n",
        "    print(\"ğŸ” ANALYSE DÃ‰TAILLÃ‰E DES RÃ‰SULTATS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\\\nğŸ“Š {env_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # Statistiques\n",
        "                total_episodes = len(history)\n",
        "                successful_episodes = sum(1 for h in history if h['successful'])\n",
        "                success_rate = successful_episodes / total_episodes\n",
        "                \n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Performance finale (derniers 20%)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                \n",
        "                print(f\"\\\\nğŸ¯ {alg_name}:\")\n",
        "                print(f\"   â€¢ Taux de succÃ¨s: {success_rate:.1%}\")\n",
        "                print(f\"   â€¢ RÃ©compense moyenne: {avg_reward:.3f} (Â±{std_reward:.3f})\")\n",
        "                print(f\"   â€¢ Performance finale: {final_avg_reward:.3f}\")\n",
        "                \n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_SuccÃ¨s': f\"{success_rate:.1%}\",\n",
        "                    'RÃ©compense_Moyenne': f\"{avg_reward:.3f}\",\n",
        "                    'RÃ©compense_Finale': f\"{final_avg_reward:.3f}\",\n",
        "                    'Ã‰pisodes': total_episodes\n",
        "                })\n",
        "            else:\n",
        "                print(f\"\\\\nâŒ {alg_name}: Aucune donnÃ©e\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_SuccÃ¨s': \"0%\",\n",
        "                    'RÃ©compense_Moyenne': \"0.000\",\n",
        "                    'RÃ©compense_Finale': \"0.000\",\n",
        "                    'Ã‰pisodes': 0\n",
        "                })\n",
        "    \n",
        "    # Tableau rÃ©capitulatif\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\\\nğŸ“‹ TABLEAU RÃ‰CAPITULATIF:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "print(\"ğŸ“Š Fonctions de visualisation dÃ©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ EntraÃ®nement Principal\n",
        "\n",
        "def run_monte_carlo_analysis(num_episodes=300):\n",
        "    \"\"\"Lance l'analyse complÃ¨te\"\"\"\n",
        "    \n",
        "    print(\"ğŸš€ DÃ‰BUT DE L'ANALYSE MONTE CARLO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ParamÃ¨tres: {num_episodes} Ã©pisodes par algorithme\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for env_name, adapter in adapters.items():\n",
        "        print(f\"\\\\nğŸ® ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"Ã‰tats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. Monte Carlo Exploring Starts\n",
        "        print(\"\\\\nğŸ¯ EntraÃ®nement Monte Carlo ES...\")\n",
        "        try:\n",
        "            mc_es = SecretMonteCarloES(adapter, gamma=0.99, name=f\"MC-ES-{env_name}\")\n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            env_results['MC-ES'] = result_es\n",
        "            print(f\"âœ… MC-ES terminÃ© - SuccÃ¨s: {result_es['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur MC-ES: {e}\")\n",
        "            env_results['MC-ES'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 2. On-Policy Monte Carlo\n",
        "        print(\"\\\\nğŸ¯ EntraÃ®nement On-Policy MC...\")\n",
        "        try:\n",
        "            on_policy_mc = SecretOnPolicyMC(adapter, gamma=0.99, epsilon=0.4, name=f\"OnPolicy-{env_name}\")\n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['On-Policy MC'] = result_on\n",
        "            print(f\"âœ… On-Policy MC terminÃ© - SuccÃ¨s: {result_on['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur On-Policy MC: {e}\")\n",
        "            env_results['On-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # RÃ©sumÃ© pour cet environnement\n",
        "        print(f\"\\\\nğŸ“Š RÃ‰SUMÃ‰ {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                avg_final_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                print(f\"   â€¢ {alg_name}: RÃ©compense finale = {avg_final_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   â€¢ {alg_name}: âŒ Ã‰chec\")\n",
        "    \n",
        "    print(\"\\\\nğŸ‰ ANALYSE COMPLÃˆTE TERMINÃ‰E !\")\n",
        "    return all_results\n",
        "\n",
        "# Lancer l'analyse\n",
        "if adapters:  # Seulement si les adaptateurs ont Ã©tÃ© crÃ©Ã©s\n",
        "    print(\"â³ Lancement de l'analyse (cela peut prendre 5-10 minutes)...\")\n",
        "    EPISODES = 300  # Ajustez selon vos besoins\n",
        "    \n",
        "    all_results = run_monte_carlo_analysis(num_episodes=EPISODES)\n",
        "else:\n",
        "    print(\"âŒ Impossible de lancer l'analyse - adaptateurs non disponibles\")\n",
        "    all_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ˆ Affichage des RÃ©sultats\n",
        "\n",
        "if all_results:\n",
        "    print(\"ğŸ“ˆ GÃ‰NÃ‰RATION DES ANALYSES VISUELLES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\\\nğŸ¯ 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        if any(result['history'] for result in env_results.values() if 'history' in result):\n",
        "            print(f\"\\\\nğŸ“Š Graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"âŒ Pas de donnÃ©es pour {env_name}\")\n",
        "    \n",
        "    # 2. Analyse dÃ©taillÃ©e\n",
        "    print(\"\\\\nğŸ¯ 2. ANALYSE DÃ‰TAILLÃ‰E\")\n",
        "    print(\"-\" * 50)\n",
        "    summary_df = analyze_results(all_results)\n",
        "    \n",
        "    # 3. Recommandations\n",
        "    print(\"\\\\nğŸ¯ 3. RECOMMANDATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                \n",
        "                # Score composite\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                \n",
        "                if composite_score > best_score:\n",
        "                    best_score = composite_score\n",
        "                    best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\\\nğŸ† MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   â€¢ {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   â€¢ {env_name}: Aucun algorithme efficace\")\n",
        "    \n",
        "    # 4. Performance globale\n",
        "    alg_scores = {'MC-ES': [], 'On-Policy MC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history'] and alg_name in alg_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                alg_scores[alg_name].append(composite_score)\n",
        "    \n",
        "    print(\"\\\\nğŸŒŸ PERFORMANCE GLOBALE:\")\n",
        "    for alg_name, scores in alg_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   â€¢ {alg_name}: {avg_score:.3f} (Â±{std_score:.3f})\")\n",
        "    \n",
        "    # 5. Conseils\n",
        "    print(\"\\\\nğŸ’¡ CONSEILS D'INTERPRÃ‰TATION:\")\n",
        "    print(\"   1. ğŸ¯ Taux de succÃ¨s Ã©levÃ© = algorithme stable\")\n",
        "    print(\"   2. ğŸ”„ RÃ©compenses croissantes = apprentissage effectif\")\n",
        "    print(\"   3. ğŸ“Š Q-values convergentes = politique stable\")\n",
        "    print(\"   4. ğŸ›ï¸  Ajustez les hyperparamÃ¨tres si nÃ©cessaire\")\n",
        "    \n",
        "    # Sauvegarde\n",
        "    try:\n",
        "        summary_df.to_csv('monte_carlo_results.csv', index=False)\n",
        "        print(f\"\\\\nğŸ’¾ RÃ©sultats sauvegardÃ©s dans 'monte_carlo_results.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur sauvegarde: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ AUCUN RÃ‰SULTAT Ã€ AFFICHER\")\n",
        "    print(\"VÃ©rifiez que l'entraÃ®nement prÃ©cÃ©dent s'est bien dÃ©roulÃ©.\")\n",
        "\n",
        "print(\"\\\\nğŸ‰ ANALYSE MONTE CARLO TERMINÃ‰E !\")\n",
        "print(\"ğŸ•µï¸ Les environnements secrets ont rÃ©vÃ©lÃ© leurs mystÃ¨res !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ•µï¸ Analyse Monte Carlo sur les Environnements Secrets\n",
        "\n",
        "## ğŸ¯ Objectif\n",
        "Ce notebook implÃ©mente et compare les trois algorithmes Monte Carlo sur les 4 environnements secrets :\n",
        "- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**\n",
        "\n",
        "## ğŸ“‹ Algorithmes Monte Carlo testÃ©s\n",
        "- **Monte Carlo Exploring Starts (MC-ES)** : Garantit l'exploration avec starts alÃ©atoires\n",
        "- **On-Policy Monte Carlo** : AmÃ©lioration de politique Îµ-greedy on-policy  \n",
        "- **Off-Policy Monte Carlo** : Apprentissage avec importance sampling\n",
        "\n",
        "## ğŸ“Š Analyses effectuÃ©es\n",
        "- Courbes d'apprentissage (rÃ©compenses, Q-values)\n",
        "- Convergence des politiques\n",
        "- Comparaison des performances entre algorithmes\n",
        "- Analyse de la stabilitÃ© d'apprentissage\n",
        "- MÃ©triques de succÃ¨s par environnement\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“š Imports et configuration\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (14, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Ajouter les chemins nÃ©cessaires\n",
        "project_root = os.path.abspath('../../')\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, os.path.join(project_root, 'game', 'secret_env'))\n",
        "\n",
        "# Imports des environnements secrets\n",
        "try:\n",
        "    from secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
        "    print(\"âœ… Environnements secrets importÃ©s avec succÃ¨s\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur d'import des environnements secrets: {e}\")\n",
        "    # Fallback pour les imports\n",
        "    import ctypes\n",
        "    import platform\n",
        "    \n",
        "print(\"ğŸ”§ Configuration terminÃ©e !\")\n",
        "\n",
        "# Test rapide des environnements\n",
        "try:\n",
        "    env0 = SecretEnv0()\n",
        "    print(f\"ğŸ“Š SecretEnv0 - Ã‰tats: {env0.num_states()}, Actions: {env0.num_actions()}\")\n",
        "    \n",
        "    env1 = SecretEnv1()\n",
        "    print(f\"ğŸ“Š SecretEnv1 - Ã‰tats: {env1.num_states()}, Actions: {env1.num_actions()}\")\n",
        "    \n",
        "    env2 = SecretEnv2()\n",
        "    print(f\"ğŸ“Š SecretEnv2 - Ã‰tats: {env2.num_states()}, Actions: {env2.num_actions()}\")\n",
        "    \n",
        "    env3 = SecretEnv3()\n",
        "    print(f\"ğŸ“Š SecretEnv3 - Ã‰tats: {env3.num_states()}, Actions: {env3.num_actions()}\")\n",
        "    \n",
        "    print(\"\\nğŸ‰ Tous les environnements secrets sont fonctionnels !\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur lors du test des environnements: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ Adaptateur d'environnement pour les Secret Envs\n",
        "\n",
        "class SecretEnvAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur pour rendre les SecretEnv compatibles avec l'API Gym standard.\n",
        "    Transforme l'interface spÃ©cifique des environnements secrets en interface standard.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        self.env = secret_env_class()\n",
        "        \n",
        "        # PropriÃ©tÃ©s MDP pour compatibilitÃ© avec Monte Carlo\n",
        "        self.nS = self.env.num_states()\n",
        "        self.nA = self.env.num_actions()\n",
        "        \n",
        "        # Ã‰tat et rÃ©compenses\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        print(f\"ğŸ—ï¸  {env_name} adapter crÃ©Ã© - Ã‰tats: {self.nS}, Actions: {self.nA}\")\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"RÃ©initialise l'environnement et retourne l'Ã©tat initial\"\"\"\n",
        "        try:\n",
        "            self.env.reset()\n",
        "            self.current_state = self.env.state_id()\n",
        "            self.last_score = self.env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur reset {self.env_name}: {e}\")\n",
        "            # CrÃ©er un nouvel environnement si reset Ã©choue\n",
        "            self.env = self.secret_env_class()\n",
        "            self.env.reset()\n",
        "            self.current_state = self.env.state_id()\n",
        "            self.last_score = self.env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        ExÃ©cute une action et retourne (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Obtenir les actions disponibles\n",
        "            available_actions = self.get_available_actions()\n",
        "            \n",
        "            # VÃ©rifier si l'action est valide\n",
        "            if action not in available_actions:\n",
        "                # Action non valide - retourner rÃ©compense nÃ©gative et rester dans l'Ã©tat\n",
        "                return self.current_state, -0.1, False, {\n",
        "                    'invalid_action': True,\n",
        "                    'available_actions': available_actions,\n",
        "                    'requested_action': action\n",
        "                }\n",
        "            \n",
        "            # Sauvegarder le score avant l'action\n",
        "            old_score = self.env.score()\n",
        "            \n",
        "            # ExÃ©cuter l'action\n",
        "            self.env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            # Obtenir le nouvel Ã©tat et calculer la rÃ©compense\n",
        "            next_state = self.env.state_id()\n",
        "            new_score = self.env.score()\n",
        "            reward = new_score - old_score  # RÃ©compense diffÃ©rentielle\n",
        "            done = self.env.is_game_over()\n",
        "            \n",
        "            # Mise Ã  jour\n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            info = {\n",
        "                'available_actions': self.get_available_actions(),\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps,\n",
        "                'valid_action': True\n",
        "            }\n",
        "            \n",
        "            # Limite de sÃ©curitÃ© pour Ã©viter les Ã©pisodes infinis\n",
        "            if self.episode_steps > 1000:\n",
        "                done = True\n",
        "                reward -= 1.0  # PÃ©nalitÃ© pour Ã©pisode trop long\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur step {self.env_name}: {e}\")\n",
        "            # Retourner un Ã©tat d'erreur\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Obtient la liste des actions disponibles dans l'Ã©tat courant\"\"\"\n",
        "        try:\n",
        "            actions = self.env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else [0]\n",
        "        except:\n",
        "            # Fallback : toutes les actions sont disponibles\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    def display(self):\n",
        "        \"\"\"Affiche l'Ã©tat courant de l'environnement\"\"\"\n",
        "        try:\n",
        "            self.env.display()\n",
        "        except:\n",
        "            print(f\"Ã‰tat courant: {self.current_state}, Score: {self.last_score}\")\n",
        "    \n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Retourne les informations MDP pour compatibilitÃ©\"\"\"\n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# Test des adaptateurs\n",
        "print(\"ğŸ§ª Test des adaptateurs...\")\n",
        "adapters = {}\n",
        "\n",
        "try:\n",
        "    adapters['SecretEnv0'] = SecretEnvAdapter(SecretEnv0, \"SecretEnv0\")\n",
        "    adapters['SecretEnv1'] = SecretEnvAdapter(SecretEnv1, \"SecretEnv1\")\n",
        "    adapters['SecretEnv2'] = SecretEnvAdapter(SecretEnv2, \"SecretEnv2\")\n",
        "    adapters['SecretEnv3'] = SecretEnvAdapter(SecretEnv3, \"SecretEnv3\")\n",
        "    \n",
        "    print(\"\\nâœ… Tous les adaptateurs crÃ©Ã©s avec succÃ¨s !\")\n",
        "    \n",
        "    # Test rapide d'un adaptateur\n",
        "    test_adapter = adapters['SecretEnv0']\n",
        "    state = test_adapter.reset()\n",
        "    available = test_adapter.get_available_actions()\n",
        "    print(f\"ğŸ” Test SecretEnv0 - Ã‰tat initial: {state}, Actions disponibles: {available}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur lors de la crÃ©ation des adaptateurs: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ® ImplÃ©mentation des Algorithmes Monte Carlo pour les Environnements Secrets\n",
        "\n",
        "class SecretMonteCarloES:\n",
        "    \"\"\"Monte Carlo avec Exploring Starts adaptÃ© aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, name=\"MC-ES\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))  # Initialisation alÃ©atoire\n",
        "        self.policy = np.zeros(self.nS, dtype=int)\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        \n",
        "        # Historique d'entraÃ®nement\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"ğŸ¯ {name} initialisÃ© pour {env_adapter.env_name}\")\n",
        "    \n",
        "    def generate_episode_with_exploring_starts(self):\n",
        "        \"\"\"GÃ©nÃ¨re un Ã©pisode avec exploring starts\"\"\"\n",
        "        episode = []\n",
        "        \n",
        "        # Reset avec Ã©tat alÃ©atoire (approximation d'exploring starts)\n",
        "        for _ in range(10):  # Essayer plusieurs resets pour varier l'Ã©tat initial\n",
        "            state = self.env_adapter.reset()\n",
        "            if np.random.random() < 0.3:  # 30% chance d'accepter cet Ã©tat\n",
        "                break\n",
        "        \n",
        "        # Action initiale alÃ©atoire (exploring starts)\n",
        "        available_actions = self.env_adapter.get_available_actions()\n",
        "        if len(available_actions) > 0:\n",
        "            action = np.random.choice(available_actions)\n",
        "        else:\n",
        "            action = 0\n",
        "        \n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            # Action suivante selon politique courante avec actions disponibles\n",
        "            state = next_state\n",
        "            available_actions = info.get('available_actions', list(range(self.nA)))\n",
        "            \n",
        "            if len(available_actions) > 0:\n",
        "                # Politique greedy avec tie-breaking alÃ©atoire sur actions disponibles\n",
        "                q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "                max_q = np.max(q_vals)\n",
        "                best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "                action = np.random.choice(best_actions)\n",
        "            else:\n",
        "                break\n",
        "            \n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"EntraÃ®nement Monte Carlo ES\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                # GÃ©nÃ©rer Ã©pisode avec exploring starts\n",
        "                episode = self.generate_episode_with_exploring_starts()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Mise Ã  jour First-Visit Monte Carlo\n",
        "                    G = 0.0\n",
        "                    visited = set()\n",
        "                    \n",
        "                    for (state, action, reward) in reversed(episode):\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        if (state, action) not in visited:\n",
        "                            visited.add((state, action))\n",
        "                            self.returns_count[(state, action)] += 1\n",
        "                            self.returns_sum[(state, action)] += G\n",
        "                            self.Q[state, action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
        "                    \n",
        "                    # AmÃ©lioration de la politique (greedy)\n",
        "                    for s in range(self.nS):\n",
        "                        self.policy[s] = np.argmax(self.Q[s])\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    # Ã‰pisode Ã©chouÃ©\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] Ã‰pisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succÃ¨s: {success_rate:.2f}, \"\n",
        "                          f\"RÃ©compense rÃ©cente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Erreur Ã©pisode {episode_num + 1}: {e}\")\n",
        "                self.history.append({\n",
        "                    'episode': episode_num + 1,\n",
        "                    'reward': 0.0,\n",
        "                    'length': 0,\n",
        "                    'avg_q': np.mean(self.Q),\n",
        "                    'successful': False\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "class SecretOnPolicyMC:\n",
        "    \"\"\"On-Policy Monte Carlo avec Îµ-greedy adaptÃ© aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, epsilon=0.3, name=\"On-Policy MC\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_epsilon = epsilon\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))\n",
        "        self.policy = np.zeros(self.nS, dtype=int)\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        \n",
        "        # Historique\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"ğŸ¯ {name} initialisÃ© pour {env_adapter.env_name} (Îµ={epsilon})\")\n",
        "    \n",
        "    def epsilon_greedy_action(self, state, available_actions):\n",
        "        \"\"\"SÃ©lectionne une action selon Îµ-greedy parmi les actions disponibles\"\"\"\n",
        "        if len(available_actions) == 0:\n",
        "            return 0\n",
        "        \n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            # Greedy : meilleure action parmi les disponibles\n",
        "            q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "            max_q = np.max(q_vals)\n",
        "            best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "    \n",
        "    def generate_episode(self):\n",
        "        \"\"\"GÃ©nÃ¨re un Ã©pisode selon la politique Îµ-greedy\"\"\"\n",
        "        episode = []\n",
        "        state = self.env_adapter.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            available_actions = self.env_adapter.get_available_actions()\n",
        "            action = self.epsilon_greedy_action(state, available_actions)\n",
        "            \n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"EntraÃ®nement On-Policy Monte Carlo\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                episode = self.generate_episode()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Mise Ã  jour First-Visit Monte Carlo\n",
        "                    G = 0.0\n",
        "                    visited = set()\n",
        "                    \n",
        "                    for (state, action, reward) in reversed(episode):\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        if (state, action) not in visited:\n",
        "                            visited.add((state, action))\n",
        "                            self.returns_count[(state, action)] += 1\n",
        "                            self.returns_sum[(state, action)] += G\n",
        "                            self.Q[state, action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
        "                    \n",
        "                    # AmÃ©lioration de politique\n",
        "                    for s in range(self.nS):\n",
        "                        self.policy[s] = np.argmax(self.Q[s])\n",
        "                    \n",
        "                    # DÃ©croissance d'epsilon\n",
        "                    self.epsilon = max(0.01, self.epsilon * 0.9995)\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'epsilon': self.epsilon,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'epsilon': self.epsilon,\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] Ã‰pisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succÃ¨s: {success_rate:.2f}, \"\n",
        "                          f\"Îµ: {self.epsilon:.3f}, \"\n",
        "                          f\"RÃ©compense rÃ©cente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Erreur Ã©pisode {episode_num + 1}: {e}\")\n",
        "                \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "class SecretOffPolicyMC:\n",
        "    \"\"\"Off-Policy Monte Carlo avec Importance Sampling adaptÃ© aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, epsilon=0.4, name=\"Off-Policy MC\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))\n",
        "        self.target_policy = np.zeros(self.nS, dtype=int)\n",
        "        self.C = np.zeros((self.nS, self.nA))  # Poids cumulatifs\n",
        "        \n",
        "        # Historique\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"ğŸ¯ {name} initialisÃ© pour {env_adapter.env_name} (Îµ={epsilon})\")\n",
        "    \n",
        "    def behavior_policy(self, state, available_actions):\n",
        "        \"\"\"Politique de comportement Îµ-greedy\"\"\"\n",
        "        if len(available_actions) == 0:\n",
        "            return 0\n",
        "            \n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "            max_q = np.max(q_vals)\n",
        "            best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "    \n",
        "    def generate_episode(self):\n",
        "        \"\"\"GÃ©nÃ¨re un Ã©pisode selon la politique de comportement\"\"\"\n",
        "        episode = []\n",
        "        state = self.env_adapter.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            available_actions = self.env_adapter.get_available_actions()\n",
        "            action = self.behavior_policy(state, available_actions)\n",
        "            \n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward, available_actions.copy()))\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"EntraÃ®nement Off-Policy Monte Carlo avec Importance Sampling\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                episode = self.generate_episode()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Importance Sampling Update\n",
        "                    G = 0.0\n",
        "                    W = 1.0\n",
        "                    \n",
        "                    for i in range(len(episode) - 1, -1, -1):\n",
        "                        state, action, reward, available_actions = episode[i]\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        # Mettre Ã  jour C et Q\n",
        "                        self.C[state, action] += W\n",
        "                        if self.C[state, action] > 0:\n",
        "                            self.Q[state, action] += (W / self.C[state, action]) * (G - self.Q[state, action])\n",
        "                        \n",
        "                        # Mettre Ã  jour la politique cible (greedy)\n",
        "                        self.target_policy[state] = np.argmax(self.Q[state])\n",
        "                        \n",
        "                        # VÃ©rifier si l'action est celle de la politique cible\n",
        "                        if action != self.target_policy[state]:\n",
        "                            break\n",
        "                        \n",
        "                        # Calculer le ratio d'importance\n",
        "                        # ProbabilitÃ© politique cible (dÃ©terministe)\n",
        "                        target_prob = 1.0\n",
        "                        \n",
        "                        # ProbabilitÃ© politique de comportement\n",
        "                        if len(available_actions) > 0:\n",
        "                            if action == np.argmax([self.Q[state, a] for a in available_actions]):\n",
        "                                behavior_prob = 1.0 - self.epsilon + self.epsilon / len(available_actions)\n",
        "                            else:\n",
        "                                behavior_prob = self.epsilon / len(available_actions)\n",
        "                        else:\n",
        "                            behavior_prob = 1.0\n",
        "                        \n",
        "                        if behavior_prob > 0:\n",
        "                            W *= target_prob / behavior_prob\n",
        "                        else:\n",
        "                            break\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r, _ in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'avg_weight': np.mean(self.C[self.C > 0]) if np.any(self.C > 0) else 0,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'avg_weight': 0,\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] Ã‰pisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succÃ¨s: {success_rate:.2f}, \"\n",
        "                          f\"RÃ©compense rÃ©cente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Erreur Ã©pisode {episode_num + 1}: {e}\")\n",
        "                \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.target_policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "print(\"ğŸ¯ Algorithmes Monte Carlo dÃ©finis avec succÃ¨s !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š Fonctions de visualisation et d'analyse\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les algorithmes\"\"\"\n",
        "    \n",
        "    n_algorithms = len(results_dict)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Couleurs pour chaque algorithme\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    # 1. RÃ©compenses par Ã©pisode\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        rewards = [h['reward'] for h in history]\n",
        "        \n",
        "        # Moyenne mobile pour lisser les courbes\n",
        "        window_size = min(50, len(rewards) // 10 + 1)\n",
        "        if len(rewards) >= window_size:\n",
        "            rewards_smooth = pd.Series(rewards).rolling(window=window_size, min_periods=1).mean()\n",
        "            ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "        else:\n",
        "            ax1.plot(episodes, rewards, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - RÃ©compenses par Ã‰pisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Ã‰pisode')\n",
        "    ax1.set_ylabel('RÃ©compense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        avg_q = [h['avg_q'] for h in history]\n",
        "        \n",
        "        ax2.plot(episodes, avg_q, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Ã‰volution des Q-values', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Ã‰pisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des Ã©pisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        lengths = [h['length'] for h in history]\n",
        "        \n",
        "        # Moyenne mobile\n",
        "        window_size = min(50, len(lengths) // 10 + 1)\n",
        "        if len(lengths) >= window_size:\n",
        "            lengths_smooth = pd.Series(lengths).rolling(window=window_size, min_periods=1).mean()\n",
        "            ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "        else:\n",
        "            ax3.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des Ã‰pisodes', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Ã‰pisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Analyse de convergence (Ã©cart-type des rÃ©compenses rÃ©centes)\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = []\n",
        "        stds = []\n",
        "        \n",
        "        window_size = 100\n",
        "        for j in range(window_size, len(history)):\n",
        "            recent_rewards = [h['reward'] for h in history[j-window_size:j]]\n",
        "            episodes.append(history[j]['episode'])\n",
        "            stds.append(np.std(recent_rewards))\n",
        "        \n",
        "        if len(episodes) > 0:\n",
        "            ax4.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - StabilitÃ© (Ã‰cart-type des rÃ©compenses)', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('Ã‰pisode')\n",
        "    ax4.set_ylabel('Ã‰cart-type des rÃ©compenses')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_performance_comparison(all_results):\n",
        "    \"\"\"Compare les performances finales de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    # PrÃ©parer les donnÃ©es pour la visualisation\n",
        "    env_names = list(all_results.keys())\n",
        "    alg_names = list(list(all_results.values())[0].keys())\n",
        "    \n",
        "    # Metrics Ã  analyser\n",
        "    final_rewards = []\n",
        "    success_rates = []\n",
        "    avg_q_values = []\n",
        "    \n",
        "    for env_name in env_names:\n",
        "        env_rewards = []\n",
        "        env_success_rates = []\n",
        "        env_avg_q = []\n",
        "        \n",
        "        for alg_name in alg_names:\n",
        "            result = all_results[env_name][alg_name]\n",
        "            \n",
        "            # RÃ©compense finale (moyenne des 100 derniers Ã©pisodes)\n",
        "            history = result['history']\n",
        "            if len(history) >= 100:\n",
        "                final_reward = np.mean([h['reward'] for h in history[-100:]])\n",
        "            else:\n",
        "                final_reward = np.mean([h['reward'] for h in history]) if history else 0\n",
        "            \n",
        "            env_rewards.append(final_reward)\n",
        "            env_success_rates.append(result.get('success_rate', 0))\n",
        "            env_avg_q.append(result['history'][-1]['avg_q'] if result['history'] else 0)\n",
        "        \n",
        "        final_rewards.append(env_rewards)\n",
        "        success_rates.append(env_success_rates)\n",
        "        avg_q_values.append(env_avg_q)\n",
        "    \n",
        "    # CrÃ©er les graphiques\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Heatmap des rÃ©compenses finales\n",
        "    ax1 = axes[0, 0]\n",
        "    im1 = ax1.imshow(final_rewards, cmap='RdYlGn', aspect='auto')\n",
        "    ax1.set_xticks(range(len(alg_names)))\n",
        "    ax1.set_xticklabels(alg_names, rotation=45)\n",
        "    ax1.set_yticks(range(len(env_names)))\n",
        "    ax1.set_yticklabels(env_names)\n",
        "    ax1.set_title('RÃ©compenses Finales par Algorithme et Environnement')\n",
        "    \n",
        "    # Ajouter les valeurs dans les cellules\n",
        "    for i in range(len(env_names)):\n",
        "        for j in range(len(alg_names)):\n",
        "            ax1.text(j, i, f'{final_rewards[i][j]:.2f}', ha='center', va='center')\n",
        "    \n",
        "    plt.colorbar(im1, ax=ax1)\n",
        "    \n",
        "    # 2. Graphique en barres des taux de succÃ¨s\n",
        "    ax2 = axes[0, 1]\n",
        "    x = np.arange(len(env_names))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        success_data = [success_rates[j][i] for j in range(len(env_names))]\n",
        "        ax2.bar(x + i * width, success_data, width, label=alg_name, alpha=0.8)\n",
        "    \n",
        "    ax2.set_xlabel('Environnements')\n",
        "    ax2.set_ylabel('Taux de SuccÃ¨s')\n",
        "    ax2.set_title('Taux de SuccÃ¨s par Environnement et Algorithme')\n",
        "    ax2.set_xticks(x + width * 1.5)\n",
        "    ax2.set_xticklabels(env_names)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Comparaison des Q-values moyennes finales\n",
        "    ax3 = axes[1, 0]\n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        q_data = [avg_q_values[j][i] for j in range(len(env_names))]\n",
        "        ax3.bar(x + i * width, q_data, width, label=alg_name, alpha=0.8)\n",
        "    \n",
        "    ax3.set_xlabel('Environnements')\n",
        "    ax3.set_ylabel('Q-value Moyenne Finale')\n",
        "    ax3.set_title('Q-values Finales par Environnement et Algorithme')\n",
        "    ax3.set_xticks(x + width * 1.5)\n",
        "    ax3.set_xticklabels(env_names)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Graphique radar des performances gÃ©nÃ©rales\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    # Normaliser les mÃ©triques pour le radar\n",
        "    final_rewards_norm = np.array(final_rewards)\n",
        "    success_rates_norm = np.array(success_rates)\n",
        "    \n",
        "    # Score composite pour chaque algorithme\n",
        "    composite_scores = []\n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        alg_rewards = [final_rewards[j][i] for j in range(len(env_names))]\n",
        "        alg_success = [success_rates[j][i] for j in range(len(env_names))]\n",
        "        \n",
        "        # Score composite (moyenne pondÃ©rÃ©e)\n",
        "        composite_score = np.mean(alg_rewards) * 0.7 + np.mean(alg_success) * 0.3\n",
        "        composite_scores.append(composite_score)\n",
        "    \n",
        "    bars = ax4.bar(alg_names, composite_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(alg_names)])\n",
        "    ax4.set_title('Score Composite Global par Algorithme')\n",
        "    ax4.set_ylabel('Score Composite')\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for bar, score in zip(bars, composite_scores):\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_algorithm_characteristics(all_results):\n",
        "    \"\"\"Analyse les caractÃ©ristiques spÃ©cifiques de chaque algorithme\"\"\"\n",
        "    \n",
        "    print(\"ğŸ” ANALYSE DÃ‰TAILLÃ‰E DES ALGORITHMES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\nğŸ“Š {env_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            history = result['history']\n",
        "            \n",
        "            if len(history) > 0:\n",
        "                # Statistiques gÃ©nÃ©rales\n",
        "                total_episodes = len(history)\n",
        "                successful_episodes = sum(1 for h in history if h['successful'])\n",
        "                success_rate = successful_episodes / total_episodes\n",
        "                \n",
        "                # RÃ©compenses\n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Convergence (derniers 20% d'Ã©pisodes)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                \n",
        "                # StabilitÃ© (Ã©cart-type des derniers Ã©pisodes)\n",
        "                final_stability = np.std(final_rewards) if len(final_rewards) > 1 else 0\n",
        "                \n",
        "                print(f\"\\nğŸ¯ {alg_name}:\")\n",
        "                print(f\"   â€¢ Taux de succÃ¨s: {success_rate:.1%}\")\n",
        "                print(f\"   â€¢ RÃ©compense moyenne: {avg_reward:.3f} (Â±{std_reward:.3f})\")\n",
        "                print(f\"   â€¢ Performance finale: {final_avg_reward:.3f}\")\n",
        "                print(f\"   â€¢ StabilitÃ© finale: {final_stability:.3f}\")\n",
        "                \n",
        "                # CaractÃ©ristiques spÃ©cifiques Ã  l'algorithme\n",
        "                if 'epsilon' in history[0]:\n",
        "                    initial_eps = history[0]['epsilon']\n",
        "                    final_eps = history[-1]['epsilon']\n",
        "                    print(f\"   â€¢ DÃ©croissance Îµ: {initial_eps:.3f} â†’ {final_eps:.3f}\")\n",
        "                \n",
        "                if 'avg_weight' in history[0]:\n",
        "                    final_weight = history[-1]['avg_weight']\n",
        "                    print(f\"   â€¢ Poids moyen final: {final_weight:.3f}\")\n",
        "            else:\n",
        "                print(f\"\\nâŒ {alg_name}: Aucune donnÃ©e d'entraÃ®nement\")\n",
        "\n",
        "print(\"ğŸ“Š Fonctions de visualisation dÃ©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ EntraÃ®nement Principal - Tous les Algorithmes sur Tous les Environnements\n",
        "\n",
        "def run_complete_analysis(num_episodes=1000):\n",
        "    \"\"\"Lance l'analyse complÃ¨te de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"ğŸš€ DÃ‰BUT DE L'ANALYSE COMPLÃˆTE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ParamÃ¨tres: {num_episodes} Ã©pisodes par algorithme\")\n",
        "    print(f\"Total: {4} environnements Ã— {3} algorithmes = {12} entraÃ®nements\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Dictionnaire pour stocker tous les rÃ©sultats\n",
        "    all_results = {}\n",
        "    \n",
        "    # Environnements Ã  tester\n",
        "    env_classes = {\n",
        "        'SecretEnv0': SecretEnv0,\n",
        "        'SecretEnv1': SecretEnv1, \n",
        "        'SecretEnv2': SecretEnv2,\n",
        "        'SecretEnv3': SecretEnv3\n",
        "    }\n",
        "    \n",
        "    # CrÃ©er les adaptateurs\n",
        "    adapters_dict = {}\n",
        "    for env_name, env_class in env_classes.items():\n",
        "        try:\n",
        "            adapters_dict[env_name] = SecretEnvAdapter(env_class, env_name)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur crÃ©ation adaptateur {env_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nâœ… {len(adapters_dict)} adaptateurs crÃ©Ã©s avec succÃ¨s\")\n",
        "    \n",
        "    # EntraÃ®nement pour chaque environnement\n",
        "    for env_name, adapter in adapters_dict.items():\n",
        "        print(f\"\\nğŸ® ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"Ã‰tats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. Monte Carlo Exploring Starts\n",
        "        print(\"\\\\nğŸ¯ EntraÃ®nement Monte Carlo ES...\")\n",
        "        try:\n",
        "            mc_es = SecretMonteCarloES(adapter, gamma=0.99, name=f\"MC-ES-{env_name}\")\n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            env_results['MC-ES'] = result_es\n",
        "            print(f\"âœ… MC-ES terminÃ© - Taux de succÃ¨s: {result_es['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur MC-ES sur {env_name}: {e}\")\n",
        "            env_results['MC-ES'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 2. On-Policy Monte Carlo\n",
        "        print(\"\\\\nğŸ¯ EntraÃ®nement On-Policy MC...\")\n",
        "        try:\n",
        "            on_policy_mc = SecretOnPolicyMC(adapter, gamma=0.99, epsilon=0.3, name=f\"OnPolicy-{env_name}\")\n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['On-Policy MC'] = result_on\n",
        "            print(f\"âœ… On-Policy MC terminÃ© - Taux de succÃ¨s: {result_on['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur On-Policy MC sur {env_name}: {e}\")\n",
        "            env_results['On-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 3. Off-Policy Monte Carlo\n",
        "        print(\"\\\\nğŸ¯ EntraÃ®nement Off-Policy MC...\")\n",
        "        try:\n",
        "            off_policy_mc = SecretOffPolicyMC(adapter, gamma=0.99, epsilon=0.4, name=f\"OffPolicy-{env_name}\")\n",
        "            result_off = off_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['Off-Policy MC'] = result_off\n",
        "            print(f\"âœ… Off-Policy MC terminÃ© - Taux de succÃ¨s: {result_off['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur Off-Policy MC sur {env_name}: {e}\")\n",
        "            env_results['Off-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # Stocker les rÃ©sultats de cet environnement\n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # Afficher un rÃ©sumÃ© pour cet environnement\n",
        "        print(f\"\\\\nğŸ“Š RÃ‰SUMÃ‰ {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_final_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                print(f\"   â€¢ {alg_name}: RÃ©compense finale = {avg_final_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   â€¢ {alg_name}: âŒ Aucun rÃ©sultat\")\n",
        "    \n",
        "    print(\"\\\\nğŸ‰ ANALYSE COMPLÃˆTE TERMINÃ‰E !\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Lancer l'analyse complÃ¨te (peut prendre plusieurs minutes)\n",
        "print(\"â³ Lancement de l'analyse complÃ¨te...\")\n",
        "print(\"Cela peut prendre plusieurs minutes selon la complexitÃ© des environnements...\")\n",
        "\n",
        "# Utiliser un nombre d'Ã©pisodes raisonnable pour le test\n",
        "EPISODES = 800  # Ajustez selon vos besoins de temps\n",
        "\n",
        "all_results = run_complete_analysis(num_episodes=EPISODES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ˆ Affichage des RÃ©sultats et Analyses ComplÃ¨tes\n",
        "\n",
        "print(\"ğŸ“ˆ GÃ‰NÃ‰RATION DES ANALYSES VISUELLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# VÃ©rifier qu'on a des rÃ©sultats\n",
        "if all_results and any(env_results for env_results in all_results.values()):\n",
        "    \n",
        "    # 1. Afficher les courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\\\nğŸ¯ 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        if any(result['history'] for result in env_results.values()):\n",
        "            print(f\"\\\\nğŸ“Š Graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"âŒ Pas de donnÃ©es valides pour {env_name}\")\n",
        "    \n",
        "    # 2. Comparaison des performances entre environnements\n",
        "    print(\"\\\\nğŸ¯ 2. COMPARAISON GLOBALE DES PERFORMANCES\")\n",
        "    print(\"-\" * 50)\n",
        "    plot_performance_comparison(all_results)\n",
        "    \n",
        "    # 3. Analyse dÃ©taillÃ©e des caractÃ©ristiques\n",
        "    print(\"\\\\nğŸ¯ 3. ANALYSE DÃ‰TAILLÃ‰E\")\n",
        "    print(\"-\" * 50)\n",
        "    analyze_algorithm_characteristics(all_results)\n",
        "    \n",
        "    # 4. Tableau rÃ©capitulatif final\n",
        "    print(\"\\\\nğŸ¯ 4. TABLEAU RÃ‰CAPITULATIF FINAL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # CrÃ©er un DataFrame pour le rÃ©sumÃ©\n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                # Calculer les mÃ©triques finales\n",
        "                history = result['history']\n",
        "                final_rewards = [h['reward'] for h in history[-100:]] if len(history) >= 100 else [h['reward'] for h in history]\n",
        "                \n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_SuccÃ¨s': f\"{result['success_rate']:.1%}\",\n",
        "                    'RÃ©compense_Finale': f\"{np.mean(final_rewards):.3f}\",\n",
        "                    'StabilitÃ©': f\"{np.std(final_rewards):.3f}\",\n",
        "                    'Ã‰pisodes_Total': len(history),\n",
        "                    'Q_Moyenne_Finale': f\"{history[-1]['avg_q']:.3f}\" if history else \"0.000\"\n",
        "                })\n",
        "            else:\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_SuccÃ¨s': \"0.0%\",\n",
        "                    'RÃ©compense_Finale': \"0.000\",\n",
        "                    'StabilitÃ©': \"N/A\",\n",
        "                    'Ã‰pisodes_Total': 0,\n",
        "                    'Q_Moyenne_Finale': \"0.000\"\n",
        "                })\n",
        "    \n",
        "    # Afficher le tableau\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\\\nğŸ“‹ RÃ©sultats par Algorithme et Environnement:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    # 5. Recommandations finales\n",
        "    print(\"\\\\nğŸ¯ 5. RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Trouver les meilleurs algorithmes par environnement\n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                # Score composite basÃ© sur rÃ©compense finale et taux de succÃ¨s\n",
        "                final_rewards = [h['reward'] for h in result['history'][-100:]] if len(result['history']) >= 100 else [h['reward'] for h in result['history']]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                \n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                \n",
        "                if composite_score > best_score:\n",
        "                    best_score = composite_score\n",
        "                    best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\\\nğŸ† MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   â€¢ {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   â€¢ {env_name}: Aucun algorithme efficace\")\n",
        "    \n",
        "    # Analyse globale\n",
        "    alg_global_scores = {'MC-ES': [], 'On-Policy MC': [], 'Off-Policy MC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history'] and alg_name in alg_global_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-100:]] if len(result['history']) >= 100 else [h['reward'] for h in result['history']]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                alg_global_scores[alg_name].append(composite_score)\n",
        "    \n",
        "    print(\"\\\\nğŸŒŸ PERFORMANCE GLOBALE DES ALGORITHMES:\")\n",
        "    for alg_name, scores in alg_global_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   â€¢ {alg_name}: {avg_score:.3f} (Â±{std_score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   â€¢ {alg_name}: Aucune donnÃ©e valide\")\n",
        "    \n",
        "    # Recommandations spÃ©cifiques\n",
        "    print(\"\\\\nğŸ’¡ RECOMMANDATIONS:\")\n",
        "    print(\"   1. ğŸ¯ Chaque environnement secret semble avoir des caractÃ©ristiques uniques\")\n",
        "    print(\"   2. ğŸ”„ L'exploration est cruciale - MC-ES peut Ãªtre avantagÃ©\")\n",
        "    print(\"   3. ğŸ“Š Surveillez les taux de succÃ¨s autant que les rÃ©compenses\")\n",
        "    print(\"   4. âš–ï¸  L'importance sampling (Off-Policy) peut Ãªtre instable sur certains environnements\")\n",
        "    print(\"   5. ğŸ›ï¸  L'ajustement des hyperparamÃ¨tres (Îµ, Î³) est critique\")\n",
        "    \n",
        "    # Sauvegarde des rÃ©sultats\n",
        "    try:\n",
        "        summary_df.to_csv('secret_env_monte_carlo_results.csv', index=False)\n",
        "        print(f\"\\\\nğŸ’¾ RÃ©sultats sauvegardÃ©s dans 'secret_env_monte_carlo_results.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur lors de la sauvegarde: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ AUCUN RÃ‰SULTAT VALIDE TROUVÃ‰\")\n",
        "    print(\"VÃ©rifiez que les environnements secrets sont accessibles et fonctionnels.\")\n",
        "\n",
        "print(\"\\\\nğŸ‰ ANALYSE MONTE CARLO TERMINÃ‰E !\")\n",
        "print(\"ğŸ•µï¸ Les mystÃ¨res des environnements secrets ont Ã©tÃ© explorÃ©s par Monte Carlo !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸ“– Guide d'Utilisation du Notebook\n",
        "\n",
        "## ğŸš€ ExÃ©cution\n",
        "1. **ExÃ©cutez les cellules dans l'ordre** (Shift+Enter pour chaque cellule)\n",
        "2. La **cellule 1** teste les imports et environnements\n",
        "3. Les **cellules 2-4** dÃ©finissent les classes et fonctions\n",
        "4. La **cellule 5** lance l'entraÃ®nement complet (â³ 5-15 minutes selon la machine)\n",
        "5. La **cellule 6** affiche tous les rÃ©sultats et graphiques\n",
        "\n",
        "## ğŸ›ï¸ ParamÃ¨tres Ajustables\n",
        "```python\n",
        "# Dans la cellule 5, modifiez cette ligne :\n",
        "EPISODES = 800  # Nombre d'Ã©pisodes par algorithme\n",
        "\n",
        "# ParamÃ¨tres des algorithmes (dans leurs constructeurs) :\n",
        "# - gamma: Facteur de discount (0.99 par dÃ©faut)\n",
        "# - epsilon: Taux d'exploration (0.3-0.4 par dÃ©faut)\n",
        "```\n",
        "\n",
        "## ğŸ“Š Outputs GÃ©nÃ©rÃ©s\n",
        "- **Courbes d'apprentissage** : RÃ©compenses, Q-values, longueur Ã©pisodes, stabilitÃ©\n",
        "- **Comparaisons globales** : Heatmaps, barres comparatives, scores composites\n",
        "- **Analyses dÃ©taillÃ©es** : MÃ©triques par algorithme et environnement\n",
        "- **Tableau rÃ©capitulatif** : Summary CSV exportable\n",
        "- **Recommandations** : Meilleurs algorithmes par environnement\n",
        "\n",
        "## ğŸ› DÃ©pannage\n",
        "- **Erreur d'import** : VÃ©rifiez que les libs natives sont prÃ©sentes dans `./libs/`\n",
        "- **Erreur de plateforme** : Le wrapper dÃ©tecte automatiquement Windows/Linux/macOS\n",
        "- **Performance lente** : RÃ©duisez `EPISODES` pour des tests rapides\n",
        "- **Manque de mÃ©moire** : Fermez d'autres applications\n",
        "\n",
        "## ğŸ“ˆ InterprÃ©tation des RÃ©sultats\n",
        "- **Taux de succÃ¨s Ã©levÃ©** : L'algorithme gÃ©nÃ¨re des Ã©pisodes valides\n",
        "- **RÃ©compenses croissantes** : L'apprentissage fonctionne\n",
        "- **Q-values qui convergent** : La politique se stabilise\n",
        "- **Faible stabilitÃ©** : Plus d'exploration nÃ©cessaire\n",
        "\n",
        "## ğŸ”§ Modifications AvancÃ©es\n",
        "Pour adapter Ã  d'autres environnements secrets :\n",
        "1. Ajoutez la classe d'environnement dans `env_classes`\n",
        "2. CrÃ©ez l'adaptateur correspondant\n",
        "3. Les algorithmes s'adaptent automatiquement\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ Bon apprentissage avec Monte Carlo sur les environnements secrets !**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
