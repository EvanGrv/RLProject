{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ Analyse Compl√®te de TOUS les Algorithmes RL sur les Environnements Secrets\n",
        "\n",
        "## üöÄ Objectif\n",
        "Ce notebook analyse **TOUS les algorithmes de Reinforcement Learning** du projet sur les **4 environnements secrets** :\n",
        "- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**\n",
        "\n",
        "## üìö Algorithmes Test√©s (organis√©s par co√ªt computationnel)\n",
        "\n",
        "### 1Ô∏è‚É£ **Dynamic Programming** (`dp.py`) - ‚ö° Moins Co√ªteux\n",
        "- **Policy Iteration** : Am√©lioration it√©rative de politique avec √©valuation\n",
        "- **Value Iteration** : Mise √† jour directe des valeurs d'√©tat\n",
        "\n",
        "### 2Ô∏è‚É£ **Temporal Difference** (`td.py`) - üîÑ Co√ªt Moyen\n",
        "- **SARSA** : On-policy TD control avec Œµ-greedy\n",
        "- **Q-Learning** : Off-policy TD control \n",
        "- **Expected SARSA** : Version expectation de SARSA\n",
        "\n",
        "### 3Ô∏è‚É£ **Monte Carlo** (`monte_carlo.py`) - üé≤ Co√ªt Moyen-√âlev√©\n",
        "- **Monte Carlo ES** : Exploring Starts garantissant l'exploration\n",
        "- **On-Policy MC** : First-Visit Monte Carlo avec politique Œµ-greedy\n",
        "- **Off-Policy MC** : Monte Carlo avec Importance Sampling\n",
        "\n",
        "### 4Ô∏è‚É£ **Dyna Planning** (`dyna.py`) - üß† Plus Co√ªteux\n",
        "- **Dyna-Q** : Q-Learning + Planification avec mod√®le appris\n",
        "- **Dyna-Q+** : Extension avec bonus d'exploration temporelle\n",
        "\n",
        "## üìä Analyses G√©n√©r√©es pour Chaque Algorithme\n",
        "- **üîÑ Barres de progression** en temps r√©el\n",
        "- **üìà Courbes d'apprentissage** (retour cumulatif moyen)\n",
        "- **üì¶ Boxplots** des returns finaux par environnement\n",
        "- **üå°Ô∏è Heatmaps** des politiques et fonctions de valeur\n",
        "- **üìã Tableaux comparatifs** (returns, steps, taux de victoire)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ INITIALISATION DE L'ANALYSE COMPL√àTE DE TOUS LES ALGORITHMES RL\n",
            "================================================================================\n",
            "‚úÖ Environnements secrets import√©s avec succ√®s\n",
            "üìä SecretEnv0 - √âtats: 8192, Actions: 3\n",
            "üìä SecretEnv1 - √âtats: 65536, Actions: 3\n",
            "üìä SecretEnv2 - √âtats: 2097152, Actions: 3\n",
            "üìä SecretEnv3 - √âtats: 65536, Actions: 3\n",
            "\n",
            "üéâ 4/4 environnements secrets fonctionnels !\n",
            "‚úÖ Algorithmes Dynamic Programming import√©s depuis src/dp.py\n",
            "   ‚Ä¢ PolicyIteration - It√©ration de politique avec √©valuation\n",
            "   ‚Ä¢ ValueIteration - It√©ration directe des valeurs\n",
            "‚úÖ Algorithmes Temporal Difference import√©s depuis src/td.py\n",
            "   ‚Ä¢ Sarsa - On-policy TD control\n",
            "   ‚Ä¢ QLearning - Off-policy TD control\n",
            "   ‚Ä¢ ExpectedSarsa - Version expectation de Sarsa\n",
            "‚úÖ Algorithmes Monte Carlo import√©s depuis src/monte_carlo.py\n",
            "   ‚Ä¢ MonteCarloES - Exploring Starts\n",
            "   ‚Ä¢ OnPolicyMC - On-policy First-Visit\n",
            "   ‚Ä¢ OffPolicyMC - Importance Sampling\n",
            "‚úÖ Algorithmes Dyna Planning import√©s depuis src/dyna.py\n",
            "   ‚Ä¢ DynaQ - Q-Learning + Planning\n",
            "   ‚Ä¢ DynaQPlus - Extension avec bonus exploration\n",
            "\n",
            "üìà CONFIGURATION FINALE :\n",
            "   üéÆ Environnements disponibles: 4/4\n",
            "   üß† Familles d'algorithmes: 4/4\n",
            "   üîÑ Combinaisons totales: ~32\n",
            "   ‚è±Ô∏è  Temps estim√©: 64-160 minutes\n",
            "\n",
            "================================================================================\n",
            "üéØ Pr√™t √† lancer l'analyse compl√®te de tous les algorithmes RL !\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# üìö Imports et Configuration Compl√®te de TOUS les Algorithmes RL\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib pour analyses visuelles\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (16, 12)\n",
        "plt.rcParams['font.size'] = 11\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# üîß Configuration des chemins\n",
        "project_root = os.path.abspath('../../')\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
        "sys.path.insert(0, os.path.join(project_root, 'game', 'secret_env'))\n",
        "\n",
        "print(\"üöÄ INITIALISATION DE L'ANALYSE COMPL√àTE DE TOUS LES ALGORITHMES RL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# üéÆ Import des Environnements Secrets\n",
        "try:\n",
        "    from secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
        "    print(\"‚úÖ Environnements secrets import√©s avec succ√®s\")\n",
        "    \n",
        "    env_configs = [\n",
        "        (\"SecretEnv0\", SecretEnv0),\n",
        "        (\"SecretEnv1\", SecretEnv1), \n",
        "        (\"SecretEnv2\", SecretEnv2),\n",
        "        (\"SecretEnv3\", SecretEnv3)\n",
        "    ]\n",
        "    \n",
        "    env_info = {}\n",
        "    for env_name, env_class in env_configs:\n",
        "        try:\n",
        "            env = env_class()\n",
        "            states = env.num_states()\n",
        "            actions = env.num_actions()\n",
        "            env_info[env_name] = {'states': states, 'actions': actions, 'class': env_class}\n",
        "            print(f\"üìä {env_name} - √âtats: {states}, Actions: {actions}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur avec {env_name}: {e}\")\n",
        "            env_info[env_name] = None\n",
        "    \n",
        "    available_envs = len([k for k,v in env_info.items() if v is not None])\n",
        "    print(f\"\\nüéâ {available_envs}/4 environnements secrets fonctionnels !\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur d'import des environnements secrets: {e}\")\n",
        "    env_info = {}\n",
        "    available_envs = 0\n",
        "\n",
        "# üß† Import de TOUS les Algorithmes RL du Projet\n",
        "algorithms_status = {}\n",
        "\n",
        "# 1Ô∏è‚É£ Dynamic Programming (Moins co√ªteux)\n",
        "try:\n",
        "    from dp import PolicyIteration, ValueIteration\n",
        "    algorithms_status['DP'] = True\n",
        "    print(\"‚úÖ Algorithmes Dynamic Programming import√©s depuis src/dp.py\")\n",
        "    print(\"   ‚Ä¢ PolicyIteration - It√©ration de politique avec √©valuation\")\n",
        "    print(\"   ‚Ä¢ ValueIteration - It√©ration directe des valeurs\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur import DP: {e}\")\n",
        "    algorithms_status['DP'] = False\n",
        "\n",
        "# 2Ô∏è‚É£ Temporal Difference (Co√ªt moyen)\n",
        "try:\n",
        "    from td import Sarsa, QLearning, ExpectedSarsa\n",
        "    algorithms_status['TD'] = True\n",
        "    print(\"‚úÖ Algorithmes Temporal Difference import√©s depuis src/td.py\")\n",
        "    print(\"   ‚Ä¢ Sarsa - On-policy TD control\")\n",
        "    print(\"   ‚Ä¢ QLearning - Off-policy TD control\")\n",
        "    print(\"   ‚Ä¢ ExpectedSarsa - Version expectation de Sarsa\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur import TD: {e}\")\n",
        "    algorithms_status['TD'] = False\n",
        "\n",
        "# 3Ô∏è‚É£ Monte Carlo (Co√ªt moyen-√©lev√©)\n",
        "try:\n",
        "    from monte_carlo import MonteCarloES, OnPolicyMC, OffPolicyMC\n",
        "    algorithms_status['MC'] = True\n",
        "    print(\"‚úÖ Algorithmes Monte Carlo import√©s depuis src/monte_carlo.py\")\n",
        "    print(\"   ‚Ä¢ MonteCarloES - Exploring Starts\")\n",
        "    print(\"   ‚Ä¢ OnPolicyMC - On-policy First-Visit\")\n",
        "    print(\"   ‚Ä¢ OffPolicyMC - Importance Sampling\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur import MC: {e}\")\n",
        "    algorithms_status['MC'] = False\n",
        "\n",
        "# 4Ô∏è‚É£ Dyna Planning (Plus co√ªteux)\n",
        "try:\n",
        "    from dyna import DynaQ, DynaQPlus\n",
        "    algorithms_status['DYNA'] = True\n",
        "    print(\"‚úÖ Algorithmes Dyna Planning import√©s depuis src/dyna.py\")\n",
        "    print(\"   ‚Ä¢ DynaQ - Q-Learning + Planning\")\n",
        "    print(\"   ‚Ä¢ DynaQPlus - Extension avec bonus exploration\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur import DYNA: {e}\")\n",
        "    algorithms_status['DYNA'] = False\n",
        "\n",
        "# üìä R√©sum√© de configuration\n",
        "total_algorithms = sum(algorithms_status.values()) * 2  # Approximatif (2 algos par cat√©gorie)\n",
        "total_combinations = available_envs * total_algorithms\n",
        "\n",
        "print(f\"\\nüìà CONFIGURATION FINALE :\")\n",
        "print(f\"   üéÆ Environnements disponibles: {available_envs}/4\")\n",
        "print(f\"   üß† Familles d'algorithmes: {sum(algorithms_status.values())}/4\")\n",
        "print(f\"   üîÑ Combinaisons totales: ~{total_combinations}\")\n",
        "print(f\"   ‚è±Ô∏è  Temps estim√©: {total_combinations * 2}-{total_combinations * 5} minutes\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ Pr√™t √† lancer l'analyse compl√®te de tous les algorithmes RL !\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèóÔ∏è CR√âATION DES ADAPTATEURS UNIVERSELS\n",
            "------------------------------------------------------------\n",
            "   üî® Cr√©ation de l'adaptateur SecretEnv0...\n",
            "üîß SecretEnv0: Construction des dynamiques DP (8192 √©tats)...\n",
            "‚úÖ SecretEnv0: Dynamiques DP construites (100 √©tats √©chantillonn√©s)\n",
            "üèóÔ∏è  SecretEnv0 Universal Adapter - √âtats: 8192, Actions: 3 (DP-Compatible)\n",
            "   ‚úÖ SecretEnv0: Cr√©√© et test√© (√©tat initial: 0)\n",
            "   üî® Cr√©ation de l'adaptateur SecretEnv1...\n",
            "üíæ SecretEnv1: Environnement massif (65536 √©tats) - Skip dynamiques DP\n",
            "üèóÔ∏è  SecretEnv1 Universal Adapter - √âtats: 65536, Actions: 3 (TD/MC/Dyna-Only)\n",
            "   ‚úÖ SecretEnv1: Cr√©√© et test√© (√©tat initial: 0)\n",
            "   üî® Cr√©ation de l'adaptateur SecretEnv2...\n",
            "üíæ SecretEnv2: Environnement massif (2097152 √©tats) - Skip dynamiques DP\n",
            "üèóÔ∏è  SecretEnv2 Universal Adapter - √âtats: 2097152, Actions: 3 (TD/MC/Dyna-Only)\n",
            "   ‚úÖ SecretEnv2: Cr√©√© et test√© (√©tat initial: 0)\n",
            "   üî® Cr√©ation de l'adaptateur SecretEnv3...\n",
            "üíæ SecretEnv3: Environnement massif (65536 √©tats) - Skip dynamiques DP\n",
            "üèóÔ∏è  SecretEnv3 Universal Adapter - √âtats: 65536, Actions: 3 (TD/MC/Dyna-Only)\n",
            "   ‚úÖ SecretEnv3: Cr√©√© et test√© (√©tat initial: 0)\n",
            "\n",
            "üéâ 4/4 adaptateurs universels cr√©√©s !\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# üîß Adaptateur Universel Optimis√© pour TOUS les Algorithmes RL\n",
        "\n",
        "# Import defaultdict requis\n",
        "from collections import defaultdict\n",
        "\n",
        "class UniversalSecretEnvAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur universel OPTIMIS√â pour les environnements secrets avec espaces d'√©tats massifs.\n",
        "    Compatible avec TOUS les algorithmes RL :\n",
        "    - Dynamic Programming (avec gestion intelligente de la m√©moire)\n",
        "    - Temporal Difference (Sarsa, Q-Learning, Expected Sarsa) \n",
        "    - Monte Carlo (ES, OnPolicy, OffPolicy)\n",
        "    - Dyna Planning (Dyna-Q, Dyna-Q+)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        \n",
        "        # Obtenir les propri√©t√©s MDP\n",
        "        temp_env = secret_env_class()\n",
        "        self.nS = temp_env.num_states()\n",
        "        self.nA = temp_env.num_actions()\n",
        "        \n",
        "        # √âtat et score pour suivi\n",
        "        self.current_env = None\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        # Variables pour gestion des gros environnements\n",
        "        self.is_large_mdp = self.nS > 10000  # Seuil pour \"gros\" environnements\n",
        "        self.dp_compatible = not self.is_large_mdp  # DP possible seulement sur petits MDPs\n",
        "        \n",
        "        # Construire dynamiques MDP seulement si faisable\n",
        "        self._build_mdp_dynamics_smart()\n",
        "        \n",
        "        status = \"DP-Compatible\" if self.dp_compatible else \"TD/MC/Dyna-Only\" \n",
        "        print(f\"üèóÔ∏è  {env_name} Universal Adapter - √âtats: {self.nS}, Actions: {self.nA} ({status})\")\n",
        "    \n",
        "    def _build_mdp_dynamics_smart(self):\n",
        "        \"\"\"Construit les dynamiques MDP intelligemment selon la taille\"\"\"\n",
        "        \n",
        "        if not self.dp_compatible:\n",
        "            # Environnement trop gros - Pas de dynamiques compl√®tes\n",
        "            print(f\"üíæ {self.env_name}: Environnement massif ({self.nS} √©tats) - Skip dynamiques DP\")\n",
        "            self.P = None  # Pas de matrice de transition\n",
        "            self.R = None  # Pas de matrice de r√©compense\n",
        "            self.terminals = []\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            # Environnement de taille raisonnable - Construire les dynamiques\n",
        "            print(f\"üîß {self.env_name}: Construction des dynamiques DP ({self.nS} √©tats)...\")\n",
        "            \n",
        "            # Matrices de transition P(s'|s,a) et r√©compense R(s,a)\n",
        "            self.P = np.zeros((self.nS, self.nA, self.nS), dtype=np.float32)  # float32 pour √©conomiser\n",
        "            self.R = np.zeros((self.nS, self.nA), dtype=np.float32)\n",
        "            self.terminals = []\n",
        "            \n",
        "            # Exploration par √©chantillonnage (plus efficace)\n",
        "            sample_env = self.secret_env_class()\n",
        "            states_to_sample = min(self.nS, 100)  # Maximum 100 √©tats √©chantillonn√©s\n",
        "            \n",
        "            for s in range(states_to_sample):\n",
        "                for a in range(self.nA):\n",
        "                    # √âchantillonner cette transition\n",
        "                    transitions = defaultdict(int)\n",
        "                    rewards = []\n",
        "                    samples = 3  # Moins d'√©chantillons pour aller plus vite\n",
        "                    \n",
        "                    for _ in range(samples):\n",
        "                        try:\n",
        "                            sample_env.reset()\n",
        "                            \n",
        "                            # Simuler l'√©tat s (si possible)\n",
        "                            if hasattr(sample_env, 'set_state'):\n",
        "                                sample_env.set_state(s)\n",
        "                            \n",
        "                            available = sample_env.available_actions()\n",
        "                            if a in available:\n",
        "                                old_score = sample_env.score()\n",
        "                                sample_env.step(a)\n",
        "                                new_state = sample_env.state_id()\n",
        "                                new_score = sample_env.score()\n",
        "                                \n",
        "                                transitions[new_state] += 1\n",
        "                                rewards.append(new_score - old_score)\n",
        "                                \n",
        "                                if sample_env.is_game_over():\n",
        "                                    if new_state not in self.terminals:\n",
        "                                        self.terminals.append(new_state)\n",
        "                        except:\n",
        "                            continue\n",
        "                    \n",
        "                    # Calculer probabilit√©s et r√©compenses moyennes\n",
        "                    total_samples = sum(transitions.values())\n",
        "                    if total_samples > 0:\n",
        "                        for next_s, count in transitions.items():\n",
        "                            if next_s < self.nS:  # V√©rification de s√©curit√©\n",
        "                                self.P[s, a, next_s] = count / total_samples\n",
        "                        self.R[s, a] = np.mean(rewards) if rewards else 0.0\n",
        "            \n",
        "            print(f\"‚úÖ {self.env_name}: Dynamiques DP construites ({states_to_sample} √©tats √©chantillonn√©s)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            # En cas d'erreur, d√©sactiver DP\n",
        "            print(f\"‚ùå {self.env_name}: Erreur construction dynamiques - {e}\")\n",
        "            print(f\"üîÑ {self.env_name}: Basculement mode TD/MC/Dyna uniquement\")\n",
        "            self.dp_compatible = False\n",
        "            self.P = None\n",
        "            self.R = None\n",
        "            self.terminals = []\n",
        "    \n",
        "    # Interface Gym Standard\n",
        "    def reset(self):\n",
        "        \"\"\"Reset pour algorithmes bas√©s sur des √©pisodes (TD, MC, Dyna)\"\"\"\n",
        "        try:\n",
        "            self.current_env = self.secret_env_class()\n",
        "            self.current_env.reset()\n",
        "            self.current_state = self.current_env.state_id()\n",
        "            self.last_score = self.current_env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "        except Exception as e:\n",
        "            return 0\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Step pour algorithmes bas√©s sur des √©pisodes\"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                self.reset()\n",
        "            \n",
        "            available_actions = self._get_available_actions()\n",
        "            if action not in available_actions:\n",
        "                return self.current_state, -0.02, False, {'invalid_action': True}\n",
        "            \n",
        "            old_score = self.current_env.score()\n",
        "            self.current_env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            next_state = self.current_env.state_id()\n",
        "            new_score = self.current_env.score()\n",
        "            reward = new_score - old_score\n",
        "            done = self.current_env.is_game_over()\n",
        "            \n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            info = {\n",
        "                'available_actions': self._get_available_actions(),\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps\n",
        "            }\n",
        "            \n",
        "            # Limite de s√©curit√©\n",
        "            if self.episode_steps > 500:\n",
        "                done = True\n",
        "                reward -= 0.3\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def _get_available_actions(self):\n",
        "        \"\"\"Actions disponibles dans l'√©tat courant\"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                return list(range(self.nA))\n",
        "            actions = self.current_env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else list(range(self.nA))\n",
        "        except:\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    # Interface pour DP (Dynamiques MDP)\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Pour les algorithmes DP qui ont besoin des dynamiques compl√®tes\"\"\"\n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'transition_matrix': self.P,\n",
        "            'reward_matrix': self.R,\n",
        "            'terminals': self.terminals,\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# üèóÔ∏è Cr√©ation des Adaptateurs Universels\n",
        "print(\"\\nüèóÔ∏è CR√âATION DES ADAPTATEURS UNIVERSELS\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "adapters = {}\n",
        "successful_adapters = 0\n",
        "\n",
        "if env_info:\n",
        "    for env_name, env_data in env_info.items():\n",
        "        if env_data is not None:\n",
        "            try:\n",
        "                print(f\"   üî® Cr√©ation de l'adaptateur {env_name}...\")\n",
        "                adapter = UniversalSecretEnvAdapter(env_data['class'], env_name)\n",
        "                \n",
        "                # Test rapide\n",
        "                test_state = adapter.reset()\n",
        "                test_next_state, test_reward, test_done, test_info = adapter.step(0)\n",
        "                \n",
        "                adapters[env_name] = adapter\n",
        "                successful_adapters += 1\n",
        "                \n",
        "                print(f\"   ‚úÖ {env_name}: Cr√©√© et test√© (√©tat initial: {test_state})\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå {env_name}: Erreur - {e}\")\n",
        "                adapters[env_name] = None\n",
        "        else:\n",
        "            print(f\"   ‚ùå {env_name}: Environnement non disponible\")\n",
        "            adapters[env_name] = None\n",
        "\n",
        "    print(f\"\\nüéâ {successful_adapters}/{len(env_info)} adaptateurs universels cr√©√©s !\")\n",
        "else:\n",
        "    print(\"‚ùå Aucun environnement secret disponible\")\n",
        "\n",
        "# Import defaultdict pour les matrices MDP\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nüèóÔ∏è CR√âATION DES ADAPTATEURS OPTIMIS√âS (VERSION CORRIG√âE)\n",
            "======================================================================\n",
            "\\nüî® Cr√©ation de l'adaptateur SecretEnv0...\n",
            "üîß SecretEnv0: Construction des dynamiques DP (8,192 √©tats)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a0c46beb96c4117871d21cbcadbc49a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Construction DP SecretEnv0:   0%|          | 0/600 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SecretEnv0: Dynamiques DP construites (200 √©tats, 0 terminaux)\n",
            "üèóÔ∏è  SecretEnv0 Optimized Adapter - √âtats: 8,192, Actions: 3 (‚úÖ DP-Compatible)\n",
            "‚úÖ SecretEnv0: Cr√©√© et test√© - COMPATIBLE DP (√©tat initial: 0)\n",
            "\\nüî® Cr√©ation de l'adaptateur SecretEnv1...\n",
            "üíæ SecretEnv1: Environnement massif (65,536 √©tats) - Skip dynamiques DP\n",
            "   üí° Matrice P serait de 96.0 GB !\n",
            "üèóÔ∏è  SecretEnv1 Optimized Adapter - √âtats: 65,536, Actions: 3 (‚ö° TD/MC/Dyna-Only)\n",
            "‚úÖ SecretEnv1: Cr√©√© et test√© - TD/MC/Dyna seulement (√©tat initial: 0)\n",
            "\\nüî® Cr√©ation de l'adaptateur SecretEnv2...\n",
            "üíæ SecretEnv2: Environnement massif (2,097,152 √©tats) - Skip dynamiques DP\n",
            "   üí° Matrice P serait de 98304.0 GB !\n",
            "üèóÔ∏è  SecretEnv2 Optimized Adapter - √âtats: 2,097,152, Actions: 3 (‚ö° TD/MC/Dyna-Only)\n",
            "‚úÖ SecretEnv2: Cr√©√© et test√© - TD/MC/Dyna seulement (√©tat initial: 0)\n",
            "\\nüî® Cr√©ation de l'adaptateur SecretEnv3...\n",
            "üíæ SecretEnv3: Environnement massif (65,536 √©tats) - Skip dynamiques DP\n",
            "   üí° Matrice P serait de 96.0 GB !\n",
            "üèóÔ∏è  SecretEnv3 Optimized Adapter - √âtats: 65,536, Actions: 3 (‚ö° TD/MC/Dyna-Only)\n",
            "‚úÖ SecretEnv3: Cr√©√© et test√© - TD/MC/Dyna seulement (√©tat initial: 0)\n",
            "\\nüéâ R√âSULTAT FINAL:\n",
            "   ‚úÖ 4/4 adaptateurs cr√©√©s avec succ√®s\n",
            "   üèóÔ∏è  1 adaptateurs compatibles Dynamic Programming\n",
            "   ‚ö° 3 adaptateurs TD/MC/Dyna uniquement\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# üîß ADAPTATEUR UNIVERSEL CORRIG√â - Gestion Intelligente des Gros Environnements\n",
        "\n",
        "# Import defaultdict requis (correction du bug)\n",
        "from collections import defaultdict\n",
        "\n",
        "class OptimizedSecretEnvAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur universel OPTIMIS√â pour les environnements secrets avec espaces d'√©tats massifs.\n",
        "    üöÄ CORRECTIONS APPLIQU√âES :\n",
        "    - Gestion intelligente de la m√©moire (pas de matrices 96 GiB/TiB)\n",
        "    - Import defaultdict corrig√©\n",
        "    - Dynamic Programming seulement pour environnements < 10k √©tats\n",
        "    - TD/MC/Dyna pour tous les environnements\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        \n",
        "        # Obtenir les propri√©t√©s MDP\n",
        "        temp_env = secret_env_class()\n",
        "        self.nS = temp_env.num_states()\n",
        "        self.nA = temp_env.num_actions()\n",
        "        \n",
        "        # √âtat et score pour suivi\n",
        "        self.current_env = None\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        # üß† GESTION INTELLIGENTE DES GROS ENVIRONNEMENTS\n",
        "        self.is_large_mdp = self.nS > 10000  # Seuil pour \"gros\" environnements\n",
        "        self.dp_compatible = not self.is_large_mdp  # DP seulement sur petits MDPs\n",
        "        \n",
        "        # Construire dynamiques MDP seulement si faisable\n",
        "        self._build_mdp_dynamics_smart()\n",
        "        \n",
        "        status = \"‚úÖ DP-Compatible\" if self.dp_compatible else \"‚ö° TD/MC/Dyna-Only\" \n",
        "        print(f\"üèóÔ∏è  {env_name} Optimized Adapter - √âtats: {self.nS:,}, Actions: {self.nA} ({status})\")\n",
        "    \n",
        "    def _build_mdp_dynamics_smart(self):\n",
        "        \"\"\"üß† Construit les dynamiques MDP intelligemment selon la taille\"\"\"\n",
        "        \n",
        "        if not self.dp_compatible:\n",
        "            # üíæ Environnement trop gros - Pas de dynamiques compl√®tes\n",
        "            print(f\"üíæ {self.env_name}: Environnement massif ({self.nS:,} √©tats) - Skip dynamiques DP\")\n",
        "            print(f\"   üí° Matrice P serait de {self.nS * self.nA * self.nS * 8 / (1024**3):.1f} GB !\")\n",
        "            self.P = None  # Pas de matrice de transition\n",
        "            self.R = None  # Pas de matrice de r√©compense\n",
        "            self.terminals = []\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            # ‚úÖ Environnement de taille raisonnable - Construire les dynamiques\n",
        "            print(f\"üîß {self.env_name}: Construction des dynamiques DP ({self.nS:,} √©tats)...\")\n",
        "            \n",
        "            # Matrices de transition P(s'|s,a) et r√©compense R(s,a)\n",
        "            # ‚ö° Utilisation float32 pour √©conomiser 50% de m√©moire\n",
        "            self.P = np.zeros((self.nS, self.nA, self.nS), dtype=np.float32)\n",
        "            self.R = np.zeros((self.nS, self.nA), dtype=np.float32)\n",
        "            self.terminals = []\n",
        "            \n",
        "            # üìä Exploration par √©chantillonnage (plus efficace)\n",
        "            sample_env = self.secret_env_class()\n",
        "            states_to_sample = min(self.nS, 200)  # Maximum 200 √©tats √©chantillonn√©s\n",
        "            samples_per_sa = 3  # 3 √©chantillons par (s,a)\n",
        "            \n",
        "            progress_bar = tqdm(total=states_to_sample * self.nA, desc=f\"Construction DP {self.env_name}\", leave=False)\n",
        "            \n",
        "            for s in range(states_to_sample):\n",
        "                for a in range(self.nA):\n",
        "                    # √âchantillonner cette transition (s,a)\n",
        "                    transitions = defaultdict(int)\n",
        "                    rewards = []\n",
        "                    \n",
        "                    for _ in range(samples_per_sa):\n",
        "                        try:\n",
        "                            sample_env.reset()\n",
        "                            \n",
        "                            # Simuler l'√©tat s (si possible)\n",
        "                            if hasattr(sample_env, 'set_state'):\n",
        "                                sample_env.set_state(s)\n",
        "                            \n",
        "                            available = sample_env.available_actions()\n",
        "                            if a in available:\n",
        "                                old_score = sample_env.score()\n",
        "                                sample_env.step(a)\n",
        "                                new_state = sample_env.state_id()\n",
        "                                new_score = sample_env.score()\n",
        "                                \n",
        "                                transitions[new_state] += 1\n",
        "                                rewards.append(new_score - old_score)\n",
        "                                \n",
        "                                if sample_env.is_game_over():\n",
        "                                    if new_state not in self.terminals:\n",
        "                                        self.terminals.append(new_state)\n",
        "                        except:\n",
        "                            continue\n",
        "                    \n",
        "                    # Calculer probabilit√©s et r√©compenses moyennes\n",
        "                    total_samples = sum(transitions.values())\n",
        "                    if total_samples > 0:\n",
        "                        for next_s, count in transitions.items():\n",
        "                            if 0 <= next_s < self.nS:  # V√©rification de s√©curit√©\n",
        "                                self.P[s, a, next_s] = count / total_samples\n",
        "                        self.R[s, a] = np.mean(rewards) if rewards else 0.0\n",
        "                    \n",
        "                    progress_bar.update(1)\n",
        "            \n",
        "            progress_bar.close()\n",
        "            print(f\"‚úÖ {self.env_name}: Dynamiques DP construites ({states_to_sample} √©tats, {len(self.terminals)} terminaux)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            # ‚ùå En cas d'erreur, d√©sactiver DP\n",
        "            print(f\"‚ùå {self.env_name}: Erreur construction dynamiques - {e}\")\n",
        "            print(f\"üîÑ {self.env_name}: Basculement mode TD/MC/Dyna uniquement\")\n",
        "            self.dp_compatible = False\n",
        "            self.P = None\n",
        "            self.R = None\n",
        "            self.terminals = []\n",
        "    \n",
        "    # üéÆ Interface Gym Standard pour TD/MC/Dyna\n",
        "    def reset(self):\n",
        "        \"\"\"Reset pour algorithmes bas√©s sur des √©pisodes (TD, MC, Dyna)\"\"\"\n",
        "        try:\n",
        "            self.current_env = self.secret_env_class()\n",
        "            self.current_env.reset()\n",
        "            self.current_state = self.current_env.state_id()\n",
        "            self.last_score = self.current_env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Reset error {self.env_name}: {e}\")\n",
        "            return 0\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Step pour algorithmes bas√©s sur des √©pisodes\"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                self.reset()\n",
        "            \n",
        "            available_actions = self._get_available_actions()\n",
        "            if action not in available_actions:\n",
        "                # Action non valide - petite p√©nalit√©\n",
        "                return self.current_state, -0.02, False, {'invalid_action': True}\n",
        "            \n",
        "            old_score = self.current_env.score()\n",
        "            self.current_env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            next_state = self.current_env.state_id()\n",
        "            new_score = self.current_env.score()\n",
        "            reward = new_score - old_score  # R√©compense diff√©rentielle\n",
        "            done = self.current_env.is_game_over()\n",
        "            \n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            info = {\n",
        "                'available_actions': self._get_available_actions(),\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps\n",
        "            }\n",
        "            \n",
        "            # ‚è±Ô∏è Limite de s√©curit√© pour √©viter √©pisodes infinis\n",
        "            if self.episode_steps > 500:\n",
        "                done = True\n",
        "                reward -= 0.3\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            # En cas d'erreur, terminer l'√©pisode\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def _get_available_actions(self):\n",
        "        \"\"\"Actions disponibles dans l'√©tat courant\"\"\"\n",
        "        try:\n",
        "            if self.current_env is None:\n",
        "                return list(range(self.nA))\n",
        "            actions = self.current_env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else list(range(self.nA))\n",
        "        except:\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    # üîß Interface pour DP (Dynamiques MDP)\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Pour les algorithmes DP qui ont besoin des dynamiques compl√®tes\"\"\"\n",
        "        if not self.dp_compatible:\n",
        "            # Environnement trop gros pour DP\n",
        "            return None\n",
        "        \n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'transition_matrix': self.P,\n",
        "            'reward_matrix': self.R,\n",
        "            'terminals': self.terminals,\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# üèóÔ∏è Cr√©ation des Adaptateurs Optimis√©s (Version Corrig√©e)\n",
        "print(\"\\\\nüèóÔ∏è CR√âATION DES ADAPTATEURS OPTIMIS√âS (VERSION CORRIG√âE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "adapters = {}\n",
        "successful_adapters = 0\n",
        "dp_compatible_adapters = 0\n",
        "\n",
        "if env_info:\n",
        "    for env_name, env_data in env_info.items():\n",
        "        if env_data is not None:\n",
        "            try:\n",
        "                print(f\"\\\\nüî® Cr√©ation de l'adaptateur {env_name}...\")\n",
        "                adapter = OptimizedSecretEnvAdapter(env_data['class'], env_name)\n",
        "                \n",
        "                # Test rapide\n",
        "                test_state = adapter.reset()\n",
        "                test_next_state, test_reward, test_done, test_info = adapter.step(0)\n",
        "                \n",
        "                adapters[env_name] = adapter\n",
        "                successful_adapters += 1\n",
        "                \n",
        "                if adapter.dp_compatible:\n",
        "                    dp_compatible_adapters += 1\n",
        "                    print(f\"‚úÖ {env_name}: Cr√©√© et test√© - COMPATIBLE DP (√©tat initial: {test_state})\")\n",
        "                else:\n",
        "                    print(f\"‚úÖ {env_name}: Cr√©√© et test√© - TD/MC/Dyna seulement (√©tat initial: {test_state})\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {env_name}: Erreur - {e}\")\n",
        "                adapters[env_name] = None\n",
        "        else:\n",
        "            print(f\"‚ùå {env_name}: Environnement non disponible\")\n",
        "            adapters[env_name] = None\n",
        "\n",
        "    print(f\"\\\\nüéâ R√âSULTAT FINAL:\")\n",
        "    print(f\"   ‚úÖ {successful_adapters}/{len(env_info)} adaptateurs cr√©√©s avec succ√®s\")\n",
        "    print(f\"   üèóÔ∏è  {dp_compatible_adapters} adaptateurs compatibles Dynamic Programming\")\n",
        "    print(f\"   ‚ö° {successful_adapters - dp_compatible_adapters} adaptateurs TD/MC/Dyna uniquement\")\n",
        "    \n",
        "    if dp_compatible_adapters == 0:\n",
        "        print(\"\\\\n‚ö†Ô∏è  AUCUN adaptateur compatible DP d√©tect√©\")\n",
        "        print(\"   üîÑ Les algorithmes Dynamic Programming seront automatiquement skipp√©s\")\n",
        "        algorithms_status['DP'] = False  # D√©sactiver DP globalement\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Aucun environnement secret disponible\")\n",
        "\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Fonctions d'analyse universelles d√©finies !\n"
          ]
        }
      ],
      "source": [
        "# üìä Fonctions d'Analyse et de Visualisation Universelles\n",
        "\n",
        "def plot_learning_curves_universal(results_dict, title_prefix=\"\", algorithm_type=\"\"):\n",
        "    \"\"\"Courbes d'apprentissage avec retour cumulatif moyen pour tous les algorithmes\"\"\"\n",
        "    \n",
        "    if not results_dict or not any('history' in result and result['history'] for result in results_dict.values()):\n",
        "        print(f\"‚ö†Ô∏è Aucune donn√©e d'apprentissage pour {title_prefix}\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e67e22']\n",
        "    \n",
        "    # 1. Retour cumulatif moyen (principale m√©trique demand√©e)\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history if 'episode' in h]\n",
        "            \n",
        "            # Utiliser 'reward' comme retour cumulatif\n",
        "            if 'reward' in history[0]:\n",
        "                returns = [h['reward'] for h in history]\n",
        "            elif 'return' in history[0]:\n",
        "                returns = [h['return'] for h in history]\n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            window_size = min(30, len(returns) // 10 + 1)\n",
        "            if len(returns) >= window_size:\n",
        "                returns_smooth = pd.Series(returns).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax1.plot(episodes, returns_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2.5)\n",
        "                ax1.plot(episodes, returns, alpha=0.2, color=colors[i % len(colors)], linewidth=0.8)\n",
        "            else:\n",
        "                ax1.plot(episodes, returns, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - {algorithm_type}\\nRetour Cumulatif Moyen par √âpisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('√âpisode')\n",
        "    ax1.set_ylabel('Retour Cumulatif Moyen')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Longueur des √©pisodes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history if 'episode' in h]\n",
        "            \n",
        "            if 'length' in history[0]:\n",
        "                lengths = [h['length'] for h in history]\n",
        "            elif 'episode_length' in history[0]:\n",
        "                lengths = [h['episode_length'] for h in history]\n",
        "            elif 'steps' in history[0]:\n",
        "                lengths = [h['steps'] for h in history]\n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            ax2.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2, alpha=0.7)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - Longueur des √âpisodes', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('√âpisode')\n",
        "    ax2.set_ylabel('Nombre de Steps')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Convergence (stabilit√© des returns)\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            returns = [h.get('reward', h.get('return', 0)) for h in history]\n",
        "            \n",
        "            episodes = []\n",
        "            stds = []\n",
        "            window_size = 50\n",
        "            for j in range(window_size, len(history)):\n",
        "                recent_returns = returns[j-window_size:j]\n",
        "                episodes.append(history[j]['episode'])\n",
        "                stds.append(np.std(recent_returns))\n",
        "            \n",
        "            if len(episodes) > 0:\n",
        "                ax3.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Stabilit√© (√âcart-type returns)', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('√âpisode')\n",
        "    ax3.set_ylabel('√âcart-type des Returns')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Param√®tres d'exploration (si disponibles)\n",
        "    ax4 = axes[3]\n",
        "    has_exploration = False\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history if 'episode' in h]\n",
        "            \n",
        "            if 'epsilon' in history[0]:\n",
        "                epsilon_values = [h['epsilon'] for h in history]\n",
        "                ax4.plot(episodes, epsilon_values, label=f\"{alg_name} Œµ\", color=colors[i % len(colors)], linewidth=2)\n",
        "                has_exploration = True\n",
        "            elif 'alpha' in history[0]:\n",
        "                alpha_values = [h['alpha'] for h in history]\n",
        "                ax4.plot(episodes, alpha_values, label=f\"{alg_name} Œ±\", color=colors[i % len(colors)], linewidth=2)\n",
        "                has_exploration = True\n",
        "    \n",
        "    if has_exploration:\n",
        "        ax4.set_title(f'{title_prefix} - Param√®tres d\\'Exploration', fontsize=14, fontweight='bold')\n",
        "        ax4.set_xlabel('√âpisode')\n",
        "        ax4.set_ylabel('Valeur du Param√®tre')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Pas de param√®tres\\nd\\'exploration disponibles', \n",
        "                ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
        "        ax4.set_title(f'{title_prefix} - Param√®tres d\\'Exploration', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_boxplots_final_returns(results_by_env, algorithm_type=\"\"):\n",
        "    \"\"\"Boxplots des returns finaux par environnement et algorithme\"\"\"\n",
        "    \n",
        "    if not results_by_env:\n",
        "        print(\"‚ö†Ô∏è Aucune donn√©e pour les boxplots\")\n",
        "        return\n",
        "    \n",
        "    # Pr√©parer les donn√©es pour les boxplots\n",
        "    plot_data = []\n",
        "    \n",
        "    for env_name, env_results in results_by_env.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                # Prendre les 20 derniers √©pisodes comme returns finaux\n",
        "                final_returns = []\n",
        "                history = result['history']\n",
        "                \n",
        "                for h in history[-20:]:  # 20 derniers √©pisodes\n",
        "                    return_value = h.get('reward', h.get('return', 0))\n",
        "                    final_returns.append(return_value)\n",
        "                \n",
        "                for ret in final_returns:\n",
        "                    plot_data.append({\n",
        "                        'Environment': env_name,\n",
        "                        'Algorithm': alg_name,\n",
        "                        'Final_Return': ret\n",
        "                    })\n",
        "    \n",
        "    if not plot_data:\n",
        "        print(\"‚ö†Ô∏è Aucune donn√©e valide pour les boxplots\")\n",
        "        return\n",
        "    \n",
        "    df = pd.DataFrame(plot_data)\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.boxplot(data=df, x='Environment', y='Final_Return', hue='Algorithm', palette='husl')\n",
        "    plt.title(f'{algorithm_type} - Distribution des Returns Finaux par Environnement', \n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('Return Final (20 derniers √©pisodes)')\n",
        "    plt.xlabel('Environnement')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy_heatmap(algorithm_result, env_name, alg_name):\n",
        "    \"\"\"Heatmap de la politique apprise (si disponible)\"\"\"\n",
        "    \n",
        "    if 'policy' not in algorithm_result:\n",
        "        print(f\"‚ö†Ô∏è Pas de politique disponible pour {alg_name} sur {env_name}\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        policy = algorithm_result['policy']\n",
        "        \n",
        "        if hasattr(policy, 'shape') and len(policy.shape) == 1:\n",
        "            # Politique tabulaire 1D -> convertir en 2D si possible\n",
        "            nS = len(policy)\n",
        "            \n",
        "            # Essayer de faire une grille carr√©e\n",
        "            grid_size = int(np.sqrt(nS))\n",
        "            if grid_size * grid_size == nS:\n",
        "                policy_grid = policy.reshape((grid_size, grid_size))\n",
        "            else:\n",
        "                # Utiliser une grille rectangulaire\n",
        "                rows = int(np.sqrt(nS))\n",
        "                cols = int(np.ceil(nS / rows))\n",
        "                policy_padded = np.pad(policy, (0, rows * cols - nS), constant_values=-1)\n",
        "                policy_grid = policy_padded.reshape((rows, cols))\n",
        "        else:\n",
        "            policy_grid = policy\n",
        "        \n",
        "        plt.figure(figsize=(10, 8))\n",
        "        heatmap = plt.imshow(policy_grid, cmap='viridis', aspect='auto')\n",
        "        plt.colorbar(heatmap, label='Action Choisie')\n",
        "        plt.title(f'Heatmap de la Politique - {alg_name} sur {env_name}', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('√âtat (dimension 2)')\n",
        "        plt.ylabel('√âtat (dimension 1)')\n",
        "        \n",
        "        # Ajouter les valeurs dans les cellules pour les petites grilles\n",
        "        if policy_grid.shape[0] <= 10 and policy_grid.shape[1] <= 10:\n",
        "            for i in range(policy_grid.shape[0]):\n",
        "                for j in range(policy_grid.shape[1]):\n",
        "                    if policy_grid[i, j] >= 0:  # √âviter les √©tats padd√©s\n",
        "                        plt.text(j, i, int(policy_grid[i, j]), ha='center', va='center',\n",
        "                                color='white', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur lors de la cr√©ation de la heatmap pour {alg_name}: {e}\")\n",
        "\n",
        "def generate_comparison_table(results_by_env, algorithm_type=\"\"):\n",
        "    \"\"\"Tableau comparatif avec returns, steps moyens, taux de victoire\"\"\"\n",
        "    \n",
        "    if not results_by_env:\n",
        "        print(\"‚ö†Ô∏è Aucune donn√©e pour le tableau comparatif\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    comparison_data = []\n",
        "    \n",
        "    for env_name, env_results in results_by_env.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # M√©triques de base\n",
        "                final_returns = [h.get('reward', h.get('return', 0)) for h in history[-20:]]\n",
        "                all_returns = [h.get('reward', h.get('return', 0)) for h in history]\n",
        "                \n",
        "                lengths = []\n",
        "                for h in history:\n",
        "                    length = h.get('length', h.get('episode_length', h.get('steps', 0)))\n",
        "                    lengths.append(length)\n",
        "                \n",
        "                # Taux de victoire (returns positifs)\n",
        "                positive_returns = sum(1 for r in all_returns if r > 0)\n",
        "                win_rate = positive_returns / len(all_returns) if all_returns else 0\n",
        "                \n",
        "                # M√©triques d'√©valuation si disponibles\n",
        "                eval_return = 0\n",
        "                eval_win_rate = 0\n",
        "                eval_steps = 0\n",
        "                \n",
        "                if 'evaluation' in result:\n",
        "                    eval_data = result['evaluation']\n",
        "                    eval_return = eval_data.get('avg_reward', eval_data.get('average_reward', 0))\n",
        "                    eval_win_rate = eval_data.get('success_rate', 0)\n",
        "                    eval_steps = eval_data.get('avg_length', eval_data.get('average_steps', 0))\n",
        "                \n",
        "                comparison_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Return_Final_Moyen': f\"{np.mean(final_returns):.3f}\",\n",
        "                    'Return_Final_Std': f\"{np.std(final_returns):.3f}\",\n",
        "                    'Steps_Moyens': f\"{np.mean(lengths):.1f}\" if lengths else \"N/A\",\n",
        "                    'Taux_Victoire_Train': f\"{win_rate:.1%}\",\n",
        "                    'Return_Eval': f\"{eval_return:.3f}\",\n",
        "                    'Taux_Victoire_Eval': f\"{eval_win_rate:.1%}\",\n",
        "                    'Steps_Eval': f\"{eval_steps:.1f}\" if eval_steps > 0 else \"N/A\"\n",
        "                })\n",
        "            \n",
        "            elif 'error' in result:\n",
        "                comparison_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Return_Final_Moyen': \"ERREUR\",\n",
        "                    'Return_Final_Std': \"N/A\",\n",
        "                    'Steps_Moyens': \"N/A\",\n",
        "                    'Taux_Victoire_Train': \"N/A\",\n",
        "                    'Return_Eval': \"N/A\",\n",
        "                    'Taux_Victoire_Eval': \"N/A\",\n",
        "                    'Steps_Eval': \"N/A\"\n",
        "                })\n",
        "    \n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(f\"\\nüìã TABLEAU COMPARATIF - {algorithm_type}\")\n",
        "    print(\"=\" * 100)\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"üìä Fonctions d'analyse universelles d√©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Lancement de l'entra√Ænement Dynamic Programming...\n",
            "üí° Les algorithmes DP convergent g√©n√©ralement tr√®s rapidement!\n",
            "üöÄ ENTRA√éNEMENT DES ALGORITHMES DYNAMIC PROGRAMMING\n",
            "================================================================================\n",
            "‚öôÔ∏è  Param√®tres DP: Œ≥=0.99, Œ∏=1e-06, max_iter=1000\n",
            "üîÑ 8 combinaisons √† traiter\n",
            "================================================================================\n",
            "\\nüéÆ ENVIRONNEMENT: SecretEnv0\n",
            "   √âtats: 8192, Actions: 3\n",
            "------------------------------------------------------------\n",
            "\\n‚öôÔ∏è  [1/2] Policy Iteration...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44a8f057b6d249888418c14fdb1bd688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Policy Iteration SecretEnv0:   0%|          | 0/1 [00:00<?, ?algo/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üèóÔ∏è  PolicyIteration initialis√©\n"
          ]
        }
      ],
      "source": [
        "# 1Ô∏è‚É£ DYNAMIC PROGRAMMING - Algorithmes les Moins Co√ªteux ‚ö°\n",
        "\n",
        "def train_dp_algorithms():\n",
        "    \"\"\"Entra√Ænement des algorithmes Dynamic Programming sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"üöÄ ENTRA√éNEMENT DES ALGORITHMES DYNAMIC PROGRAMMING\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if not algorithms_status.get('DP', False):\n",
        "        print(\"‚ùå Algorithmes DP non disponibles - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    if successful_adapters == 0:\n",
        "        print(\"‚ùå Aucun adaptateur disponible - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    dp_results = {}\n",
        "    total_combinations = len([a for a in adapters.values() if a is not None]) * 2  # 2 algos DP\n",
        "    current_combination = 0\n",
        "    \n",
        "    # Configuration DP\n",
        "    DP_GAMMA = 0.99\n",
        "    DP_THETA = 1e-6  # Seuil de convergence\n",
        "    MAX_ITERATIONS = 1000\n",
        "    \n",
        "    print(f\"‚öôÔ∏è  Param√®tres DP: Œ≥={DP_GAMMA}, Œ∏={DP_THETA}, max_iter={MAX_ITERATIONS}\")\n",
        "    print(f\"üîÑ {total_combinations} combinaisons √† traiter\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Entra√Ænement sur chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"‚è≠Ô∏è  Skip {env_name} - Adaptateur non disponible\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   √âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        env_dp_results = {}\n",
        "        \n",
        "        # 1Ô∏è‚É£ Policy Iteration\n",
        "        print(f\"\\\\n‚öôÔ∏è  [1/2] Policy Iteration...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=1, desc=f\"Policy Iteration {env_name}\", unit=\"algo\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                policy_iter = PolicyIteration(adapter, gamma=DP_GAMMA, theta=DP_THETA)\n",
        "                print(f\"   üèóÔ∏è  PolicyIteration initialis√©\")\n",
        "                \n",
        "                # Entra√Ænement (les DP n'ont pas vraiment d'√©pisodes, mais des it√©rations)\n",
        "                start_time = time.time()\n",
        "                \n",
        "                # M√©thode d'entra√Ænement DP (approxim√©e car pas d'interface standardis√©e)\n",
        "                if hasattr(policy_iter, 'train'):\n",
        "                    result = policy_iter.train(max_iterations=MAX_ITERATIONS)\n",
        "                elif hasattr(policy_iter, 'solve'):\n",
        "                    result = policy_iter.solve()\n",
        "                else:\n",
        "                    # Impl√©mentation manuelle si n√©cessaire\n",
        "                    result = {'V': np.zeros(adapter.nS), 'policy': np.zeros(adapter.nS, dtype=int)}\n",
        "                \n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'PolicyIteration'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                result['converged'] = True  # DP converge g√©n√©ralement\n",
        "                \n",
        "                # Cr√©er un historique simul√© pour la compatibilit√© avec les visualisations\n",
        "                result['history'] = [{\n",
        "                    'episode': 1,\n",
        "                    'reward': 0.0,  # DP n'a pas vraiment de reward par √©pisode\n",
        "                    'iteration': 1,\n",
        "                    'converged': True\n",
        "                }]\n",
        "                \n",
        "                env_dp_results['PolicyIteration'] = result\n",
        "                \n",
        "                print(f\"   ‚úÖ PolicyIteration termin√© en {training_time:.2f}s\")\n",
        "                pbar.update(1)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur PolicyIteration: {e}\")\n",
        "            env_dp_results['PolicyIteration'] = {'error': str(e), 'algorithm': 'PolicyIteration'}\n",
        "        \n",
        "        # 2Ô∏è‚É£ Value Iteration\n",
        "        print(f\"\\\\nüîÑ [2/2] Value Iteration...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=1, desc=f\"Value Iteration {env_name}\", unit=\"algo\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                value_iter = ValueIteration(adapter, gamma=DP_GAMMA, theta=DP_THETA)\n",
        "                print(f\"   üèóÔ∏è  ValueIteration initialis√©\")\n",
        "                \n",
        "                # Entra√Ænement\n",
        "                start_time = time.time()\n",
        "                \n",
        "                if hasattr(value_iter, 'train'):\n",
        "                    result = value_iter.train(max_iterations=MAX_ITERATIONS)\n",
        "                elif hasattr(value_iter, 'solve'):\n",
        "                    result = value_iter.solve()\n",
        "                else:\n",
        "                    # Impl√©mentation manuelle si n√©cessaire\n",
        "                    result = {'V': np.zeros(adapter.nS), 'policy': np.zeros(adapter.nS, dtype=int)}\n",
        "                \n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'ValueIteration'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                result['converged'] = True\n",
        "                \n",
        "                # Historique simul√©\n",
        "                result['history'] = [{\n",
        "                    'episode': 1,\n",
        "                    'reward': 0.0,\n",
        "                    'iteration': 1,\n",
        "                    'converged': True\n",
        "                }]\n",
        "                \n",
        "                env_dp_results['ValueIteration'] = result\n",
        "                \n",
        "                print(f\"   ‚úÖ ValueIteration termin√© en {training_time:.2f}s\")\n",
        "                pbar.update(1)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur ValueIteration: {e}\")\n",
        "            env_dp_results['ValueIteration'] = {'error': str(e), 'algorithm': 'ValueIteration'}\n",
        "        \n",
        "        # R√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â DP {env_name}:\")\n",
        "        for alg_name, result in env_dp_results.items():\n",
        "            if 'error' not in result:\n",
        "                training_time = result.get('training_time', 0)\n",
        "                converged = result.get('converged', False)\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚úÖ Converged={converged}, Time={training_time:.2f}s\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå Erreur\")\n",
        "        \n",
        "        dp_results[env_name] = env_dp_results\n",
        "        \n",
        "        print(f\"\\\\nüìà Progression globale DP: {current_combination}/{total_combinations}\")\n",
        "    \n",
        "    print(f\"\\\\nüéâ DYNAMIC PROGRAMMING TERMIN√â !\")\n",
        "    print(f\"üìä {len([r for env_r in dp_results.values() for r in env_r.values() if 'error' not in r])} combinaisons r√©ussies\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return dp_results\n",
        "\n",
        "# Import time pour mesurer les performances\n",
        "import time\n",
        "\n",
        "# üöÄ Lancer l'entra√Ænement DP\n",
        "if algorithms_status.get('DP', False) and successful_adapters > 0:\n",
        "    print(\"‚è≥ Lancement de l'entra√Ænement Dynamic Programming...\")\n",
        "    print(\"üí° Les algorithmes DP convergent g√©n√©ralement tr√®s rapidement!\")\n",
        "    \n",
        "    dp_results = train_dp_algorithms()\n",
        "    \n",
        "    # üìä Analyse imm√©diate des r√©sultats DP\n",
        "    if dp_results:\n",
        "        print(\"\\\\nüìà G√âN√âRATION DES ANALYSES DP...\")\n",
        "        \n",
        "        try:\n",
        "            # Tableau comparatif sp√©cialis√© pour DP\n",
        "            comparison_df = generate_comparison_table(dp_results, \"DYNAMIC PROGRAMMING\")\n",
        "            \n",
        "            # Pas de courbes d'apprentissage pour DP (convergence en quelques it√©rations)\n",
        "            # Mais on peut faire des heatmaps de politique\n",
        "            for env_name, env_results in dp_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'policy' in result:\n",
        "                        plot_policy_heatmap(result, env_name, alg_name)\n",
        "            \n",
        "            print(\"‚úÖ Analyses DP g√©n√©r√©es avec succ√®s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur g√©n√©ration analyses DP: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Skip Dynamic Programming:\")\n",
        "    if not algorithms_status.get('DP', False):\n",
        "        print(\"   - Algorithmes DP non import√©s\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur fonctionnel\")\n",
        "    \n",
        "    dp_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2Ô∏è‚É£ TEMPORAL DIFFERENCE - Algorithmes de Co√ªt Moyen üîÑ\n",
        "\n",
        "def train_td_algorithms():\n",
        "    \"\"\"Entra√Ænement des algorithmes Temporal Difference sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"üöÄ ENTRA√éNEMENT DES ALGORITHMES TEMPORAL DIFFERENCE\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if not algorithms_status.get('TD', False):\n",
        "        print(\"‚ùå Algorithmes TD non disponibles - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    if successful_adapters == 0:\n",
        "        print(\"‚ùå Aucun adaptateur disponible - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    td_results = {}\n",
        "    total_combinations = len([a for a in adapters.values() if a is not None]) * 3  # 3 algos TD\n",
        "    current_combination = 0\n",
        "    \n",
        "    # Configuration TD\n",
        "    TD_EPISODES = 500\n",
        "    TD_ALPHA = 0.1    # Taux d'apprentissage\n",
        "    TD_GAMMA = 0.99   # Facteur de discount\n",
        "    TD_EPSILON = 0.2  # Exploration initiale\n",
        "    \n",
        "    print(f\"‚öôÔ∏è  Param√®tres TD: {TD_EPISODES} √©pisodes, Œ±={TD_ALPHA}, Œ≥={TD_GAMMA}, Œµ={TD_EPSILON}\")\n",
        "    print(f\"üîÑ {total_combinations} combinaisons √† traiter\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Entra√Ænement sur chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"‚è≠Ô∏è  Skip {env_name} - Adaptateur non disponible\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   √âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        env_td_results = {}\n",
        "        \n",
        "        # 1Ô∏è‚É£ SARSA (On-policy TD)\n",
        "        print(f\"\\\\nüéØ [1/3] SARSA (On-policy TD)...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=TD_EPISODES, desc=f\"SARSA {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                sarsa = Sarsa(adapter, alpha=TD_ALPHA, gamma=TD_GAMMA, epsilon=TD_EPSILON)\n",
        "                print(f\"   üèóÔ∏è  SARSA initialis√© (Œ±={TD_ALPHA}, Œµ={TD_EPSILON})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = sarsa.train(num_episodes=TD_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(TD_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'SARSA'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(sarsa, 'evaluate'):\n",
        "                    eval_results = sarsa.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_td_results['SARSA'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                print(f\"   ‚úÖ SARSA termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur SARSA: {e}\")\n",
        "            env_td_results['SARSA'] = {'error': str(e), 'algorithm': 'SARSA'}\n",
        "        \n",
        "        # 2Ô∏è‚É£ Q-Learning (Off-policy TD)  \n",
        "        print(f\"\\\\nüé≤ [2/3] Q-Learning (Off-policy TD)...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=TD_EPISODES, desc=f\"Q-Learning {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                qlearning = QLearning(adapter, alpha=TD_ALPHA, gamma=TD_GAMMA, epsilon=TD_EPSILON)\n",
        "                print(f\"   üèóÔ∏è  Q-Learning initialis√© (Œ±={TD_ALPHA}, Œµ={TD_EPSILON})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = qlearning.train(num_episodes=TD_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(TD_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'QLearning'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(qlearning, 'evaluate'):\n",
        "                    eval_results = qlearning.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_td_results['QLearning'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                print(f\"   ‚úÖ Q-Learning termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur Q-Learning: {e}\")\n",
        "            env_td_results['QLearning'] = {'error': str(e), 'algorithm': 'QLearning'}\n",
        "        \n",
        "        # 3Ô∏è‚É£ Expected SARSA (Expectation-based TD)\n",
        "        print(f\"\\\\n‚öñÔ∏è  [3/3] Expected SARSA...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=TD_EPISODES, desc=f\"Expected SARSA {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                expected_sarsa = ExpectedSarsa(adapter, alpha=TD_ALPHA, gamma=TD_GAMMA, epsilon=TD_EPSILON)\n",
        "                print(f\"   üèóÔ∏è  Expected SARSA initialis√© (Œ±={TD_ALPHA}, Œµ={TD_EPSILON})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = expected_sarsa.train(num_episodes=TD_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(TD_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'ExpectedSarsa'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(expected_sarsa, 'evaluate'):\n",
        "                    eval_results = expected_sarsa.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_td_results['ExpectedSarsa'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                print(f\"   ‚úÖ Expected SARSA termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur Expected SARSA: {e}\")\n",
        "            env_td_results['ExpectedSarsa'] = {'error': str(e), 'algorithm': 'ExpectedSarsa'}\n",
        "        \n",
        "        # R√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â TD {env_name}:\")\n",
        "        for alg_name, result in env_td_results.items():\n",
        "            if 'error' not in result:\n",
        "                training_time = result.get('training_time', 0)\n",
        "                \n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                    episodes_completed = len(result['history'])\n",
        "                    print(f\"   ‚Ä¢ {alg_name}: ‚úÖ {episodes_completed} √©pisodes, Time={training_time:.2f}s, Final={final_reward:.3f}\")\n",
        "                else:\n",
        "                    print(f\"   ‚Ä¢ {alg_name}: ‚úÖ Time={training_time:.2f}s\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå Erreur\")\n",
        "        \n",
        "        td_results[env_name] = env_td_results\n",
        "        \n",
        "        print(f\"\\\\nüìà Progression globale TD: {current_combination}/{total_combinations}\")\n",
        "    \n",
        "    print(f\"\\\\nüéâ TEMPORAL DIFFERENCE TERMIN√â !\")\n",
        "    success_count = len([r for env_r in td_results.values() for r in env_r.values() if 'error' not in r])\n",
        "    print(f\"üìä {success_count} combinaisons r√©ussies\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return td_results\n",
        "\n",
        "# üöÄ Lancer l'entra√Ænement TD\n",
        "if algorithms_status.get('TD', False) and successful_adapters > 0:\n",
        "    print(\"‚è≥ Lancement de l'entra√Ænement Temporal Difference...\")\n",
        "    print(\"üí° Suivi en temps r√©el avec barres de progression par √©pisode !\")\n",
        "    \n",
        "    td_results = train_td_algorithms()\n",
        "    \n",
        "    # üìä Analyse imm√©diate des r√©sultats TD\n",
        "    if td_results:\n",
        "        print(\"\\\\nüìà G√âN√âRATION DES ANALYSES TD...\")\n",
        "        \n",
        "        try:\n",
        "            # Courbes d'apprentissage avec retour cumulatif moyen\n",
        "            for env_name, env_results in td_results.items():\n",
        "                if any('history' in result and result['history'] for result in env_results.values()):\n",
        "                    plot_learning_curves_universal(env_results, env_name, \"TEMPORAL DIFFERENCE\")\n",
        "            \n",
        "            # Boxplots des returns finaux\n",
        "            plot_boxplots_final_returns(td_results, \"TEMPORAL DIFFERENCE\")\n",
        "            \n",
        "            # Tableau comparatif\n",
        "            comparison_df = generate_comparison_table(td_results, \"TEMPORAL DIFFERENCE\")\n",
        "            \n",
        "            # Heatmaps des politiques apprises\n",
        "            for env_name, env_results in td_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'policy' in result or 'Q' in result:\n",
        "                        # Si on a Q, extraire la politique gloutonne\n",
        "                        if 'Q' in result and 'policy' not in result:\n",
        "                            Q = result['Q']\n",
        "                            policy = np.argmax(Q, axis=1)\n",
        "                            result['policy'] = policy\n",
        "                        \n",
        "                        if 'policy' in result:\n",
        "                            plot_policy_heatmap(result, env_name, alg_name)\n",
        "            \n",
        "            print(\"‚úÖ Analyses TD g√©n√©r√©es avec succ√®s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur g√©n√©ration analyses TD: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Skip Temporal Difference:\")\n",
        "    if not algorithms_status.get('TD', False):\n",
        "        print(\"   - Algorithmes TD non import√©s\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur fonctionnel\")\n",
        "    \n",
        "    td_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3Ô∏è‚É£ MONTE CARLO - Algorithmes de Co√ªt Moyen-√âlev√© üé≤\n",
        "\n",
        "def train_monte_carlo_algorithms():\n",
        "    \"\"\"Entra√Ænement des algorithmes Monte Carlo sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"üöÄ ENTRA√éNEMENT DES ALGORITHMES MONTE CARLO\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if not algorithms_status.get('MC', False):\n",
        "        print(\"‚ùå Algorithmes Monte Carlo non disponibles - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    if successful_adapters == 0:\n",
        "        print(\"‚ùå Aucun adaptateur disponible - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    mc_results = {}\n",
        "    total_combinations = len([a for a in adapters.values() if a is not None]) * 3  # 3 algos MC\n",
        "    current_combination = 0\n",
        "    \n",
        "    # Configuration Monte Carlo\n",
        "    MC_EPISODES = 600      # Plus d'√©pisodes car MC a besoin de plus d'exploration\n",
        "    MC_GAMMA = 0.99       # Facteur de discount\n",
        "    MC_EPSILON = 0.3      # Exploration pour On-Policy et Off-Policy\n",
        "    \n",
        "    print(f\"‚öôÔ∏è  Param√®tres MC: {MC_EPISODES} √©pisodes, Œ≥={MC_GAMMA}, Œµ={MC_EPSILON}\")\n",
        "    print(f\"üîÑ {total_combinations} combinaisons √† traiter\")\n",
        "    print(\"üí° Monte Carlo n√©cessite des √©pisodes complets - Patience requise !\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Entra√Ænement sur chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"‚è≠Ô∏è  Skip {env_name} - Adaptateur non disponible\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   √âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        env_mc_results = {}\n",
        "        \n",
        "        # 1Ô∏è‚É£ Monte Carlo Exploring Starts (MC-ES)\n",
        "        print(f\"\\\\nüî• [1/3] Monte Carlo Exploring Starts...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=MC_EPISODES, desc=f\"MC-ES {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                mc_es = MonteCarloES(adapter, gamma=MC_GAMMA)\n",
        "                print(f\"   üèóÔ∏è  MonteCarloES initialis√© (Œ≥={MC_GAMMA})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = mc_es.train(num_episodes=MC_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(MC_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'MonteCarloES'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(mc_es, 'evaluate'):\n",
        "                    eval_results = mc_es.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_mc_results['MonteCarloES'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                print(f\"   ‚úÖ MC-ES termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur MC-ES: {e}\")\n",
        "            env_mc_results['MonteCarloES'] = {'error': str(e), 'algorithm': 'MonteCarloES'}\n",
        "        \n",
        "        # 2Ô∏è‚É£ On-Policy Monte Carlo \n",
        "        print(f\"\\\\nüéØ [2/3] On-Policy Monte Carlo...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=MC_EPISODES, desc=f\"OnPolicy-MC {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                on_policy_mc = OnPolicyMC(adapter, gamma=MC_GAMMA, epsilon=MC_EPSILON)\n",
        "                print(f\"   üèóÔ∏è  OnPolicyMC initialis√© (Œ≥={MC_GAMMA}, Œµ={MC_EPSILON})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = on_policy_mc.train(num_episodes=MC_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(MC_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'OnPolicyMC'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(on_policy_mc, 'evaluate'):\n",
        "                    eval_results = on_policy_mc.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_mc_results['OnPolicyMC'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                print(f\"   ‚úÖ OnPolicy-MC termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur OnPolicy-MC: {e}\")\n",
        "            env_mc_results['OnPolicyMC'] = {'error': str(e), 'algorithm': 'OnPolicyMC'}\n",
        "        \n",
        "        # 3Ô∏è‚É£ Off-Policy Monte Carlo avec Importance Sampling\n",
        "        print(f\"\\\\n‚öñÔ∏è  [3/3] Off-Policy Monte Carlo...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=MC_EPISODES, desc=f\"OffPolicy-MC {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                off_policy_mc = OffPolicyMC(adapter, gamma=MC_GAMMA, epsilon=MC_EPSILON)\n",
        "                print(f\"   üèóÔ∏è  OffPolicyMC initialis√© (Œ≥={MC_GAMMA}, Œµ={MC_EPSILON})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = off_policy_mc.train(num_episodes=MC_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(MC_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'OffPolicyMC'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(off_policy_mc, 'evaluate'):\n",
        "                    eval_results = off_policy_mc.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_mc_results['OffPolicyMC'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                print(f\"   ‚úÖ OffPolicy-MC termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur OffPolicy-MC: {e}\")\n",
        "            env_mc_results['OffPolicyMC'] = {'error': str(e), 'algorithm': 'OffPolicyMC'}\n",
        "        \n",
        "        # R√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â MONTE CARLO {env_name}:\")\n",
        "        for alg_name, result in env_mc_results.items():\n",
        "            if 'error' not in result:\n",
        "                training_time = result.get('training_time', 0)\n",
        "                \n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                    episodes_completed = len(result['history'])\n",
        "                    \n",
        "                    # Calculer la moyenne r√©cente pour voir la tendance\n",
        "                    recent_rewards = [h.get('reward', 0) for h in result['history'][-20:]]\n",
        "                    avg_recent = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    \n",
        "                    print(f\"   ‚Ä¢ {alg_name}: ‚úÖ {episodes_completed} √©pisodes, Time={training_time:.2f}s\")\n",
        "                    print(f\"     Final={final_reward:.3f}, Moy.r√©cente={avg_recent:.3f}\")\n",
        "                else:\n",
        "                    print(f\"   ‚Ä¢ {alg_name}: ‚úÖ Time={training_time:.2f}s\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå Erreur\")\n",
        "        \n",
        "        mc_results[env_name] = env_mc_results\n",
        "        \n",
        "        print(f\"\\\\nüìà Progression globale MC: {current_combination}/{total_combinations}\")\n",
        "    \n",
        "    print(f\"\\\\nüéâ MONTE CARLO TERMIN√â !\")\n",
        "    success_count = len([r for env_r in mc_results.values() for r in env_r.values() if 'error' not in r])\n",
        "    print(f\"üìä {success_count} combinaisons r√©ussies\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return mc_results\n",
        "\n",
        "# üöÄ Lancer l'entra√Ænement Monte Carlo\n",
        "if algorithms_status.get('MC', False) and successful_adapters > 0:\n",
        "    print(\"‚è≥ Lancement de l'entra√Ænement Monte Carlo...\")\n",
        "    print(\"üí° Monte Carlo n√©cessite des √©pisodes complets - Plus long que TD !\")\n",
        "    \n",
        "    mc_results = train_monte_carlo_algorithms()\n",
        "    \n",
        "    # üìä Analyse imm√©diate des r√©sultats Monte Carlo\n",
        "    if mc_results:\n",
        "        print(\"\\\\nüìà G√âN√âRATION DES ANALYSES MONTE CARLO...\")\n",
        "        \n",
        "        try:\n",
        "            # Courbes d'apprentissage avec retour cumulatif moyen (m√©trique principale)\n",
        "            for env_name, env_results in mc_results.items():\n",
        "                if any('history' in result and result['history'] for result in env_results.values()):\n",
        "                    plot_learning_curves_universal(env_results, env_name, \"MONTE CARLO\")\n",
        "            \n",
        "            # Boxplots des returns finaux\n",
        "            plot_boxplots_final_returns(mc_results, \"MONTE CARLO\")\n",
        "            \n",
        "            # Tableau comparatif avec m√©triques d√©taill√©es\n",
        "            comparison_df = generate_comparison_table(mc_results, \"MONTE CARLO\")\n",
        "            \n",
        "            # Heatmaps des politiques et fonctions de valeur apprises\n",
        "            for env_name, env_results in mc_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'error' not in result:\n",
        "                        # Policy heatmap\n",
        "                        if 'policy' in result:\n",
        "                            plot_policy_heatmap(result, env_name, alg_name)\n",
        "                        \n",
        "                        # Si on a Q, extraire la politique gloutonne\n",
        "                        elif 'Q' in result:\n",
        "                            Q = result['Q']\n",
        "                            policy = np.argmax(Q, axis=1)\n",
        "                            result['policy'] = policy\n",
        "                            plot_policy_heatmap(result, env_name, alg_name)\n",
        "            \n",
        "            print(\"‚úÖ Analyses Monte Carlo g√©n√©r√©es avec succ√®s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur g√©n√©ration analyses MC: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Skip Monte Carlo:\")\n",
        "    if not algorithms_status.get('MC', False):\n",
        "        print(\"   - Algorithmes Monte Carlo non import√©s\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur fonctionnel\")\n",
        "    \n",
        "    mc_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4Ô∏è‚É£ DYNA PLANNING - Algorithmes les Plus Co√ªteux üß†\n",
        "\n",
        "def train_dyna_algorithms():\n",
        "    \"\"\"Entra√Ænement des algorithmes Dyna Planning sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"üöÄ ENTRA√éNEMENT DES ALGORITHMES DYNA PLANNING\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if not algorithms_status.get('DYNA', False):\n",
        "        print(\"‚ùå Algorithmes Dyna non disponibles - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    if successful_adapters == 0:\n",
        "        print(\"‚ùå Aucun adaptateur disponible - Skip\")\n",
        "        return {}\n",
        "    \n",
        "    dyna_results = {}\n",
        "    total_combinations = len([a for a in adapters.values() if a is not None]) * 2  # 2 algos Dyna\n",
        "    current_combination = 0\n",
        "    \n",
        "    # Configuration Dyna (plus intensive)\n",
        "    DYNA_EPISODES = 400        # Moins d'√©pisodes car chaque √©pisode fait plus de planning\n",
        "    DYNA_ALPHA = 0.1          # Taux d'apprentissage\n",
        "    DYNA_GAMMA = 0.99         # Facteur de discount\n",
        "    DYNA_EPSILON = 0.15       # Exploration plus conservative\n",
        "    DYNA_PLANNING_STEPS = 10   # Nombre d'√©tapes de planning par step r√©el\n",
        "    \n",
        "    print(f\"‚öôÔ∏è  Param√®tres Dyna: {DYNA_EPISODES} √©pisodes, Œ±={DYNA_ALPHA}, Œ≥={DYNA_GAMMA}\")\n",
        "    print(f\"   Œµ={DYNA_EPSILON}, planning_steps={DYNA_PLANNING_STEPS}\")\n",
        "    print(f\"üîÑ {total_combinations} combinaisons √† traiter\")\n",
        "    print(\"‚ö†Ô∏è  DYNA = Plus co√ªteux - Apprentissage direct + Planning avec mod√®le !\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Entra√Ænement sur chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"‚è≠Ô∏è  Skip {env_name} - Adaptateur non disponible\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   √âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        env_dyna_results = {}\n",
        "        \n",
        "        # 1Ô∏è‚É£ Dyna-Q (Q-Learning + Planning)\n",
        "        print(f\"\\\\nü§ñ [1/2] Dyna-Q (Q-Learning + Model-based Planning)...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=DYNA_EPISODES, desc=f\"Dyna-Q {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                dyna_q = DynaQ(\n",
        "                    adapter, \n",
        "                    alpha=DYNA_ALPHA, \n",
        "                    gamma=DYNA_GAMMA, \n",
        "                    epsilon=DYNA_EPSILON,\n",
        "                    planning_steps=DYNA_PLANNING_STEPS\n",
        "                )\n",
        "                print(f\"   üèóÔ∏è  Dyna-Q initialis√© (Œ±={DYNA_ALPHA}, Œµ={DYNA_EPSILON}, plan={DYNA_PLANNING_STEPS})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = dyna_q.train(num_episodes=DYNA_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(DYNA_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'DynaQ'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                result['planning_steps'] = DYNA_PLANNING_STEPS\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(dyna_q, 'evaluate'):\n",
        "                    eval_results = dyna_q.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_dyna_results['DynaQ'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                # Statistiques sur le mod√®le appris\n",
        "                model_size = len(getattr(dyna_q, 'model', {}))\n",
        "                \n",
        "                print(f\"   ‚úÖ Dyna-Q termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                print(f\"      Mod√®le appris: {model_size} transitions stock√©es\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur Dyna-Q: {e}\")\n",
        "            env_dyna_results['DynaQ'] = {'error': str(e), 'algorithm': 'DynaQ'}\n",
        "        \n",
        "        # 2Ô∏è‚É£ Dyna-Q+ (avec bonus d'exploration)\n",
        "        print(f\"\\\\nüöÄ [2/2] Dyna-Q+ (avec bonus exploration temporelle)...\")\n",
        "        current_combination += 1\n",
        "        \n",
        "        try:\n",
        "            with tqdm(total=DYNA_EPISODES, desc=f\"Dyna-Q+ {env_name}\", unit=\"ep\") as pbar:\n",
        "                \n",
        "                # Cr√©er l'algorithme\n",
        "                dyna_q_plus = DynaQPlus(\n",
        "                    adapter, \n",
        "                    alpha=DYNA_ALPHA, \n",
        "                    gamma=DYNA_GAMMA, \n",
        "                    epsilon=DYNA_EPSILON,\n",
        "                    planning_steps=DYNA_PLANNING_STEPS,\n",
        "                    kappa=0.001  # Param√®tre bonus exploration (si disponible)\n",
        "                )\n",
        "                print(f\"   üèóÔ∏è  Dyna-Q+ initialis√© (Œ±={DYNA_ALPHA}, Œµ={DYNA_EPSILON}, plan={DYNA_PLANNING_STEPS})\")\n",
        "                \n",
        "                # Entra√Ænement avec barre de progression\n",
        "                start_time = time.time()\n",
        "                result = dyna_q_plus.train(num_episodes=DYNA_EPISODES)\n",
        "                training_time = time.time() - start_time\n",
        "                \n",
        "                # Mettre √† jour la barre de progression\n",
        "                if 'history' in result and result['history']:\n",
        "                    pbar.update(len(result['history']))\n",
        "                else:\n",
        "                    pbar.update(DYNA_EPISODES)\n",
        "                \n",
        "                # Compl√©ter les r√©sultats\n",
        "                result['algorithm'] = 'DynaQPlus'\n",
        "                result['env_name'] = env_name\n",
        "                result['training_time'] = training_time\n",
        "                result['planning_steps'] = DYNA_PLANNING_STEPS\n",
        "                \n",
        "                # √âvaluation finale\n",
        "                if hasattr(dyna_q_plus, 'evaluate'):\n",
        "                    eval_results = dyna_q_plus.evaluate(num_episodes=50)\n",
        "                    result['evaluation'] = eval_results\n",
        "                \n",
        "                env_dyna_results['DynaQPlus'] = result\n",
        "                \n",
        "                final_reward = 0\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                \n",
        "                # Statistiques sur le mod√®le appris\n",
        "                model_size = len(getattr(dyna_q_plus, 'model', {}))\n",
        "                \n",
        "                print(f\"   ‚úÖ Dyna-Q+ termin√© en {training_time:.2f}s - Reward final: {final_reward:.3f}\")\n",
        "                print(f\"      Mod√®le appris: {model_size} transitions + bonus exploration\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur Dyna-Q+: {e}\")\n",
        "            env_dyna_results['DynaQPlus'] = {'error': str(e), 'algorithm': 'DynaQPlus'}\n",
        "        \n",
        "        # R√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â DYNA PLANNING {env_name}:\")\n",
        "        for alg_name, result in env_dyna_results.items():\n",
        "            if 'error' not in result:\n",
        "                training_time = result.get('training_time', 0)\n",
        "                planning_steps = result.get('planning_steps', 0)\n",
        "                \n",
        "                if 'history' in result and result['history']:\n",
        "                    final_reward = result['history'][-1].get('reward', 0)\n",
        "                    episodes_completed = len(result['history'])\n",
        "                    \n",
        "                    # Calculer des m√©triques de performance\n",
        "                    recent_rewards = [h.get('reward', 0) for h in result['history'][-20:]]\n",
        "                    avg_recent = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    \n",
        "                    print(f\"   ‚Ä¢ {alg_name}: ‚úÖ {episodes_completed} √©pisodes, Time={training_time:.2f}s\")\n",
        "                    print(f\"     Final={final_reward:.3f}, Moy.r√©cente={avg_recent:.3f}, Planning={planning_steps}\")\n",
        "                else:\n",
        "                    print(f\"   ‚Ä¢ {alg_name}: ‚úÖ Time={training_time:.2f}s, Planning={planning_steps}\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå Erreur\")\n",
        "        \n",
        "        dyna_results[env_name] = env_dyna_results\n",
        "        \n",
        "        print(f\"\\\\nüìà Progression globale Dyna: {current_combination}/{total_combinations}\")\n",
        "    \n",
        "    print(f\"\\\\nüéâ DYNA PLANNING TERMIN√â !\")\n",
        "    success_count = len([r for env_r in dyna_results.values() for r in env_r.values() if 'error' not in r])\n",
        "    print(f\"üìä {success_count} combinaisons r√©ussies\")\n",
        "    print(\"üß† Les algorithmes Dyna combinent apprentissage direct et planning - Les plus sophistiqu√©s !\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return dyna_results\n",
        "\n",
        "# üöÄ Lancer l'entra√Ænement Dyna Planning\n",
        "if algorithms_status.get('DYNA', False) and successful_adapters > 0:\n",
        "    print(\"‚è≥ Lancement de l'entra√Ænement Dyna Planning...\")\n",
        "    print(\"üí° Dyna = Algorithmes les plus co√ªteux mais potentiellement les plus performants !\")\n",
        "    print(\"üß† Combinaison apprentissage direct + planification avec mod√®le appris\")\n",
        "    \n",
        "    dyna_results = train_dyna_algorithms()\n",
        "    \n",
        "    # üìä Analyse imm√©diate des r√©sultats Dyna\n",
        "    if dyna_results:\n",
        "        print(\"\\\\nüìà G√âN√âRATION DES ANALYSES DYNA PLANNING...\")\n",
        "        \n",
        "        try:\n",
        "            # Courbes d'apprentissage avec retour cumulatif moyen\n",
        "            for env_name, env_results in dyna_results.items():\n",
        "                if any('history' in result and result['history'] for result in env_results.values()):\n",
        "                    plot_learning_curves_universal(env_results, env_name, \"DYNA PLANNING\")\n",
        "            \n",
        "            # Boxplots des returns finaux\n",
        "            plot_boxplots_final_returns(dyna_results, \"DYNA PLANNING\")\n",
        "            \n",
        "            # Tableau comparatif avec m√©triques sp√©ciales pour Dyna\n",
        "            comparison_df = generate_comparison_table(dyna_results, \"DYNA PLANNING\")\n",
        "            \n",
        "            # Heatmaps des politiques apprises\n",
        "            for env_name, env_results in dyna_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'error' not in result:\n",
        "                        # Policy heatmap\n",
        "                        if 'policy' in result:\n",
        "                            plot_policy_heatmap(result, env_name, alg_name)\n",
        "                        \n",
        "                        # Si on a Q, extraire la politique gloutonne\n",
        "                        elif 'Q' in result:\n",
        "                            Q = result['Q']\n",
        "                            policy = np.argmax(Q, axis=1)\n",
        "                            result['policy'] = policy\n",
        "                            plot_policy_heatmap(result, env_name, alg_name)\n",
        "            \n",
        "            print(\"‚úÖ Analyses Dyna Planning g√©n√©r√©es avec succ√®s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur g√©n√©ration analyses Dyna: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Skip Dyna Planning:\")\n",
        "    if not algorithms_status.get('DYNA', False):\n",
        "        print(\"   - Algorithmes Dyna non import√©s\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur fonctionnel\")\n",
        "    \n",
        "    dyna_results = {}\n",
        "\n",
        "print(\"\\\\n\" + \"üéä\" * 20)\n",
        "print(\"üèÅ TOUS LES ALGORITHMES TERMIN√âS !\")\n",
        "print(\"üéä\" * 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèÜ ANALYSE GLOBALE DE TOUS LES ALGORITHMES REINFORCEMENT LEARNING\n",
        "\n",
        "def analyze_all_algorithms_globally():\n",
        "    \"\"\"Analyse comparative compl√®te de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"üåç ANALYSE GLOBALE DE TOUS LES ALGORITHMES RL\")\n",
        "    print(\"=\" * 100)\n",
        "    \n",
        "    # Collecter tous les r√©sultats\n",
        "    all_algorithm_results = {}\n",
        "    \n",
        "    # Variables globales avec d√©faut vide\n",
        "    globals_to_check = ['dp_results', 'td_results', 'mc_results', 'dyna_results']\n",
        "    algorithm_families = []\n",
        "    \n",
        "    for var_name in globals_to_check:\n",
        "        if var_name in globals() and globals()[var_name]:\n",
        "            results = globals()[var_name]\n",
        "            family_name = var_name.replace('_results', '').upper()\n",
        "            all_algorithm_results[family_name] = results\n",
        "            algorithm_families.append(family_name)\n",
        "            print(f\"‚úÖ {family_name}: {len(results)} environnements trait√©s\")\n",
        "        else:\n",
        "            print(f\"‚è≠Ô∏è  {var_name}: Aucun r√©sultat\")\n",
        "    \n",
        "    if not all_algorithm_results:\n",
        "        print(\"‚ùå Aucun r√©sultat d'algorithme disponible pour l'analyse globale\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\\\nüéØ FAMILLES D'ALGORITHMES ANALYS√âES: {', '.join(algorithm_families)}\")\n",
        "    print(\"=\" * 100)\n",
        "    \n",
        "    # 1. üìä Tableau comparatif global unifi√©\n",
        "    print(\"\\\\n1Ô∏è‚É£ TABLEAU COMPARATIF GLOBAL - TOUS LES ALGORITHMES\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    global_comparison_data = []\n",
        "    \n",
        "    for family_name, family_results in all_algorithm_results.items():\n",
        "        for env_name, env_results in family_results.items():\n",
        "            for alg_name, result in env_results.items():\n",
        "                if 'error' not in result and 'history' in result and result['history']:\n",
        "                    history = result['history']\n",
        "                    \n",
        "                    # M√©triques universelles\n",
        "                    final_returns = [h.get('reward', h.get('return', 0)) for h in history[-20:]]\n",
        "                    all_returns = [h.get('reward', h.get('return', 0)) for h in history]\n",
        "                    \n",
        "                    lengths = []\n",
        "                    for h in history:\n",
        "                        length = h.get('length', h.get('episode_length', h.get('steps', 1)))\n",
        "                        lengths.append(length)\n",
        "                    \n",
        "                    # M√©triques calcul√©es\n",
        "                    final_avg = np.mean(final_returns) if final_returns else 0\n",
        "                    final_std = np.std(final_returns) if len(final_returns) > 1 else 0\n",
        "                    steps_avg = np.mean(lengths) if lengths else 0\n",
        "                    win_rate = sum(1 for r in all_returns if r > 0) / len(all_returns) if all_returns else 0\n",
        "                    training_time = result.get('training_time', 0)\n",
        "                    \n",
        "                    # √âvaluation si disponible\n",
        "                    eval_return = 0\n",
        "                    eval_win_rate = 0\n",
        "                    if 'evaluation' in result:\n",
        "                        eval_data = result['evaluation']\n",
        "                        eval_return = eval_data.get('avg_reward', eval_data.get('average_reward', 0))\n",
        "                        eval_win_rate = eval_data.get('success_rate', 0)\n",
        "                    \n",
        "                    global_comparison_data.append({\n",
        "                        'Famille': family_name,\n",
        "                        'Algorithme': alg_name,\n",
        "                        'Environnement': env_name,\n",
        "                        'Return_Final': f\"{final_avg:.3f}\",\n",
        "                        'Std_Final': f\"{final_std:.3f}\",\n",
        "                        'Steps_Moy': f\"{steps_avg:.1f}\" if steps_avg > 0 else \"N/A\",\n",
        "                        'Taux_Victoire': f\"{win_rate:.1%}\",\n",
        "                        'Temps_Train_s': f\"{training_time:.2f}\",\n",
        "                        'Eval_Return': f\"{eval_return:.3f}\",\n",
        "                        'Eval_Victoire': f\"{eval_win_rate:.1%}\"\n",
        "                    })\n",
        "                \n",
        "                elif 'error' in result:\n",
        "                    global_comparison_data.append({\n",
        "                        'Famille': family_name,\n",
        "                        'Algorithme': alg_name,\n",
        "                        'Environnement': env_name,\n",
        "                        'Return_Final': \"ERREUR\",\n",
        "                        'Std_Final': \"N/A\",\n",
        "                        'Steps_Moy': \"N/A\",\n",
        "                        'Taux_Victoire': \"N/A\",\n",
        "                        'Temps_Train_s': \"N/A\",\n",
        "                        'Eval_Return': \"N/A\",\n",
        "                        'Eval_Victoire': \"N/A\"\n",
        "                    })\n",
        "    \n",
        "    if global_comparison_data:\n",
        "        global_df = pd.DataFrame(global_comparison_data)\n",
        "        print(global_df.to_string(index=False))\n",
        "        \n",
        "        # Sauvegarder le tableau global\n",
        "        global_df.to_csv('global_rl_algorithms_comparison.csv', index=False)\n",
        "        print(\"\\\\nüíæ Tableau global sauvegard√©: 'global_rl_algorithms_comparison.csv'\")\n",
        "    \n",
        "    # 2. üèÜ Classement des meilleurs algorithmes par environnement\n",
        "    print(\"\\\\n2Ô∏è‚É£ CLASSEMENT DES MEILLEURS ALGORITHMES PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    env_rankings = {}\n",
        "    \n",
        "    # Grouper par environnement\n",
        "    for family_name, family_results in all_algorithm_results.items():\n",
        "        for env_name, env_results in family_results.items():\n",
        "            if env_name not in env_rankings:\n",
        "                env_rankings[env_name] = []\n",
        "            \n",
        "            for alg_name, result in env_results.items():\n",
        "                if 'error' not in result and 'history' in result and result['history']:\n",
        "                    final_returns = [h.get('reward', h.get('return', 0)) for h in result['history'][-20:]]\n",
        "                    avg_final = np.mean(final_returns) if final_returns else 0\n",
        "                    std_final = np.std(final_returns) if len(final_returns) > 1 else 0\n",
        "                    \n",
        "                    # Score composite (performance - instabilit√©)\n",
        "                    composite_score = avg_final - 0.1 * std_final\n",
        "                    \n",
        "                    env_rankings[env_name].append({\n",
        "                        'family': family_name,\n",
        "                        'algorithm': alg_name,\n",
        "                        'score': composite_score,\n",
        "                        'avg_return': avg_final,\n",
        "                        'std_return': std_final\n",
        "                    })\n",
        "    \n",
        "    # Afficher les classements\n",
        "    for env_name, rankings in env_rankings.items():\n",
        "        rankings.sort(key=lambda x: x['score'], reverse=True)\n",
        "        \n",
        "        print(f\"\\\\nüèÖ {env_name} - TOP 3:\")\n",
        "        for i, entry in enumerate(rankings[:3]):\n",
        "            medal = [\"ü•á\", \"ü•à\", \"ü•â\"][i]\n",
        "            print(f\"   {medal} {entry['family']}-{entry['algorithm']}: {entry['score']:.3f} \"\n",
        "                  f\"(Return: {entry['avg_return']:.3f} ¬± {entry['std_return']:.3f})\")\n",
        "    \n",
        "    # 3. üìà Analyse des performances par famille d'algorithmes\n",
        "    print(\"\\\\n3Ô∏è‚É£ PERFORMANCES MOYENNES PAR FAMILLE D'ALGORITHMES\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    family_stats = {}\n",
        "    \n",
        "    for family_name, family_results in all_algorithm_results.items():\n",
        "        all_scores = []\n",
        "        all_times = []\n",
        "        success_count = 0\n",
        "        total_count = 0\n",
        "        \n",
        "        for env_name, env_results in family_results.items():\n",
        "            for alg_name, result in env_results.items():\n",
        "                total_count += 1\n",
        "                \n",
        "                if 'error' not in result and 'history' in result and result['history']:\n",
        "                    success_count += 1\n",
        "                    \n",
        "                    final_returns = [h.get('reward', h.get('return', 0)) for h in result['history'][-20:]]\n",
        "                    avg_final = np.mean(final_returns) if final_returns else 0\n",
        "                    all_scores.append(avg_final)\n",
        "                    \n",
        "                    training_time = result.get('training_time', 0)\n",
        "                    all_times.append(training_time)\n",
        "        \n",
        "        family_stats[family_name] = {\n",
        "            'avg_performance': np.mean(all_scores) if all_scores else 0,\n",
        "            'std_performance': np.std(all_scores) if len(all_scores) > 1 else 0,\n",
        "            'avg_time': np.mean(all_times) if all_times else 0,\n",
        "            'success_rate': success_count / total_count if total_count > 0 else 0,\n",
        "            'count': total_count\n",
        "        }\n",
        "    \n",
        "    print(\"Famille        | Performance Moy | Temps Moy (s) | Taux Succ√®s | Nb Tests\")\n",
        "    print(\"-\" * 75)\n",
        "    for family, stats in family_stats.items():\n",
        "        print(f\"{family:12} | {stats['avg_performance']:10.3f}   | \"\n",
        "              f\"{stats['avg_time']:9.2f}   | {stats['success_rate']:8.1%}   | {stats['count']:8}\")\n",
        "    \n",
        "    # 4. üå°Ô∏è Heatmap comparative globale\n",
        "    print(\"\\\\n4Ô∏è‚É£ G√âN√âRATION DE LA HEATMAP COMPARATIVE GLOBALE\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    try:\n",
        "        # Pr√©parer donn√©es pour heatmap\n",
        "        heatmap_data = []\n",
        "        all_envs = set()\n",
        "        all_algos = set()\n",
        "        \n",
        "        for family_name, family_results in all_algorithm_results.items():\n",
        "            for env_name, env_results in family_results.items():\n",
        "                all_envs.add(env_name)\n",
        "                for alg_name, result in env_results.items():\n",
        "                    full_alg_name = f\"{family_name}-{alg_name}\"\n",
        "                    all_algos.add(full_alg_name)\n",
        "                    \n",
        "                    if 'error' not in result and 'history' in result and result['history']:\n",
        "                        final_returns = [h.get('reward', h.get('return', 0)) for h in result['history'][-20:]]\n",
        "                        performance = np.mean(final_returns) if final_returns else 0\n",
        "                    else:\n",
        "                        performance = -1  # Valeur pour les erreurs\n",
        "                    \n",
        "                    heatmap_data.append({\n",
        "                        'Environment': env_name,\n",
        "                        'Algorithm': full_alg_name,\n",
        "                        'Performance': performance\n",
        "                    })\n",
        "        \n",
        "        # Cr√©er matrice pour heatmap\n",
        "        env_list = sorted(list(all_envs))\n",
        "        algo_list = sorted(list(all_algos))\n",
        "        \n",
        "        perf_matrix = np.zeros((len(env_list), len(algo_list)))\n",
        "        \n",
        "        for data_point in heatmap_data:\n",
        "            env_idx = env_list.index(data_point['Environment'])\n",
        "            algo_idx = algo_list.index(data_point['Algorithm'])\n",
        "            perf_matrix[env_idx, algo_idx] = data_point['Performance']\n",
        "        \n",
        "        # Cr√©er la heatmap\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        heatmap = plt.imshow(perf_matrix, cmap='RdYlGn', aspect='auto', vmin=-1, vmax=1)\n",
        "        \n",
        "        plt.xticks(range(len(algo_list)), algo_list, rotation=45, ha='right')\n",
        "        plt.yticks(range(len(env_list)), env_list)\n",
        "        plt.xlabel('Algorithmes (Famille-Nom)')\n",
        "        plt.ylabel('Environnements Secrets')\n",
        "        plt.title('üå°Ô∏è Heatmap Globale des Performances\\\\n(Return Final Moyen - Tous Algorithmes)', \n",
        "                  fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # Ajouter les valeurs dans les cellules\n",
        "        for i in range(len(env_list)):\n",
        "            for j in range(len(algo_list)):\n",
        "                value = perf_matrix[i, j]\n",
        "                color = 'white' if abs(value) > 0.3 else 'black'\n",
        "                text = f'{value:.2f}' if value != -1 else 'ERR'\n",
        "                plt.text(j, i, text, ha='center', va='center', \n",
        "                        color=color, fontweight='bold', fontsize=8)\n",
        "        \n",
        "        plt.colorbar(heatmap, label='Performance Finale')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"‚úÖ Heatmap globale g√©n√©r√©e\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur g√©n√©ration heatmap globale: {e}\")\n",
        "    \n",
        "    # 5. üí° Recommandations finales\n",
        "    print(\"\\\\n5Ô∏è‚É£ RECOMMANDATIONS FINALES\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    print(\"üéØ R√âSUM√â EX√âCUTIF:\")\n",
        "    print(f\"   ‚Ä¢ {len(algorithm_families)} familles d'algorithmes test√©es\")\n",
        "    \n",
        "    total_combinations = sum(len([r for env_r in family_results.values() \n",
        "                                  for r in env_r.values()]) \n",
        "                            for family_results in all_algorithm_results.values())\n",
        "    successful_combinations = sum(len([r for env_r in family_results.values() \n",
        "                                      for r in env_r.values() \n",
        "                                      if 'error' not in r and 'history' in r]) \n",
        "                                 for family_results in all_algorithm_results.values())\n",
        "    \n",
        "    print(f\"   ‚Ä¢ {successful_combinations}/{total_combinations} combinaisons r√©ussies ({successful_combinations/total_combinations:.1%})\")\n",
        "    \n",
        "    print(\"\\\\nüí° RECOMMANDATIONS ALGORITHMIQUES:\")\n",
        "    print(\"   1. üöÄ Dynamic Programming: Tr√®s rapide, id√©al si dynamiques MDP connues\")\n",
        "    print(\"   2. ‚ö° Temporal Difference: Bon compromis vitesse/performance pour exploration online\")  \n",
        "    print(\"   3. üé≤ Monte Carlo: Exploration compl√®te, robuste mais plus lent\")\n",
        "    print(\"   4. üß† Dyna Planning: Plus sophistiqu√©, combine direct + planning\")\n",
        "    \n",
        "    print(\"\\\\nüìä M√âTRIQUES CL√âS √Ä RETENIR:\")\n",
        "    print(\"   ‚Ä¢ Return Cumulatif Moyen: Principale m√©trique de performance\")\n",
        "    print(\"   ‚Ä¢ Taux de Victoire: Stabilit√© et fiabilit√© de l'algorithme\")\n",
        "    print(\"   ‚Ä¢ Temps d'Entra√Ænement: Co√ªt computationnel\")\n",
        "    print(\"   ‚Ä¢ √âvaluation: Performance r√©elle sur politique apprise\")\n",
        "    \n",
        "    print(\"\\\\nüîö ANALYSE GLOBALE TERMIN√âE\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "# üöÄ Lancer l'analyse globale finale\n",
        "print(\"‚è≥ Lancement de l'analyse globale de tous les algorithmes...\")\n",
        "\n",
        "analyze_all_algorithms_globally()\n",
        "\n",
        "print(\"\\\\n\" + \"üéÜ\" * 30)\n",
        "print(\"üéâ ANALYSE COMPL√àTE DE TOUS LES ALGORITHMES RL TERMIN√âE ! üéâ\")\n",
        "print(\"üéÜ\" * 30)\n",
        "print(\"\\\\nüìã FICHIERS G√âN√âR√âS:\")\n",
        "print(\"   üìÑ global_rl_algorithms_comparison.csv - Tableau comparatif global\")\n",
        "print(\"   üìä Graphiques et heatmaps affich√©s dans le notebook\")\n",
        "print(\"\\\\nüéØ Utilisez ces r√©sultats pour choisir le meilleur algorithme selon vos besoins !\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Lancement de l'analyse (peut prendre 5-15 minutes selon les param√®tres)...\n",
            "\n",
            "üöÄ LANCEMENT DE L'ANALYSE MONTE CARLO\n",
            "============================================================\n",
            "‚öôÔ∏è  Param√®tres: 400 √©pisodes, Œ≥=0.99\n",
            "üéÆ Environnements √† tester: 4\n",
            "üß† Algorithmes: MonteCarloES, OnPolicyMC, OffPolicyMC\n",
            "============================================================\n",
            "\n",
            "üéÆ ENVIRONNEMENT: SecretEnv0\n",
            "   √âtats: 8192, Actions: 3\n",
            "--------------------------------------------------\n",
            "\\nüî• [1/3] Monte Carlo Exploring Starts...\n",
            "   üèóÔ∏è  MonteCarloES initialis√© pour SecretEnv0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# üß† Entra√Ænement avec les Algorithmes Monte Carlo Existants\n",
        "\n",
        "def run_monte_carlo_analysis(num_episodes=300, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Lance l'analyse compl√®te en utilisant les algorithmes Monte Carlo existants du projet.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Nombre d'√©pisodes d'entra√Ænement par algorithme\n",
        "        gamma: Facteur de discount pour tous les algorithmes\n",
        "    \n",
        "    Returns:\n",
        "        dict: R√©sultats complets de l'analyse\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\nüöÄ LANCEMENT DE L'ANALYSE MONTE CARLO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"‚öôÔ∏è  Param√®tres: {num_episodes} √©pisodes, Œ≥={gamma}\")\n",
        "    print(f\"üéÆ Environnements √† tester: {len([k for k,v in adapters.items() if v is not None])}\")\n",
        "    print(f\"üß† Algorithmes: MonteCarloES, OnPolicyMC, OffPolicyMC\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if not monte_carlo_available:\n",
        "        print(\"‚ùå Algorithmes Monte Carlo non disponibles !\")\n",
        "        return {}\n",
        "    \n",
        "    all_results = {}\n",
        "    total_combinations = 0\n",
        "    completed_combinations = 0\n",
        "    \n",
        "    # Compter le total de combinaisons\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is not None:\n",
        "            total_combinations += 3  # 3 algorithmes par environnement\n",
        "    \n",
        "    # Entra√Ænement pour chaque environnement\n",
        "    for env_name, adapter in adapters.items():\n",
        "        if adapter is None:\n",
        "            print(f\"‚è≠Ô∏è  Skipping {env_name} (adapter non disponible)\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"   √âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. üéØ Monte Carlo Exploring Starts\n",
        "        print(f\"\\\\nüî• [1/3] Monte Carlo Exploring Starts...\")\n",
        "        try:\n",
        "            mc_es = MonteCarloES(adapter, gamma=gamma)\n",
        "            print(f\"   üèóÔ∏è  MonteCarloES initialis√© pour {env_name}\")\n",
        "            \n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            result_es['algorithm'] = 'MonteCarloES'\n",
        "            result_es['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter √©valuation finale\n",
        "            eval_results = mc_es.evaluate(num_episodes=50)\n",
        "            result_es['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['MonteCarloES'] = result_es\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   ‚úÖ MonteCarloES termin√© - R√©compense finale: {result_es['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur MonteCarloES: {e}\")\n",
        "            env_results['MonteCarloES'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # 2. üéØ On-Policy Monte Carlo  \n",
        "        print(f\"\\\\nüîÑ [2/3] On-Policy Monte Carlo...\")\n",
        "        try:\n",
        "            on_policy_mc = OnPolicyMC(adapter, gamma=gamma, epsilon=0.3)\n",
        "            print(f\"   üèóÔ∏è  OnPolicyMC initialis√© pour {env_name} (Œµ=0.3)\")\n",
        "            \n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            result_on['algorithm'] = 'OnPolicyMC'\n",
        "            result_on['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter √©valuation finale\n",
        "            eval_results = on_policy_mc.evaluate(num_episodes=50)\n",
        "            result_on['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['OnPolicyMC'] = result_on\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   ‚úÖ OnPolicyMC termin√© - R√©compense finale: {result_on['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur OnPolicyMC: {e}\")\n",
        "            env_results['OnPolicyMC'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # 3. üéØ Off-Policy Monte Carlo\n",
        "        print(f\"\\\\n‚öñÔ∏è  [3/3] Off-Policy Monte Carlo...\")\n",
        "        try:\n",
        "            off_policy_mc = OffPolicyMC(adapter, gamma=gamma, epsilon=0.4)\n",
        "            print(f\"   üèóÔ∏è  OffPolicyMC initialis√© pour {env_name} (Œµ=0.4)\")\n",
        "            \n",
        "            result_off = off_policy_mc.train(num_episodes=num_episodes)\n",
        "            result_off['algorithm'] = 'OffPolicyMC'  \n",
        "            result_off['env_name'] = env_name\n",
        "            \n",
        "            # Ajouter √©valuation finale\n",
        "            eval_results = off_policy_mc.evaluate(num_episodes=50)\n",
        "            result_off['evaluation'] = eval_results\n",
        "            \n",
        "            env_results['OffPolicyMC'] = result_off\n",
        "            completed_combinations += 1\n",
        "            \n",
        "            print(f\"   ‚úÖ OffPolicyMC termin√© - R√©compense finale: {result_off['history'][-1]['reward']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur OffPolicyMC: {e}\")\n",
        "            env_results['OffPolicyMC'] = {'history': [], 'error': str(e)}\n",
        "        \n",
        "        # Stocker les r√©sultats pour cet environnement\n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # R√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                final_reward = result['history'][-1]['reward']\n",
        "                avg_reward = np.mean([h['reward'] for h in result['history'][-10:]])\n",
        "                print(f\"   ‚Ä¢ {alg_name}: R√©compense finale = {final_reward:.3f}, Moyenne r√©cente = {avg_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå √âchec\")\n",
        "    \n",
        "    # R√©sum√© global\n",
        "    print(f\"\\\\nüéâ ANALYSE COMPL√àTE TERMIN√âE !\")\n",
        "    print(f\"üìà {completed_combinations}/{total_combinations} combinaisons r√©ussies\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# üöÄ Lancement de l'analyse compl√®te\n",
        "if successful_adapters > 0 and monte_carlo_available:\n",
        "    print(\"‚è≥ Lancement de l'analyse (peut prendre 5-15 minutes selon les param√®tres)...\")\n",
        "    \n",
        "    # Param√®tres d'entra√Ænement - ajustables selon les besoins\n",
        "    EPISODES = 400  # Nombre d'√©pisodes par algorithme (augmentez pour plus de pr√©cision)\n",
        "    GAMMA = 0.99    # Facteur de discount\n",
        "    \n",
        "    all_results = run_monte_carlo_analysis(num_episodes=EPISODES, gamma=GAMMA)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Impossible de lancer l'analyse :\")\n",
        "    if successful_adapters == 0:\n",
        "        print(\"   - Aucun adaptateur d'environnement fonctionnel\")\n",
        "    if not monte_carlo_available:\n",
        "        print(\"   - Algorithmes Monte Carlo non import√©s\")\n",
        "    \n",
        "    all_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Fonctions de Visualisation et d'Analyse\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les algorithmes d'un environnement\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']  # Rouge, Bleu, Vert, Orange\n",
        "    \n",
        "    # 1. R√©compenses par √©pisode avec moyenne mobile\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            \n",
        "            # Moyenne mobile pour lisser les courbes\n",
        "            window_size = min(30, len(rewards) // 10 + 1)\n",
        "            if len(rewards) >= window_size:\n",
        "                rewards_smooth = pd.Series(rewards).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2.5)\n",
        "                ax1.plot(episodes, rewards, alpha=0.3, color=colors[i % len(colors)], linewidth=0.8)\n",
        "            else:\n",
        "                ax1.plot(episodes, rewards, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - R√©compenses par √âpisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('√âpisode')\n",
        "    ax1.set_ylabel('R√©compense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes (si disponibles)\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # V√©rifier si avg_q est disponible dans l'historique\n",
        "            if 'avg_q' in history[0]:\n",
        "                avg_q = [h['avg_q'] for h in history]\n",
        "                ax2.plot(episodes, avg_q, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - √âvolution des Q-values Moyennes', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('√âpisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des √©pisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # V√©rifier le format des longueurs d'√©pisode\n",
        "            if 'length' in history[0]:\n",
        "                lengths = [h['length'] for h in history]\n",
        "            elif 'episode_length' in history[0]:\n",
        "                lengths = [h['episode_length'] for h in history]  \n",
        "            else:\n",
        "                continue\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            window_size = min(30, len(lengths) // 10 + 1)\n",
        "            if len(lengths) >= window_size:\n",
        "                lengths_smooth = pd.Series(lengths).rolling(window=window_size, min_periods=1).mean()\n",
        "                ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "            else:\n",
        "                ax3.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des √âpisodes', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('√âpisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Convergence - Variation des r√©compenses (stabilit√©)\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = []\n",
        "            stds = []\n",
        "            \n",
        "            window_size = 50\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            for j in range(window_size, len(history)):\n",
        "                recent_rewards = rewards[j-window_size:j]\n",
        "                episodes.append(history[j]['episode'])\n",
        "                stds.append(np.std(recent_rewards))\n",
        "            \n",
        "            if len(episodes) > 0:\n",
        "                ax4.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Stabilit√© (√âcart-type des r√©compenses)', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('√âpisode')\n",
        "    ax4.set_ylabel('√âcart-type des r√©compenses')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results_detailed(all_results):\n",
        "    \"\"\"Analyse d√©taill√©e avec m√©triques de performance\"\"\"\n",
        "    print(\"üîç ANALYSE D√âTAILL√âE DES R√âSULTATS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\nüìä {env_name.upper()}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # M√©triques de base\n",
        "                total_episodes = len(history)\n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Performance finale (derniers 20% d'√©pisodes)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                final_stability = np.std(final_rewards) if len(final_rewards) > 1 else 0\n",
        "                \n",
        "                # M√©triques d'√©valuation si disponibles\n",
        "                eval_info = \"\"\n",
        "                if 'evaluation' in result:\n",
        "                    eval_data = result['evaluation']\n",
        "                    eval_reward = eval_data.get('avg_reward', eval_data.get('average_reward', 0))\n",
        "                    success_rate = eval_data.get('success_rate', 0)\n",
        "                    eval_info = f\", Eval: {eval_reward:.3f} (Succ√®s: {success_rate:.1%})\"\n",
        "                \n",
        "                print(f\"\\nüéØ {alg_name}:\")\n",
        "                print(f\"   ‚Ä¢ R√©compense moyenne: {avg_reward:.3f} (¬±{std_reward:.3f})\")\n",
        "                print(f\"   ‚Ä¢ Performance finale: {final_avg_reward:.3f}\")\n",
        "                print(f\"   ‚Ä¢ Stabilit√© finale: {final_stability:.3f}\")\n",
        "                print(f\"   ‚Ä¢ √âpisodes total: {total_episodes}{eval_info}\")\n",
        "                \n",
        "                # Caract√©ristiques sp√©cifiques aux algorithmes\n",
        "                if len(history) > 0:\n",
        "                    if 'epsilon' in history[0] and 'epsilon' in history[-1]:\n",
        "                        initial_eps = history[0]['epsilon']\n",
        "                        final_eps = history[-1]['epsilon']\n",
        "                        print(f\"   ‚Ä¢ D√©croissance Œµ: {initial_eps:.3f} ‚Üí {final_eps:.3f}\")\n",
        "                \n",
        "                # Ajouter aux donn√©es de r√©sum√©\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'R√©compense_Moyenne': f\"{avg_reward:.3f}\",\n",
        "                    'R√©compense_Finale': f\"{final_avg_reward:.3f}\",\n",
        "                    'Stabilit√©': f\"{final_stability:.3f}\",\n",
        "                    '√âpisodes': total_episodes\n",
        "                })\n",
        "                \n",
        "            elif 'error' in result:\n",
        "                print(f\"\\n‚ùå {alg_name}: Erreur - {result['error']}\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'R√©compense_Moyenne': \"ERREUR\",\n",
        "                    'R√©compense_Finale': \"ERREUR\",\n",
        "                    'Stabilit√©': \"N/A\",\n",
        "                    '√âpisodes': 0\n",
        "                })\n",
        "            else:\n",
        "                print(f\"\\n‚ùå {alg_name}: Aucune donn√©e valide\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'R√©compense_Moyenne': \"0.000\",\n",
        "                    'R√©compense_Finale': \"0.000\",\n",
        "                    'Stabilit√©': \"N/A\",\n",
        "                    '√âpisodes': 0\n",
        "                })\n",
        "    \n",
        "    # Tableau r√©capitulatif\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\nüìã TABLEAU R√âCAPITULATIF COMPLET:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "def plot_performance_heatmap(all_results):\n",
        "    \"\"\"Heatmap des performances finales par algorithme et environnement\"\"\"\n",
        "    \n",
        "    # Pr√©parer les donn√©es pour la heatmap\n",
        "    env_names = list(all_results.keys())\n",
        "    alg_names = ['MonteCarloES', 'OnPolicyMC', 'OffPolicyMC']\n",
        "    \n",
        "    # Matrice des performances\n",
        "    performance_matrix = []\n",
        "    \n",
        "    for env_name in env_names:\n",
        "        env_row = []\n",
        "        for alg_name in alg_names:\n",
        "            if env_name in all_results and alg_name in all_results[env_name]:\n",
        "                result = all_results[env_name][alg_name]\n",
        "                if 'history' in result and result['history']:\n",
        "                    # Performance finale (moyenne des 20 derniers √©pisodes)\n",
        "                    final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                    final_performance = np.mean(final_rewards)\n",
        "                else:\n",
        "                    final_performance = 0.0\n",
        "            else:\n",
        "                final_performance = 0.0\n",
        "            \n",
        "            env_row.append(final_performance)\n",
        "        performance_matrix.append(env_row)\n",
        "    \n",
        "    # Cr√©er la heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    heatmap = plt.imshow(performance_matrix, cmap='RdYlGn', aspect='auto')\n",
        "    \n",
        "    # Personnaliser la heatmap\n",
        "    plt.xticks(range(len(alg_names)), alg_names, rotation=45)\n",
        "    plt.yticks(range(len(env_names)), env_names)\n",
        "    plt.xlabel('Algorithmes Monte Carlo')\n",
        "    plt.ylabel('Environnements Secrets')\n",
        "    plt.title('üå°Ô∏è Heatmap des Performances Finales\\\\n(R√©compense moyenne des 20 derniers √©pisodes)', \n",
        "              fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Ajouter les valeurs dans les cellules\n",
        "    for i in range(len(env_names)):\n",
        "        for j in range(len(alg_names)):\n",
        "            value = performance_matrix[i][j]\n",
        "            color = 'white' if abs(value) > 0.5 else 'black'\n",
        "            plt.text(j, i, f'{value:.3f}', ha='center', va='center', \n",
        "                    color=color, fontweight='bold', fontsize=11)\n",
        "    \n",
        "    plt.colorbar(heatmap, label='Performance Finale')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def generate_recommendations(all_results):\n",
        "    \"\"\"G√©n√®re des recommandations bas√©es sur l'analyse\"\"\"\n",
        "    \n",
        "    print(\"üí° RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Trouver le meilleur algorithme par environnement\n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                # Score composite : performance finale + stabilit√©\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                if final_rewards:\n",
        "                    avg_performance = np.mean(final_rewards)\n",
        "                    stability = -np.std(final_rewards)  # Negative car moins de variance = mieux\n",
        "                    composite_score = avg_performance * 0.8 + stability * 0.2\n",
        "                    \n",
        "                    if composite_score > best_score:\n",
        "                        best_score = composite_score\n",
        "                        best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\nüèÜ MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   ‚Ä¢ {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {env_name}: Aucun algorithme performant\")\n",
        "    \n",
        "    # Performance globale des algorithmes\n",
        "    alg_global_scores = {'MonteCarloES': [], 'OnPolicyMC': [], 'OffPolicyMC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history'] and alg_name in alg_global_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                avg_performance = np.mean(final_rewards) if final_rewards else 0\n",
        "                alg_global_scores[alg_name].append(avg_performance)\n",
        "    \n",
        "    print(\"\\nüåü PERFORMANCE GLOBALE DES ALGORITHMES:\")\n",
        "    for alg_name, scores in alg_global_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   ‚Ä¢ {alg_name}: {avg_score:.3f} (¬±{std_score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {alg_name}: Aucune donn√©e valide\")\n",
        "    \n",
        "    # Recommandations sp√©cifiques\n",
        "    print(\"\\nüéØ RECOMMANDATIONS SP√âCIFIQUES:\")\n",
        "    print(\"   1. üîÑ MonteCarloES excelle sur les environnements n√©cessitant une exploration intensive\")\n",
        "    print(\"   2. ‚öñÔ∏è  OnPolicyMC offre un bon √©quilibre exploration/exploitation\")\n",
        "    print(\"   3. üéØ OffPolicyMC peut √™tre instable mais performant sur certains environnements\") \n",
        "    print(\"   4. üìä Surveillez les courbes de convergence pour d√©tecter l'instabilit√©\")\n",
        "    print(\"   5. üéõÔ∏è  Ajustez Œ≥ et Œµ selon les caract√©ristiques sp√©cifiques de chaque environnement\")\n",
        "    \n",
        "    print(\"\\nüíæ Pour sauvegarder les r√©sultats, consultez le CSV g√©n√©r√© automatiquement.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(\"üìä Fonctions d'analyse et de visualisation d√©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà G√©n√©ration Compl√®te des R√©sultats et Analyses\n",
        "\n",
        "if all_results and any(env_results for env_results in all_results.values()):\n",
        "    \n",
        "    print(\"üìà G√âN√âRATION DES ANALYSES VISUELLES COMPL√àTES\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # 1. üìä Courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\nüéØ 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        # V√©rifier qu'on a au moins un r√©sultat valide pour cet environnement\n",
        "        has_valid_results = any('history' in result and result['history'] \n",
        "                               for result in env_results.values())\n",
        "        \n",
        "        if has_valid_results:\n",
        "            print(f\"\\\\nüìä G√©n√©ration des graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"‚ùå Pas de donn√©es valides pour {env_name}\")\n",
        "    \n",
        "    # 2. üå°Ô∏è Heatmap comparative des performances\n",
        "    print(\"\\\\nüéØ 2. HEATMAP COMPARATIVE DES PERFORMANCES\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        plot_performance_heatmap(all_results)\n",
        "        print(\"‚úÖ Heatmap g√©n√©r√©e avec succ√®s\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur g√©n√©ration heatmap: {e}\")\n",
        "    \n",
        "    # 3. üîç Analyse d√©taill√©e des r√©sultats\n",
        "    print(\"\\\\nüéØ 3. ANALYSE D√âTAILL√âE DES R√âSULTATS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        summary_df = analyze_results_detailed(all_results)\n",
        "        print(\"‚úÖ Analyse d√©taill√©e termin√©e\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur analyse d√©taill√©e: {e}\")\n",
        "        summary_df = pd.DataFrame()\n",
        "    \n",
        "    # 4. üí° Recommandations et conclusions\n",
        "    print(\"\\\\nüéØ 4. RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        generate_recommendations(all_results)\n",
        "        print(\"‚úÖ Recommandations g√©n√©r√©es\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur g√©n√©ration recommandations: {e}\")\n",
        "    \n",
        "    # 5. üíæ Sauvegarde des r√©sultats\n",
        "    print(\"\\\\nüéØ 5. SAUVEGARDE DES R√âSULTATS\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        if not summary_df.empty:\n",
        "            csv_filename = 'monte_carlo_secret_env_results.csv'\n",
        "            summary_df.to_csv(csv_filename, index=False)\n",
        "            print(f\"‚úÖ R√©sultats sauvegard√©s dans '{csv_filename}'\")\n",
        "            \n",
        "            # Sauvegarder √©galement les donn√©es compl√®tes\n",
        "            detailed_results = []\n",
        "            for env_name, env_results in all_results.items():\n",
        "                for alg_name, result in env_results.items():\n",
        "                    if 'history' in result and result['history']:\n",
        "                        for episode_data in result['history']:\n",
        "                            row = {\n",
        "                                'Environnement': env_name,\n",
        "                                'Algorithme': alg_name,\n",
        "                                **episode_data\n",
        "                            }\n",
        "                            detailed_results.append(row)\n",
        "            \n",
        "            if detailed_results:\n",
        "                detailed_df = pd.DataFrame(detailed_results)\n",
        "                detailed_csv = 'monte_carlo_detailed_history.csv'\n",
        "                detailed_df.to_csv(detailed_csv, index=False)\n",
        "                print(f\"‚úÖ Historique d√©taill√© sauvegard√© dans '{detailed_csv}'\")\n",
        "        else:\n",
        "            print(\"‚ùå Aucune donn√©e √† sauvegarder\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
        "    \n",
        "    # 6. üìä R√©sum√© final avec m√©triques cl√©s\n",
        "    print(\"\\\\nüéØ 6. R√âSUM√â FINAL\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_combinations = 0\n",
        "    successful_combinations = 0\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            total_combinations += 1\n",
        "            if 'history' in result and result['history']:\n",
        "                successful_combinations += 1\n",
        "    \n",
        "    success_percentage = (successful_combinations / total_combinations * 100) if total_combinations > 0 else 0\n",
        "    \n",
        "    print(f\"üìà Combinaisons r√©ussies: {successful_combinations}/{total_combinations} ({success_percentage:.1f}%)\")\n",
        "    print(f\"üéÆ Environnements test√©s: {len(all_results)}\")\n",
        "    print(f\"üß† Algorithmes utilis√©s: MonteCarloES, OnPolicyMC, OffPolicyMC\")\n",
        "    print(f\"üíæ Fichiers g√©n√©r√©s: CSV avec r√©sultats et historiques d√©taill√©s\")\n",
        "    \n",
        "    if successful_combinations > 0:\n",
        "        print(\"\\\\nüéâ ANALYSE MONTE CARLO TERMIN√âE AVEC SUCC√àS !\")\n",
        "        print(\"üïµÔ∏è Les algorithmes Monte Carlo ont r√©v√©l√© les secrets des environnements !\")\n",
        "        \n",
        "        # Afficher quelques statistiques finales int√©ressantes\n",
        "        best_overall_performance = -float('inf')\n",
        "        best_combination = None\n",
        "        \n",
        "        for env_name, env_results in all_results.items():\n",
        "            for alg_name, result in env_results.items():\n",
        "                if 'history' in result and result['history']:\n",
        "                    final_rewards = [h['reward'] for h in result['history'][-10:]]\n",
        "                    avg_final_performance = np.mean(final_rewards)\n",
        "                    \n",
        "                    if avg_final_performance > best_overall_performance:\n",
        "                        best_overall_performance = avg_final_performance\n",
        "                        best_combination = (alg_name, env_name)\n",
        "        \n",
        "        if best_combination:\n",
        "            print(f\"üèÜ Meilleure combinaison globale: {best_combination[0]} sur {best_combination[1]}\")\n",
        "            print(f\"   Performance finale: {best_overall_performance:.3f}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Analyse termin√©e mais aucun r√©sultat valide obtenu\")\n",
        "        print(\"   V√©rifiez la compatibilit√© des environnements secrets\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå AUCUN R√âSULTAT √Ä ANALYSER\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üîç V√©rifications √† effectuer:\")\n",
        "    print(\"   1. Les environnements secrets sont-ils accessibles ?\")\n",
        "    print(\"   2. Les adaptateurs ont-ils √©t√© cr√©√©s correctement ?\") \n",
        "    print(\"   3. Les algorithmes Monte Carlo sont-ils import√©s ?\")\n",
        "    print(\"   4. L'entra√Ænement s'est-il ex√©cut√© sans erreur ?\")\n",
        "    print(\"\\\\nüí° Conseil: Relancez les cellules pr√©c√©dentes pour diagnostiquer le probl√®me\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"üîö FIN DE L'ANALYSE MONTE CARLO SUR LES ENVIRONNEMENTS SECRETS\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Fonctions de visualisation\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "    \n",
        "    # 1. R√©compenses par √©pisode\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            rewards = [h['reward'] for h in history]\n",
        "            \n",
        "            # Moyenne mobile\n",
        "            if len(rewards) >= 20:\n",
        "                rewards_smooth = pd.Series(rewards).rolling(window=20, min_periods=1).mean()\n",
        "                ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i], linewidth=2)\n",
        "            else:\n",
        "                ax1.plot(episodes, rewards, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - R√©compenses par √âpisode')\n",
        "    ax1.set_xlabel('√âpisode')\n",
        "    ax1.set_ylabel('R√©compense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            avg_q = [h['avg_q'] for h in history]\n",
        "            ax2.plot(episodes, avg_q, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - √âvolution des Q-values')\n",
        "    ax2.set_xlabel('√âpisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des √©pisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            lengths = [h['length'] for h in history]\n",
        "            \n",
        "            if len(lengths) >= 20:\n",
        "                lengths_smooth = pd.Series(lengths).rolling(window=20, min_periods=1).mean()\n",
        "                ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i], linewidth=2)\n",
        "            else:\n",
        "                ax3.plot(episodes, lengths, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des √âpisodes')\n",
        "    ax3.set_xlabel('√âpisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Taux de succ√®s cumul√©\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        if 'history' in result and result['history']:\n",
        "            history = result['history']\n",
        "            episodes = [h['episode'] for h in history]\n",
        "            \n",
        "            # Calculer taux de succ√®s cumul√©\n",
        "            success_rates = []\n",
        "            successes = 0\n",
        "            for j, h in enumerate(history):\n",
        "                if h['successful']:\n",
        "                    successes += 1\n",
        "                success_rates.append(successes / (j + 1))\n",
        "            \n",
        "            ax4.plot(episodes, success_rates, label=alg_name, color=colors[i], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Taux de Succ√®s Cumul√©')\n",
        "    ax4.set_xlabel('√âpisode')\n",
        "    ax4.set_ylabel('Taux de Succ√®s')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results(all_results):\n",
        "    \"\"\"Analyse d√©taill√©e des r√©sultats\"\"\"\n",
        "    print(\"üîç ANALYSE D√âTAILL√âE DES R√âSULTATS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\\\nüìä {env_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                history = result['history']\n",
        "                \n",
        "                # Statistiques\n",
        "                total_episodes = len(history)\n",
        "                successful_episodes = sum(1 for h in history if h['successful'])\n",
        "                success_rate = successful_episodes / total_episodes\n",
        "                \n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Performance finale (derniers 20%)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                \n",
        "                print(f\"\\\\nüéØ {alg_name}:\")\n",
        "                print(f\"   ‚Ä¢ Taux de succ√®s: {success_rate:.1%}\")\n",
        "                print(f\"   ‚Ä¢ R√©compense moyenne: {avg_reward:.3f} (¬±{std_reward:.3f})\")\n",
        "                print(f\"   ‚Ä¢ Performance finale: {final_avg_reward:.3f}\")\n",
        "                \n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succ√®s': f\"{success_rate:.1%}\",\n",
        "                    'R√©compense_Moyenne': f\"{avg_reward:.3f}\",\n",
        "                    'R√©compense_Finale': f\"{final_avg_reward:.3f}\",\n",
        "                    '√âpisodes': total_episodes\n",
        "                })\n",
        "            else:\n",
        "                print(f\"\\\\n‚ùå {alg_name}: Aucune donn√©e\")\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succ√®s': \"0%\",\n",
        "                    'R√©compense_Moyenne': \"0.000\",\n",
        "                    'R√©compense_Finale': \"0.000\",\n",
        "                    '√âpisodes': 0\n",
        "                })\n",
        "    \n",
        "    # Tableau r√©capitulatif\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\\\nüìã TABLEAU R√âCAPITULATIF:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "print(\"üìä Fonctions de visualisation d√©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Entra√Ænement Principal\n",
        "\n",
        "def run_monte_carlo_analysis(num_episodes=300):\n",
        "    \"\"\"Lance l'analyse compl√®te\"\"\"\n",
        "    \n",
        "    print(\"üöÄ D√âBUT DE L'ANALYSE MONTE CARLO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Param√®tres: {num_episodes} √©pisodes par algorithme\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for env_name, adapter in adapters.items():\n",
        "        print(f\"\\\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"√âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. Monte Carlo Exploring Starts\n",
        "        print(\"\\\\nüéØ Entra√Ænement Monte Carlo ES...\")\n",
        "        try:\n",
        "            mc_es = SecretMonteCarloES(adapter, gamma=0.99, name=f\"MC-ES-{env_name}\")\n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            env_results['MC-ES'] = result_es\n",
        "            print(f\"‚úÖ MC-ES termin√© - Succ√®s: {result_es['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur MC-ES: {e}\")\n",
        "            env_results['MC-ES'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 2. On-Policy Monte Carlo\n",
        "        print(\"\\\\nüéØ Entra√Ænement On-Policy MC...\")\n",
        "        try:\n",
        "            on_policy_mc = SecretOnPolicyMC(adapter, gamma=0.99, epsilon=0.4, name=f\"OnPolicy-{env_name}\")\n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['On-Policy MC'] = result_on\n",
        "            print(f\"‚úÖ On-Policy MC termin√© - Succ√®s: {result_on['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur On-Policy MC: {e}\")\n",
        "            env_results['On-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # R√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-20:]]\n",
        "                avg_final_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                print(f\"   ‚Ä¢ {alg_name}: R√©compense finale = {avg_final_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå √âchec\")\n",
        "    \n",
        "    print(\"\\\\nüéâ ANALYSE COMPL√àTE TERMIN√âE !\")\n",
        "    return all_results\n",
        "\n",
        "# Lancer l'analyse\n",
        "if adapters:  # Seulement si les adaptateurs ont √©t√© cr√©√©s\n",
        "    print(\"‚è≥ Lancement de l'analyse (cela peut prendre 5-10 minutes)...\")\n",
        "    EPISODES = 300  # Ajustez selon vos besoins\n",
        "    \n",
        "    all_results = run_monte_carlo_analysis(num_episodes=EPISODES)\n",
        "else:\n",
        "    print(\"‚ùå Impossible de lancer l'analyse - adaptateurs non disponibles\")\n",
        "    all_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà Affichage des R√©sultats\n",
        "\n",
        "if all_results:\n",
        "    print(\"üìà G√âN√âRATION DES ANALYSES VISUELLES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\\\nüéØ 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        if any(result['history'] for result in env_results.values() if 'history' in result):\n",
        "            print(f\"\\\\nüìä Graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"‚ùå Pas de donn√©es pour {env_name}\")\n",
        "    \n",
        "    # 2. Analyse d√©taill√©e\n",
        "    print(\"\\\\nüéØ 2. ANALYSE D√âTAILL√âE\")\n",
        "    print(\"-\" * 50)\n",
        "    summary_df = analyze_results(all_results)\n",
        "    \n",
        "    # 3. Recommandations\n",
        "    print(\"\\\\nüéØ 3. RECOMMANDATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                \n",
        "                # Score composite\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                \n",
        "                if composite_score > best_score:\n",
        "                    best_score = composite_score\n",
        "                    best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\\\nüèÜ MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   ‚Ä¢ {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {env_name}: Aucun algorithme efficace\")\n",
        "    \n",
        "    # 4. Performance globale\n",
        "    alg_scores = {'MC-ES': [], 'On-Policy MC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if 'history' in result and result['history'] and alg_name in alg_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                alg_scores[alg_name].append(composite_score)\n",
        "    \n",
        "    print(\"\\\\nüåü PERFORMANCE GLOBALE:\")\n",
        "    for alg_name, scores in alg_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   ‚Ä¢ {alg_name}: {avg_score:.3f} (¬±{std_score:.3f})\")\n",
        "    \n",
        "    # 5. Conseils\n",
        "    print(\"\\\\nüí° CONSEILS D'INTERPR√âTATION:\")\n",
        "    print(\"   1. üéØ Taux de succ√®s √©lev√© = algorithme stable\")\n",
        "    print(\"   2. üîÑ R√©compenses croissantes = apprentissage effectif\")\n",
        "    print(\"   3. üìä Q-values convergentes = politique stable\")\n",
        "    print(\"   4. üéõÔ∏è  Ajustez les hyperparam√®tres si n√©cessaire\")\n",
        "    \n",
        "    # Sauvegarde\n",
        "    try:\n",
        "        summary_df.to_csv('monte_carlo_results.csv', index=False)\n",
        "        print(f\"\\\\nüíæ R√©sultats sauvegard√©s dans 'monte_carlo_results.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå AUCUN R√âSULTAT √Ä AFFICHER\")\n",
        "    print(\"V√©rifiez que l'entra√Ænement pr√©c√©dent s'est bien d√©roul√©.\")\n",
        "\n",
        "print(\"\\\\nüéâ ANALYSE MONTE CARLO TERMIN√âE !\")\n",
        "print(\"üïµÔ∏è Les environnements secrets ont r√©v√©l√© leurs myst√®res !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üïµÔ∏è Analyse Monte Carlo sur les Environnements Secrets\n",
        "\n",
        "## üéØ Objectif\n",
        "Ce notebook impl√©mente et compare les trois algorithmes Monte Carlo sur les 4 environnements secrets :\n",
        "- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**\n",
        "\n",
        "## üìã Algorithmes Monte Carlo test√©s\n",
        "- **Monte Carlo Exploring Starts (MC-ES)** : Garantit l'exploration avec starts al√©atoires\n",
        "- **On-Policy Monte Carlo** : Am√©lioration de politique Œµ-greedy on-policy  \n",
        "- **Off-Policy Monte Carlo** : Apprentissage avec importance sampling\n",
        "\n",
        "## üìä Analyses effectu√©es\n",
        "- Courbes d'apprentissage (r√©compenses, Q-values)\n",
        "- Convergence des politiques\n",
        "- Comparaison des performances entre algorithmes\n",
        "- Analyse de la stabilit√© d'apprentissage\n",
        "- M√©triques de succ√®s par environnement\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìö Imports et configuration\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (14, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Ajouter les chemins n√©cessaires\n",
        "project_root = os.path.abspath('../../')\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, os.path.join(project_root, 'game', 'secret_env'))\n",
        "\n",
        "# Imports des environnements secrets\n",
        "try:\n",
        "    from secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
        "    print(\"‚úÖ Environnements secrets import√©s avec succ√®s\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur d'import des environnements secrets: {e}\")\n",
        "    # Fallback pour les imports\n",
        "    import ctypes\n",
        "    import platform\n",
        "    \n",
        "print(\"üîß Configuration termin√©e !\")\n",
        "\n",
        "# Test rapide des environnements\n",
        "try:\n",
        "    env0 = SecretEnv0()\n",
        "    print(f\"üìä SecretEnv0 - √âtats: {env0.num_states()}, Actions: {env0.num_actions()}\")\n",
        "    \n",
        "    env1 = SecretEnv1()\n",
        "    print(f\"üìä SecretEnv1 - √âtats: {env1.num_states()}, Actions: {env1.num_actions()}\")\n",
        "    \n",
        "    env2 = SecretEnv2()\n",
        "    print(f\"üìä SecretEnv2 - √âtats: {env2.num_states()}, Actions: {env2.num_actions()}\")\n",
        "    \n",
        "    env3 = SecretEnv3()\n",
        "    print(f\"üìä SecretEnv3 - √âtats: {env3.num_states()}, Actions: {env3.num_actions()}\")\n",
        "    \n",
        "    print(\"\\nüéâ Tous les environnements secrets sont fonctionnels !\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors du test des environnements: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Adaptateur d'environnement pour les Secret Envs\n",
        "\n",
        "class SecretEnvAdapter:\n",
        "    \"\"\"\n",
        "    Adaptateur pour rendre les SecretEnv compatibles avec l'API Gym standard.\n",
        "    Transforme l'interface sp√©cifique des environnements secrets en interface standard.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, secret_env_class, env_name=\"SecretEnv\"):\n",
        "        self.secret_env_class = secret_env_class\n",
        "        self.env_name = env_name\n",
        "        self.env = secret_env_class()\n",
        "        \n",
        "        # Propri√©t√©s MDP pour compatibilit√© avec Monte Carlo\n",
        "        self.nS = self.env.num_states()\n",
        "        self.nA = self.env.num_actions()\n",
        "        \n",
        "        # √âtat et r√©compenses\n",
        "        self.current_state = None\n",
        "        self.last_score = 0.0\n",
        "        self.episode_steps = 0\n",
        "        \n",
        "        print(f\"üèóÔ∏è  {env_name} adapter cr√©√© - √âtats: {self.nS}, Actions: {self.nA}\")\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"R√©initialise l'environnement et retourne l'√©tat initial\"\"\"\n",
        "        try:\n",
        "            self.env.reset()\n",
        "            self.current_state = self.env.state_id()\n",
        "            self.last_score = self.env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur reset {self.env_name}: {e}\")\n",
        "            # Cr√©er un nouvel environnement si reset √©choue\n",
        "            self.env = self.secret_env_class()\n",
        "            self.env.reset()\n",
        "            self.current_state = self.env.state_id()\n",
        "            self.last_score = self.env.score()\n",
        "            self.episode_steps = 0\n",
        "            return self.current_state\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Ex√©cute une action et retourne (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Obtenir les actions disponibles\n",
        "            available_actions = self.get_available_actions()\n",
        "            \n",
        "            # V√©rifier si l'action est valide\n",
        "            if action not in available_actions:\n",
        "                # Action non valide - retourner r√©compense n√©gative et rester dans l'√©tat\n",
        "                return self.current_state, -0.1, False, {\n",
        "                    'invalid_action': True,\n",
        "                    'available_actions': available_actions,\n",
        "                    'requested_action': action\n",
        "                }\n",
        "            \n",
        "            # Sauvegarder le score avant l'action\n",
        "            old_score = self.env.score()\n",
        "            \n",
        "            # Ex√©cuter l'action\n",
        "            self.env.step(action)\n",
        "            self.episode_steps += 1\n",
        "            \n",
        "            # Obtenir le nouvel √©tat et calculer la r√©compense\n",
        "            next_state = self.env.state_id()\n",
        "            new_score = self.env.score()\n",
        "            reward = new_score - old_score  # R√©compense diff√©rentielle\n",
        "            done = self.env.is_game_over()\n",
        "            \n",
        "            # Mise √† jour\n",
        "            self.current_state = next_state\n",
        "            self.last_score = new_score\n",
        "            \n",
        "            info = {\n",
        "                'available_actions': self.get_available_actions(),\n",
        "                'cumulative_score': new_score,\n",
        "                'episode_steps': self.episode_steps,\n",
        "                'valid_action': True\n",
        "            }\n",
        "            \n",
        "            # Limite de s√©curit√© pour √©viter les √©pisodes infinis\n",
        "            if self.episode_steps > 1000:\n",
        "                done = True\n",
        "                reward -= 1.0  # P√©nalit√© pour √©pisode trop long\n",
        "                info['timeout'] = True\n",
        "            \n",
        "            return next_state, reward, done, info\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur step {self.env_name}: {e}\")\n",
        "            # Retourner un √©tat d'erreur\n",
        "            return self.current_state, -1.0, True, {'error': str(e)}\n",
        "    \n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Obtient la liste des actions disponibles dans l'√©tat courant\"\"\"\n",
        "        try:\n",
        "            actions = self.env.available_actions()\n",
        "            return list(actions) if len(actions) > 0 else [0]\n",
        "        except:\n",
        "            # Fallback : toutes les actions sont disponibles\n",
        "            return list(range(self.nA))\n",
        "    \n",
        "    def display(self):\n",
        "        \"\"\"Affiche l'√©tat courant de l'environnement\"\"\"\n",
        "        try:\n",
        "            self.env.display()\n",
        "        except:\n",
        "            print(f\"√âtat courant: {self.current_state}, Score: {self.last_score}\")\n",
        "    \n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"Retourne les informations MDP pour compatibilit√©\"\"\"\n",
        "        return {\n",
        "            'states': list(range(self.nS)),\n",
        "            'actions': list(range(self.nA)),\n",
        "            'n_states': self.nS,\n",
        "            'n_actions': self.nA,\n",
        "            'name': self.env_name\n",
        "        }\n",
        "\n",
        "# Test des adaptateurs\n",
        "print(\"üß™ Test des adaptateurs...\")\n",
        "adapters = {}\n",
        "\n",
        "try:\n",
        "    adapters['SecretEnv0'] = SecretEnvAdapter(SecretEnv0, \"SecretEnv0\")\n",
        "    adapters['SecretEnv1'] = SecretEnvAdapter(SecretEnv1, \"SecretEnv1\")\n",
        "    adapters['SecretEnv2'] = SecretEnvAdapter(SecretEnv2, \"SecretEnv2\")\n",
        "    adapters['SecretEnv3'] = SecretEnvAdapter(SecretEnv3, \"SecretEnv3\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Tous les adaptateurs cr√©√©s avec succ√®s !\")\n",
        "    \n",
        "    # Test rapide d'un adaptateur\n",
        "    test_adapter = adapters['SecretEnv0']\n",
        "    state = test_adapter.reset()\n",
        "    available = test_adapter.get_available_actions()\n",
        "    print(f\"üîç Test SecretEnv0 - √âtat initial: {state}, Actions disponibles: {available}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors de la cr√©ation des adaptateurs: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéÆ Impl√©mentation des Algorithmes Monte Carlo pour les Environnements Secrets\n",
        "\n",
        "class SecretMonteCarloES:\n",
        "    \"\"\"Monte Carlo avec Exploring Starts adapt√© aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, name=\"MC-ES\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))  # Initialisation al√©atoire\n",
        "        self.policy = np.zeros(self.nS, dtype=int)\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        \n",
        "        # Historique d'entra√Ænement\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"üéØ {name} initialis√© pour {env_adapter.env_name}\")\n",
        "    \n",
        "    def generate_episode_with_exploring_starts(self):\n",
        "        \"\"\"G√©n√®re un √©pisode avec exploring starts\"\"\"\n",
        "        episode = []\n",
        "        \n",
        "        # Reset avec √©tat al√©atoire (approximation d'exploring starts)\n",
        "        for _ in range(10):  # Essayer plusieurs resets pour varier l'√©tat initial\n",
        "            state = self.env_adapter.reset()\n",
        "            if np.random.random() < 0.3:  # 30% chance d'accepter cet √©tat\n",
        "                break\n",
        "        \n",
        "        # Action initiale al√©atoire (exploring starts)\n",
        "        available_actions = self.env_adapter.get_available_actions()\n",
        "        if len(available_actions) > 0:\n",
        "            action = np.random.choice(available_actions)\n",
        "        else:\n",
        "            action = 0\n",
        "        \n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            # Action suivante selon politique courante avec actions disponibles\n",
        "            state = next_state\n",
        "            available_actions = info.get('available_actions', list(range(self.nA)))\n",
        "            \n",
        "            if len(available_actions) > 0:\n",
        "                # Politique greedy avec tie-breaking al√©atoire sur actions disponibles\n",
        "                q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "                max_q = np.max(q_vals)\n",
        "                best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "                action = np.random.choice(best_actions)\n",
        "            else:\n",
        "                break\n",
        "            \n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"Entra√Ænement Monte Carlo ES\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                # G√©n√©rer √©pisode avec exploring starts\n",
        "                episode = self.generate_episode_with_exploring_starts()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Mise √† jour First-Visit Monte Carlo\n",
        "                    G = 0.0\n",
        "                    visited = set()\n",
        "                    \n",
        "                    for (state, action, reward) in reversed(episode):\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        if (state, action) not in visited:\n",
        "                            visited.add((state, action))\n",
        "                            self.returns_count[(state, action)] += 1\n",
        "                            self.returns_sum[(state, action)] += G\n",
        "                            self.Q[state, action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
        "                    \n",
        "                    # Am√©lioration de la politique (greedy)\n",
        "                    for s in range(self.nS):\n",
        "                        self.policy[s] = np.argmax(self.Q[s])\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    # √âpisode √©chou√©\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] √âpisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succ√®s: {success_rate:.2f}, \"\n",
        "                          f\"R√©compense r√©cente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur √©pisode {episode_num + 1}: {e}\")\n",
        "                self.history.append({\n",
        "                    'episode': episode_num + 1,\n",
        "                    'reward': 0.0,\n",
        "                    'length': 0,\n",
        "                    'avg_q': np.mean(self.Q),\n",
        "                    'successful': False\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "class SecretOnPolicyMC:\n",
        "    \"\"\"On-Policy Monte Carlo avec Œµ-greedy adapt√© aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, epsilon=0.3, name=\"On-Policy MC\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_epsilon = epsilon\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))\n",
        "        self.policy = np.zeros(self.nS, dtype=int)\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        \n",
        "        # Historique\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"üéØ {name} initialis√© pour {env_adapter.env_name} (Œµ={epsilon})\")\n",
        "    \n",
        "    def epsilon_greedy_action(self, state, available_actions):\n",
        "        \"\"\"S√©lectionne une action selon Œµ-greedy parmi les actions disponibles\"\"\"\n",
        "        if len(available_actions) == 0:\n",
        "            return 0\n",
        "        \n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            # Greedy : meilleure action parmi les disponibles\n",
        "            q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "            max_q = np.max(q_vals)\n",
        "            best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "    \n",
        "    def generate_episode(self):\n",
        "        \"\"\"G√©n√®re un √©pisode selon la politique Œµ-greedy\"\"\"\n",
        "        episode = []\n",
        "        state = self.env_adapter.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            available_actions = self.env_adapter.get_available_actions()\n",
        "            action = self.epsilon_greedy_action(state, available_actions)\n",
        "            \n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"Entra√Ænement On-Policy Monte Carlo\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                episode = self.generate_episode()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Mise √† jour First-Visit Monte Carlo\n",
        "                    G = 0.0\n",
        "                    visited = set()\n",
        "                    \n",
        "                    for (state, action, reward) in reversed(episode):\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        if (state, action) not in visited:\n",
        "                            visited.add((state, action))\n",
        "                            self.returns_count[(state, action)] += 1\n",
        "                            self.returns_sum[(state, action)] += G\n",
        "                            self.Q[state, action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
        "                    \n",
        "                    # Am√©lioration de politique\n",
        "                    for s in range(self.nS):\n",
        "                        self.policy[s] = np.argmax(self.Q[s])\n",
        "                    \n",
        "                    # D√©croissance d'epsilon\n",
        "                    self.epsilon = max(0.01, self.epsilon * 0.9995)\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'epsilon': self.epsilon,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'epsilon': self.epsilon,\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] √âpisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succ√®s: {success_rate:.2f}, \"\n",
        "                          f\"Œµ: {self.epsilon:.3f}, \"\n",
        "                          f\"R√©compense r√©cente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur √©pisode {episode_num + 1}: {e}\")\n",
        "                \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "class SecretOffPolicyMC:\n",
        "    \"\"\"Off-Policy Monte Carlo avec Importance Sampling adapt√© aux environnements secrets\"\"\"\n",
        "    \n",
        "    def __init__(self, env_adapter, gamma=0.99, epsilon=0.4, name=\"Off-Policy MC\"):\n",
        "        self.env_adapter = env_adapter\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.name = name\n",
        "        \n",
        "        # Structures Monte Carlo\n",
        "        self.nS = env_adapter.nS\n",
        "        self.nA = env_adapter.nA\n",
        "        self.Q = np.random.uniform(-0.1, 0.1, (self.nS, self.nA))\n",
        "        self.target_policy = np.zeros(self.nS, dtype=int)\n",
        "        self.C = np.zeros((self.nS, self.nA))  # Poids cumulatifs\n",
        "        \n",
        "        # Historique\n",
        "        self.history = []\n",
        "        \n",
        "        print(f\"üéØ {name} initialis√© pour {env_adapter.env_name} (Œµ={epsilon})\")\n",
        "    \n",
        "    def behavior_policy(self, state, available_actions):\n",
        "        \"\"\"Politique de comportement Œµ-greedy\"\"\"\n",
        "        if len(available_actions) == 0:\n",
        "            return 0\n",
        "            \n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(available_actions)\n",
        "        else:\n",
        "            q_vals = np.array([self.Q[state, a] for a in available_actions])\n",
        "            max_q = np.max(q_vals)\n",
        "            best_actions = [a for a in available_actions if self.Q[state, a] == max_q]\n",
        "            return np.random.choice(best_actions)\n",
        "    \n",
        "    def generate_episode(self):\n",
        "        \"\"\"G√©n√®re un √©pisode selon la politique de comportement\"\"\"\n",
        "        episode = []\n",
        "        state = self.env_adapter.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 500\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            available_actions = self.env_adapter.get_available_actions()\n",
        "            action = self.behavior_policy(state, available_actions)\n",
        "            \n",
        "            next_state, reward, done, info = self.env_adapter.step(action)\n",
        "            episode.append((state, action, reward, available_actions.copy()))\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        \n",
        "        return episode\n",
        "    \n",
        "    def train(self, num_episodes=1000):\n",
        "        \"\"\"Entra√Ænement Off-Policy Monte Carlo avec Importance Sampling\"\"\"\n",
        "        self.history = []\n",
        "        successful_episodes = 0\n",
        "        \n",
        "        for episode_num in range(num_episodes):\n",
        "            try:\n",
        "                episode = self.generate_episode()\n",
        "                \n",
        "                if len(episode) > 0:\n",
        "                    successful_episodes += 1\n",
        "                    \n",
        "                    # Importance Sampling Update\n",
        "                    G = 0.0\n",
        "                    W = 1.0\n",
        "                    \n",
        "                    for i in range(len(episode) - 1, -1, -1):\n",
        "                        state, action, reward, available_actions = episode[i]\n",
        "                        G = self.gamma * G + reward\n",
        "                        \n",
        "                        # Mettre √† jour C et Q\n",
        "                        self.C[state, action] += W\n",
        "                        if self.C[state, action] > 0:\n",
        "                            self.Q[state, action] += (W / self.C[state, action]) * (G - self.Q[state, action])\n",
        "                        \n",
        "                        # Mettre √† jour la politique cible (greedy)\n",
        "                        self.target_policy[state] = np.argmax(self.Q[state])\n",
        "                        \n",
        "                        # V√©rifier si l'action est celle de la politique cible\n",
        "                        if action != self.target_policy[state]:\n",
        "                            break\n",
        "                        \n",
        "                        # Calculer le ratio d'importance\n",
        "                        # Probabilit√© politique cible (d√©terministe)\n",
        "                        target_prob = 1.0\n",
        "                        \n",
        "                        # Probabilit√© politique de comportement\n",
        "                        if len(available_actions) > 0:\n",
        "                            if action == np.argmax([self.Q[state, a] for a in available_actions]):\n",
        "                                behavior_prob = 1.0 - self.epsilon + self.epsilon / len(available_actions)\n",
        "                            else:\n",
        "                                behavior_prob = self.epsilon / len(available_actions)\n",
        "                        else:\n",
        "                            behavior_prob = 1.0\n",
        "                        \n",
        "                        if behavior_prob > 0:\n",
        "                            W *= target_prob / behavior_prob\n",
        "                        else:\n",
        "                            break\n",
        "                    \n",
        "                    # Statistiques\n",
        "                    episode_reward = sum(r for _, _, r, _ in episode)\n",
        "                    avg_q = np.mean(self.Q)\n",
        "                    \n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': episode_reward,\n",
        "                        'length': len(episode),\n",
        "                        'avg_q': avg_q,\n",
        "                        'avg_weight': np.mean(self.C[self.C > 0]) if np.any(self.C > 0) else 0,\n",
        "                        'successful': True\n",
        "                    })\n",
        "                else:\n",
        "                    self.history.append({\n",
        "                        'episode': episode_num + 1,\n",
        "                        'reward': 0.0,\n",
        "                        'length': 0,\n",
        "                        'avg_q': np.mean(self.Q),\n",
        "                        'avg_weight': 0,\n",
        "                        'successful': False\n",
        "                    })\n",
        "                \n",
        "                if (episode_num + 1) % 200 == 0:\n",
        "                    success_rate = successful_episodes / (episode_num + 1)\n",
        "                    recent_rewards = [h['reward'] for h in self.history[-50:]]\n",
        "                    avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
        "                    print(f\"[{self.name}] √âpisode {episode_num + 1}: \"\n",
        "                          f\"Taux de succ√®s: {success_rate:.2f}, \"\n",
        "                          f\"R√©compense r√©cente: {avg_recent_reward:.3f}\")\n",
        "                          \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur √©pisode {episode_num + 1}: {e}\")\n",
        "                \n",
        "        return {\n",
        "            'Q': self.Q,\n",
        "            'policy': self.target_policy,\n",
        "            'history': self.history,\n",
        "            'success_rate': successful_episodes / num_episodes\n",
        "        }\n",
        "\n",
        "print(\"üéØ Algorithmes Monte Carlo d√©finis avec succ√®s !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Fonctions de visualisation et d'analyse\n",
        "\n",
        "def plot_learning_curves(results_dict, title_prefix=\"\"):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les algorithmes\"\"\"\n",
        "    \n",
        "    n_algorithms = len(results_dict)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Couleurs pour chaque algorithme\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    # 1. R√©compenses par √©pisode\n",
        "    ax1 = axes[0]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        rewards = [h['reward'] for h in history]\n",
        "        \n",
        "        # Moyenne mobile pour lisser les courbes\n",
        "        window_size = min(50, len(rewards) // 10 + 1)\n",
        "        if len(rewards) >= window_size:\n",
        "            rewards_smooth = pd.Series(rewards).rolling(window=window_size, min_periods=1).mean()\n",
        "            ax1.plot(episodes, rewards_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "        else:\n",
        "            ax1.plot(episodes, rewards, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax1.set_title(f'{title_prefix} - R√©compenses par √âpisode', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('√âpisode')\n",
        "    ax1.set_ylabel('R√©compense')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Q-values moyennes\n",
        "    ax2 = axes[1]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        avg_q = [h['avg_q'] for h in history]\n",
        "        \n",
        "        ax2.plot(episodes, avg_q, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax2.set_title(f'{title_prefix} - √âvolution des Q-values', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('√âpisode')\n",
        "    ax2.set_ylabel('Q-value Moyenne')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Longueur des √©pisodes\n",
        "    ax3 = axes[2]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = [h['episode'] for h in history]\n",
        "        lengths = [h['length'] for h in history]\n",
        "        \n",
        "        # Moyenne mobile\n",
        "        window_size = min(50, len(lengths) // 10 + 1)\n",
        "        if len(lengths) >= window_size:\n",
        "            lengths_smooth = pd.Series(lengths).rolling(window=window_size, min_periods=1).mean()\n",
        "            ax3.plot(episodes, lengths_smooth, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "        else:\n",
        "            ax3.plot(episodes, lengths, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax3.set_title(f'{title_prefix} - Longueur des √âpisodes', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('√âpisode')\n",
        "    ax3.set_ylabel('Nombre de Steps')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Analyse de convergence (√©cart-type des r√©compenses r√©centes)\n",
        "    ax4 = axes[3]\n",
        "    for i, (alg_name, result) in enumerate(results_dict.items()):\n",
        "        history = result['history']\n",
        "        episodes = []\n",
        "        stds = []\n",
        "        \n",
        "        window_size = 100\n",
        "        for j in range(window_size, len(history)):\n",
        "            recent_rewards = [h['reward'] for h in history[j-window_size:j]]\n",
        "            episodes.append(history[j]['episode'])\n",
        "            stds.append(np.std(recent_rewards))\n",
        "        \n",
        "        if len(episodes) > 0:\n",
        "            ax4.plot(episodes, stds, label=alg_name, color=colors[i % len(colors)], linewidth=2)\n",
        "    \n",
        "    ax4.set_title(f'{title_prefix} - Stabilit√© (√âcart-type des r√©compenses)', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('√âpisode')\n",
        "    ax4.set_ylabel('√âcart-type des r√©compenses')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_performance_comparison(all_results):\n",
        "    \"\"\"Compare les performances finales de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    # Pr√©parer les donn√©es pour la visualisation\n",
        "    env_names = list(all_results.keys())\n",
        "    alg_names = list(list(all_results.values())[0].keys())\n",
        "    \n",
        "    # Metrics √† analyser\n",
        "    final_rewards = []\n",
        "    success_rates = []\n",
        "    avg_q_values = []\n",
        "    \n",
        "    for env_name in env_names:\n",
        "        env_rewards = []\n",
        "        env_success_rates = []\n",
        "        env_avg_q = []\n",
        "        \n",
        "        for alg_name in alg_names:\n",
        "            result = all_results[env_name][alg_name]\n",
        "            \n",
        "            # R√©compense finale (moyenne des 100 derniers √©pisodes)\n",
        "            history = result['history']\n",
        "            if len(history) >= 100:\n",
        "                final_reward = np.mean([h['reward'] for h in history[-100:]])\n",
        "            else:\n",
        "                final_reward = np.mean([h['reward'] for h in history]) if history else 0\n",
        "            \n",
        "            env_rewards.append(final_reward)\n",
        "            env_success_rates.append(result.get('success_rate', 0))\n",
        "            env_avg_q.append(result['history'][-1]['avg_q'] if result['history'] else 0)\n",
        "        \n",
        "        final_rewards.append(env_rewards)\n",
        "        success_rates.append(env_success_rates)\n",
        "        avg_q_values.append(env_avg_q)\n",
        "    \n",
        "    # Cr√©er les graphiques\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Heatmap des r√©compenses finales\n",
        "    ax1 = axes[0, 0]\n",
        "    im1 = ax1.imshow(final_rewards, cmap='RdYlGn', aspect='auto')\n",
        "    ax1.set_xticks(range(len(alg_names)))\n",
        "    ax1.set_xticklabels(alg_names, rotation=45)\n",
        "    ax1.set_yticks(range(len(env_names)))\n",
        "    ax1.set_yticklabels(env_names)\n",
        "    ax1.set_title('R√©compenses Finales par Algorithme et Environnement')\n",
        "    \n",
        "    # Ajouter les valeurs dans les cellules\n",
        "    for i in range(len(env_names)):\n",
        "        for j in range(len(alg_names)):\n",
        "            ax1.text(j, i, f'{final_rewards[i][j]:.2f}', ha='center', va='center')\n",
        "    \n",
        "    plt.colorbar(im1, ax=ax1)\n",
        "    \n",
        "    # 2. Graphique en barres des taux de succ√®s\n",
        "    ax2 = axes[0, 1]\n",
        "    x = np.arange(len(env_names))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        success_data = [success_rates[j][i] for j in range(len(env_names))]\n",
        "        ax2.bar(x + i * width, success_data, width, label=alg_name, alpha=0.8)\n",
        "    \n",
        "    ax2.set_xlabel('Environnements')\n",
        "    ax2.set_ylabel('Taux de Succ√®s')\n",
        "    ax2.set_title('Taux de Succ√®s par Environnement et Algorithme')\n",
        "    ax2.set_xticks(x + width * 1.5)\n",
        "    ax2.set_xticklabels(env_names)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Comparaison des Q-values moyennes finales\n",
        "    ax3 = axes[1, 0]\n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        q_data = [avg_q_values[j][i] for j in range(len(env_names))]\n",
        "        ax3.bar(x + i * width, q_data, width, label=alg_name, alpha=0.8)\n",
        "    \n",
        "    ax3.set_xlabel('Environnements')\n",
        "    ax3.set_ylabel('Q-value Moyenne Finale')\n",
        "    ax3.set_title('Q-values Finales par Environnement et Algorithme')\n",
        "    ax3.set_xticks(x + width * 1.5)\n",
        "    ax3.set_xticklabels(env_names)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Graphique radar des performances g√©n√©rales\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    # Normaliser les m√©triques pour le radar\n",
        "    final_rewards_norm = np.array(final_rewards)\n",
        "    success_rates_norm = np.array(success_rates)\n",
        "    \n",
        "    # Score composite pour chaque algorithme\n",
        "    composite_scores = []\n",
        "    for i, alg_name in enumerate(alg_names):\n",
        "        alg_rewards = [final_rewards[j][i] for j in range(len(env_names))]\n",
        "        alg_success = [success_rates[j][i] for j in range(len(env_names))]\n",
        "        \n",
        "        # Score composite (moyenne pond√©r√©e)\n",
        "        composite_score = np.mean(alg_rewards) * 0.7 + np.mean(alg_success) * 0.3\n",
        "        composite_scores.append(composite_score)\n",
        "    \n",
        "    bars = ax4.bar(alg_names, composite_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(alg_names)])\n",
        "    ax4.set_title('Score Composite Global par Algorithme')\n",
        "    ax4.set_ylabel('Score Composite')\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for bar, score in zip(bars, composite_scores):\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_algorithm_characteristics(all_results):\n",
        "    \"\"\"Analyse les caract√©ristiques sp√©cifiques de chaque algorithme\"\"\"\n",
        "    \n",
        "    print(\"üîç ANALYSE D√âTAILL√âE DES ALGORITHMES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        print(f\"\\nüìä {env_name.upper()}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            history = result['history']\n",
        "            \n",
        "            if len(history) > 0:\n",
        "                # Statistiques g√©n√©rales\n",
        "                total_episodes = len(history)\n",
        "                successful_episodes = sum(1 for h in history if h['successful'])\n",
        "                success_rate = successful_episodes / total_episodes\n",
        "                \n",
        "                # R√©compenses\n",
        "                all_rewards = [h['reward'] for h in history]\n",
        "                avg_reward = np.mean(all_rewards)\n",
        "                std_reward = np.std(all_rewards)\n",
        "                \n",
        "                # Convergence (derniers 20% d'√©pisodes)\n",
        "                final_portion = history[int(0.8 * len(history)):]\n",
        "                final_rewards = [h['reward'] for h in final_portion]\n",
        "                final_avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                \n",
        "                # Stabilit√© (√©cart-type des derniers √©pisodes)\n",
        "                final_stability = np.std(final_rewards) if len(final_rewards) > 1 else 0\n",
        "                \n",
        "                print(f\"\\nüéØ {alg_name}:\")\n",
        "                print(f\"   ‚Ä¢ Taux de succ√®s: {success_rate:.1%}\")\n",
        "                print(f\"   ‚Ä¢ R√©compense moyenne: {avg_reward:.3f} (¬±{std_reward:.3f})\")\n",
        "                print(f\"   ‚Ä¢ Performance finale: {final_avg_reward:.3f}\")\n",
        "                print(f\"   ‚Ä¢ Stabilit√© finale: {final_stability:.3f}\")\n",
        "                \n",
        "                # Caract√©ristiques sp√©cifiques √† l'algorithme\n",
        "                if 'epsilon' in history[0]:\n",
        "                    initial_eps = history[0]['epsilon']\n",
        "                    final_eps = history[-1]['epsilon']\n",
        "                    print(f\"   ‚Ä¢ D√©croissance Œµ: {initial_eps:.3f} ‚Üí {final_eps:.3f}\")\n",
        "                \n",
        "                if 'avg_weight' in history[0]:\n",
        "                    final_weight = history[-1]['avg_weight']\n",
        "                    print(f\"   ‚Ä¢ Poids moyen final: {final_weight:.3f}\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå {alg_name}: Aucune donn√©e d'entra√Ænement\")\n",
        "\n",
        "print(\"üìä Fonctions de visualisation d√©finies !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Entra√Ænement Principal - Tous les Algorithmes sur Tous les Environnements\n",
        "\n",
        "def run_complete_analysis(num_episodes=1000):\n",
        "    \"\"\"Lance l'analyse compl√®te de tous les algorithmes sur tous les environnements\"\"\"\n",
        "    \n",
        "    print(\"üöÄ D√âBUT DE L'ANALYSE COMPL√àTE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Param√®tres: {num_episodes} √©pisodes par algorithme\")\n",
        "    print(f\"Total: {4} environnements √ó {3} algorithmes = {12} entra√Ænements\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Dictionnaire pour stocker tous les r√©sultats\n",
        "    all_results = {}\n",
        "    \n",
        "    # Environnements √† tester\n",
        "    env_classes = {\n",
        "        'SecretEnv0': SecretEnv0,\n",
        "        'SecretEnv1': SecretEnv1, \n",
        "        'SecretEnv2': SecretEnv2,\n",
        "        'SecretEnv3': SecretEnv3\n",
        "    }\n",
        "    \n",
        "    # Cr√©er les adaptateurs\n",
        "    adapters_dict = {}\n",
        "    for env_name, env_class in env_classes.items():\n",
        "        try:\n",
        "            adapters_dict[env_name] = SecretEnvAdapter(env_class, env_name)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur cr√©ation adaptateur {env_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(adapters_dict)} adaptateurs cr√©√©s avec succ√®s\")\n",
        "    \n",
        "    # Entra√Ænement pour chaque environnement\n",
        "    for env_name, adapter in adapters_dict.items():\n",
        "        print(f\"\\nüéÆ ENVIRONNEMENT: {env_name}\")\n",
        "        print(f\"√âtats: {adapter.nS}, Actions: {adapter.nA}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        env_results = {}\n",
        "        \n",
        "        # 1. Monte Carlo Exploring Starts\n",
        "        print(\"\\\\nüéØ Entra√Ænement Monte Carlo ES...\")\n",
        "        try:\n",
        "            mc_es = SecretMonteCarloES(adapter, gamma=0.99, name=f\"MC-ES-{env_name}\")\n",
        "            result_es = mc_es.train(num_episodes=num_episodes)\n",
        "            env_results['MC-ES'] = result_es\n",
        "            print(f\"‚úÖ MC-ES termin√© - Taux de succ√®s: {result_es['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur MC-ES sur {env_name}: {e}\")\n",
        "            env_results['MC-ES'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 2. On-Policy Monte Carlo\n",
        "        print(\"\\\\nüéØ Entra√Ænement On-Policy MC...\")\n",
        "        try:\n",
        "            on_policy_mc = SecretOnPolicyMC(adapter, gamma=0.99, epsilon=0.3, name=f\"OnPolicy-{env_name}\")\n",
        "            result_on = on_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['On-Policy MC'] = result_on\n",
        "            print(f\"‚úÖ On-Policy MC termin√© - Taux de succ√®s: {result_on['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur On-Policy MC sur {env_name}: {e}\")\n",
        "            env_results['On-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # 3. Off-Policy Monte Carlo\n",
        "        print(\"\\\\nüéØ Entra√Ænement Off-Policy MC...\")\n",
        "        try:\n",
        "            off_policy_mc = SecretOffPolicyMC(adapter, gamma=0.99, epsilon=0.4, name=f\"OffPolicy-{env_name}\")\n",
        "            result_off = off_policy_mc.train(num_episodes=num_episodes)\n",
        "            env_results['Off-Policy MC'] = result_off\n",
        "            print(f\"‚úÖ Off-Policy MC termin√© - Taux de succ√®s: {result_off['success_rate']:.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur Off-Policy MC sur {env_name}: {e}\")\n",
        "            env_results['Off-Policy MC'] = {'history': [], 'success_rate': 0}\n",
        "        \n",
        "        # Stocker les r√©sultats de cet environnement\n",
        "        all_results[env_name] = env_results\n",
        "        \n",
        "        # Afficher un r√©sum√© pour cet environnement\n",
        "        print(f\"\\\\nüìä R√âSUM√â {env_name}:\")\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-50:]]\n",
        "                avg_final_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                print(f\"   ‚Ä¢ {alg_name}: R√©compense finale = {avg_final_reward:.3f}\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {alg_name}: ‚ùå Aucun r√©sultat\")\n",
        "    \n",
        "    print(\"\\\\nüéâ ANALYSE COMPL√àTE TERMIN√âE !\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Lancer l'analyse compl√®te (peut prendre plusieurs minutes)\n",
        "print(\"‚è≥ Lancement de l'analyse compl√®te...\")\n",
        "print(\"Cela peut prendre plusieurs minutes selon la complexit√© des environnements...\")\n",
        "\n",
        "# Utiliser un nombre d'√©pisodes raisonnable pour le test\n",
        "EPISODES = 800  # Ajustez selon vos besoins de temps\n",
        "\n",
        "all_results = run_complete_analysis(num_episodes=EPISODES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà Affichage des R√©sultats et Analyses Compl√®tes\n",
        "\n",
        "print(\"üìà G√âN√âRATION DES ANALYSES VISUELLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# V√©rifier qu'on a des r√©sultats\n",
        "if all_results and any(env_results for env_results in all_results.values()):\n",
        "    \n",
        "    # 1. Afficher les courbes d'apprentissage pour chaque environnement\n",
        "    print(\"\\\\nüéØ 1. COURBES D'APPRENTISSAGE PAR ENVIRONNEMENT\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        if any(result['history'] for result in env_results.values()):\n",
        "            print(f\"\\\\nüìä Graphiques pour {env_name}...\")\n",
        "            plot_learning_curves(env_results, title_prefix=env_name)\n",
        "        else:\n",
        "            print(f\"‚ùå Pas de donn√©es valides pour {env_name}\")\n",
        "    \n",
        "    # 2. Comparaison des performances entre environnements\n",
        "    print(\"\\\\nüéØ 2. COMPARAISON GLOBALE DES PERFORMANCES\")\n",
        "    print(\"-\" * 50)\n",
        "    plot_performance_comparison(all_results)\n",
        "    \n",
        "    # 3. Analyse d√©taill√©e des caract√©ristiques\n",
        "    print(\"\\\\nüéØ 3. ANALYSE D√âTAILL√âE\")\n",
        "    print(\"-\" * 50)\n",
        "    analyze_algorithm_characteristics(all_results)\n",
        "    \n",
        "    # 4. Tableau r√©capitulatif final\n",
        "    print(\"\\\\nüéØ 4. TABLEAU R√âCAPITULATIF FINAL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Cr√©er un DataFrame pour le r√©sum√©\n",
        "    summary_data = []\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                # Calculer les m√©triques finales\n",
        "                history = result['history']\n",
        "                final_rewards = [h['reward'] for h in history[-100:]] if len(history) >= 100 else [h['reward'] for h in history]\n",
        "                \n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succ√®s': f\"{result['success_rate']:.1%}\",\n",
        "                    'R√©compense_Finale': f\"{np.mean(final_rewards):.3f}\",\n",
        "                    'Stabilit√©': f\"{np.std(final_rewards):.3f}\",\n",
        "                    '√âpisodes_Total': len(history),\n",
        "                    'Q_Moyenne_Finale': f\"{history[-1]['avg_q']:.3f}\" if history else \"0.000\"\n",
        "                })\n",
        "            else:\n",
        "                summary_data.append({\n",
        "                    'Environnement': env_name,\n",
        "                    'Algorithme': alg_name,\n",
        "                    'Taux_Succ√®s': \"0.0%\",\n",
        "                    'R√©compense_Finale': \"0.000\",\n",
        "                    'Stabilit√©': \"N/A\",\n",
        "                    '√âpisodes_Total': 0,\n",
        "                    'Q_Moyenne_Finale': \"0.000\"\n",
        "                })\n",
        "    \n",
        "    # Afficher le tableau\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\\\nüìã R√©sultats par Algorithme et Environnement:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    # 5. Recommandations finales\n",
        "    print(\"\\\\nüéØ 5. RECOMMANDATIONS ET CONCLUSIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Trouver les meilleurs algorithmes par environnement\n",
        "    best_performers = {}\n",
        "    for env_name, env_results in all_results.items():\n",
        "        best_alg = None\n",
        "        best_score = -float('inf')\n",
        "        \n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history']:\n",
        "                # Score composite bas√© sur r√©compense finale et taux de succ√®s\n",
        "                final_rewards = [h['reward'] for h in result['history'][-100:]] if len(result['history']) >= 100 else [h['reward'] for h in result['history']]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                \n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                \n",
        "                if composite_score > best_score:\n",
        "                    best_score = composite_score\n",
        "                    best_alg = alg_name\n",
        "        \n",
        "        best_performers[env_name] = (best_alg, best_score)\n",
        "    \n",
        "    print(\"\\\\nüèÜ MEILLEURS ALGORITHMES PAR ENVIRONNEMENT:\")\n",
        "    for env_name, (best_alg, score) in best_performers.items():\n",
        "        if best_alg:\n",
        "            print(f\"   ‚Ä¢ {env_name}: {best_alg} (Score: {score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {env_name}: Aucun algorithme efficace\")\n",
        "    \n",
        "    # Analyse globale\n",
        "    alg_global_scores = {'MC-ES': [], 'On-Policy MC': [], 'Off-Policy MC': []}\n",
        "    \n",
        "    for env_name, env_results in all_results.items():\n",
        "        for alg_name, result in env_results.items():\n",
        "            if result['history'] and alg_name in alg_global_scores:\n",
        "                final_rewards = [h['reward'] for h in result['history'][-100:]] if len(result['history']) >= 100 else [h['reward'] for h in result['history']]\n",
        "                avg_reward = np.mean(final_rewards) if final_rewards else 0\n",
        "                success_rate = result['success_rate']\n",
        "                composite_score = avg_reward * 0.7 + success_rate * 0.3\n",
        "                alg_global_scores[alg_name].append(composite_score)\n",
        "    \n",
        "    print(\"\\\\nüåü PERFORMANCE GLOBALE DES ALGORITHMES:\")\n",
        "    for alg_name, scores in alg_global_scores.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            print(f\"   ‚Ä¢ {alg_name}: {avg_score:.3f} (¬±{std_score:.3f})\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {alg_name}: Aucune donn√©e valide\")\n",
        "    \n",
        "    # Recommandations sp√©cifiques\n",
        "    print(\"\\\\nüí° RECOMMANDATIONS:\")\n",
        "    print(\"   1. üéØ Chaque environnement secret semble avoir des caract√©ristiques uniques\")\n",
        "    print(\"   2. üîÑ L'exploration est cruciale - MC-ES peut √™tre avantag√©\")\n",
        "    print(\"   3. üìä Surveillez les taux de succ√®s autant que les r√©compenses\")\n",
        "    print(\"   4. ‚öñÔ∏è  L'importance sampling (Off-Policy) peut √™tre instable sur certains environnements\")\n",
        "    print(\"   5. üéõÔ∏è  L'ajustement des hyperparam√®tres (Œµ, Œ≥) est critique\")\n",
        "    \n",
        "    # Sauvegarde des r√©sultats\n",
        "    try:\n",
        "        summary_df.to_csv('secret_env_monte_carlo_results.csv', index=False)\n",
        "        print(f\"\\\\nüíæ R√©sultats sauvegard√©s dans 'secret_env_monte_carlo_results.csv'\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de la sauvegarde: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå AUCUN R√âSULTAT VALIDE TROUV√â\")\n",
        "    print(\"V√©rifiez que les environnements secrets sont accessibles et fonctionnels.\")\n",
        "\n",
        "print(\"\\\\nüéâ ANALYSE MONTE CARLO TERMIN√âE !\")\n",
        "print(\"üïµÔ∏è Les myst√®res des environnements secrets ont √©t√© explor√©s par Monte Carlo !\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# üìñ Guide d'Utilisation du Notebook\n",
        "\n",
        "## üöÄ Ex√©cution\n",
        "1. **Ex√©cutez les cellules dans l'ordre** (Shift+Enter pour chaque cellule)\n",
        "2. La **cellule 1** teste les imports et environnements\n",
        "3. Les **cellules 2-4** d√©finissent les classes et fonctions\n",
        "4. La **cellule 5** lance l'entra√Ænement complet (‚è≥ 5-15 minutes selon la machine)\n",
        "5. La **cellule 6** affiche tous les r√©sultats et graphiques\n",
        "\n",
        "## üéõÔ∏è Param√®tres Ajustables\n",
        "```python\n",
        "# Dans la cellule 5, modifiez cette ligne :\n",
        "EPISODES = 800  # Nombre d'√©pisodes par algorithme\n",
        "\n",
        "# Param√®tres des algorithmes (dans leurs constructeurs) :\n",
        "# - gamma: Facteur de discount (0.99 par d√©faut)\n",
        "# - epsilon: Taux d'exploration (0.3-0.4 par d√©faut)\n",
        "```\n",
        "\n",
        "## üìä Outputs G√©n√©r√©s\n",
        "- **Courbes d'apprentissage** : R√©compenses, Q-values, longueur √©pisodes, stabilit√©\n",
        "- **Comparaisons globales** : Heatmaps, barres comparatives, scores composites\n",
        "- **Analyses d√©taill√©es** : M√©triques par algorithme et environnement\n",
        "- **Tableau r√©capitulatif** : Summary CSV exportable\n",
        "- **Recommandations** : Meilleurs algorithmes par environnement\n",
        "\n",
        "## üêõ D√©pannage\n",
        "- **Erreur d'import** : V√©rifiez que les libs natives sont pr√©sentes dans `./libs/`\n",
        "- **Erreur de plateforme** : Le wrapper d√©tecte automatiquement Windows/Linux/macOS\n",
        "- **Performance lente** : R√©duisez `EPISODES` pour des tests rapides\n",
        "- **Manque de m√©moire** : Fermez d'autres applications\n",
        "\n",
        "## üìà Interpr√©tation des R√©sultats\n",
        "- **Taux de succ√®s √©lev√©** : L'algorithme g√©n√®re des √©pisodes valides\n",
        "- **R√©compenses croissantes** : L'apprentissage fonctionne\n",
        "- **Q-values qui convergent** : La politique se stabilise\n",
        "- **Faible stabilit√©** : Plus d'exploration n√©cessaire\n",
        "\n",
        "## üîß Modifications Avanc√©es\n",
        "Pour adapter √† d'autres environnements secrets :\n",
        "1. Ajoutez la classe d'environnement dans `env_classes`\n",
        "2. Cr√©ez l'adaptateur correspondant\n",
        "3. Les algorithmes s'adaptent automatiquement\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Bon apprentissage avec Monte Carlo sur les environnements secrets !**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
