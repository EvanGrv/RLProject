# ğŸ•µï¸ Analyse Monte Carlo sur les Environnements Secrets

Ce dossier contient une analyse complÃ¨te des algorithmes Monte Carlo appliquÃ©s aux environnements secrets fournis via des bibliothÃ¨ques natives compilÃ©es.

## ğŸ“ Structure du Dossier

```
game/secret_env/
â”œâ”€â”€ monte_carlo_analysis.ipynb    # Notebook principal d'analyse
â”œâ”€â”€ secret_envs_wrapper.py        # Wrapper Python pour les environnements secrets
â”œâ”€â”€ README.md                     # Ce fichier
â””â”€â”€ libs/                         # BibliothÃ¨ques natives compilÃ©es
    â”œâ”€â”€ secret_envs.dll          # Windows
    â”œâ”€â”€ libsecret_envs.so        # Linux  
    â”œâ”€â”€ libsecret_envs.dylib     # macOS ARM
    â””â”€â”€ libsecret_envs_intel_macos.dylib  # macOS Intel
```

## ğŸ¯ Objectifs de l'Analyse

Le notebook `monte_carlo_analysis.ipynb` implÃ©mente et compare **3 algorithmes Monte Carlo** sur **4 environnements secrets** :

### ğŸ“‹ Algorithmes TestÃ©s
1. **Monte Carlo Exploring Starts (MC-ES)**
   - Garantit l'exploration avec dÃ©parts alÃ©atoires
   - Converge vers la politique optimale sous certaines conditions

2. **On-Policy Monte Carlo**
   - Politique Îµ-greedy avec dÃ©croissance d'epsilon
   - Apprend la politique qu'il suit

3. **Off-Policy Monte Carlo**
   - Importance sampling pour sÃ©parer exploration et exploitation
   - Politique cible dÃ©terministe, politique de comportement Îµ-greedy

### ğŸŒŸ Environnements Secrets
- **SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3**
- Interface native via ctypes
- Actions disponibles dynamiques selon l'Ã©tat
- RÃ©compenses basÃ©es sur des scores diffÃ©rentiels

## ğŸš€ Utilisation

### PrÃ©requis
```bash
pip install numpy pandas matplotlib seaborn tqdm
```

### Lancement
1. Ouvrez `monte_carlo_analysis.ipynb` dans Jupyter Lab/Notebook
2. ExÃ©cutez les cellules dans l'ordre sÃ©quentiel
3. Ajustez le paramÃ¨tre `EPISODES` selon vos besoins de temps

### ParamÃ¨tres Configurables
```python
EPISODES = 800          # Nombre d'Ã©pisodes par algorithme
gamma = 0.99           # Facteur de discount
epsilon = 0.3-0.4      # Taux d'exploration initial
```

## ğŸ“Š Analyses Fournies

### 1. ğŸ“ˆ Courbes d'Apprentissage
- **RÃ©compenses par Ã©pisode** avec moyenne mobile
- **Ã‰volution des Q-values** moyennes
- **Longueur des Ã©pisodes** au fil du temps
- **Analyse de stabilitÃ©** (Ã©cart-type des rÃ©compenses)

### 2. ğŸ† Comparaisons Globales  
- **Heatmap des performances** par algorithme/environnement
- **Taux de succÃ¨s** comparatifs
- **Q-values finales** moyennes
- **Scores composites** globaux

### 3. ğŸ“‹ MÃ©triques DÃ©taillÃ©es
- Taux de succÃ¨s des Ã©pisodes
- RÃ©compenses moyennes et finales
- StabilitÃ© d'apprentissage
- CaractÃ©ristiques spÃ©cifiques (Îµ, importance sampling weights)

### 4. ğŸ¯ Recommandations
- Meilleur algorithme par environnement
- Performance globale des algorithmes
- Suggestions d'hyperparamÃ¨tres

## ğŸ”§ Architecture Technique

### Adaptateur d'Environnement
```python
class SecretEnvAdapter:
    - Traduit l'API native vers l'API Gym standard
    - GÃ¨re les actions disponibles dynamiques  
    - Calcule les rÃ©compenses diffÃ©rentielles
    - Limite les Ã©pisodes infinis
```

### Algorithmes Monte Carlo
```python
class SecretMonteCarloES:        # Exploring Starts
class SecretOnPolicyMC:          # Îµ-greedy On-Policy  
class SecretOffPolicyMC:         # Importance Sampling
```

### Fonctions de Visualisation
```python
plot_learning_curves()          # Courbes d'apprentissage
plot_performance_comparison()   # Comparaisons globales
analyze_algorithm_characteristics()  # Analyses dÃ©taillÃ©es
```

## ğŸ“ˆ Outputs GÃ©nÃ©rÃ©s

### Fichiers de Sortie
- `secret_env_monte_carlo_results.csv` - Tableau rÃ©capitulatif exportable

### Visualisations
- **16 graphiques** de courbes d'apprentissage (4 par environnement)
- **4 graphiques** de comparaison globale
- **Analyses textuelles** dÃ©taillÃ©es
- **Recommandations** personnalisÃ©es

## ğŸ› RÃ©solution de ProblÃ¨mes

### Erreurs Communes
```bash
# Erreur d'import des environnements
âŒ ImportError: DLL load failed
âœ… VÃ©rifiez que les libs natives correspondent Ã  votre OS

# Performance lente  
âŒ EntraÃ®nement trop long
âœ… RÃ©duisez EPISODES Ã  200-400 pour des tests rapides

# Actions invalides
âŒ Action not in available_actions
âœ… L'adaptateur gÃ¨re automatiquement ce cas
```

### DÃ©pendances SystÃ¨me
- **Windows** : `secret_envs.dll` 
- **Linux** : `libsecret_envs.so`
- **macOS** : `libsecret_envs.dylib` (ARM) ou `libsecret_envs_intel_macos.dylib` (Intel)

## ğŸ›ï¸ Personnalisation

### Ajouter un Nouvel Environnement Secret
```python
# Dans la cellule 5, ajoutez :
env_classes = {
    'SecretEnv0': SecretEnv0,
    'SecretEnv1': SecretEnv1, 
    'SecretEnv2': SecretEnv2,
    'SecretEnv3': SecretEnv3,
    'SecretEnv4': SecretEnv4  # <- Nouveau
}
```

### Modifier les HyperparamÃ¨tres
```python
# Algorithmes plus exploratoires
SecretOnPolicyMC(adapter, epsilon=0.5, gamma=0.95)

# Plus d'Ã©pisodes pour convergence
run_complete_analysis(num_episodes=1500)
```

## ğŸ“š RÃ©fÃ©rences ThÃ©oriques

Les algorithmes implÃ©mentÃ©s suivent les spÃ©cifications de :
- Sutton & Barto - *Reinforcement Learning: An Introduction*
- Chapitres 5.1-5.7 sur les mÃ©thodes Monte Carlo
- Importance Sampling et Off-Policy Learning

## ğŸ Temps d'ExÃ©cution EstimÃ©

| Configuration | Temps EstimÃ© | Utilisation |
|---------------|-------------|-------------|
| 200 Ã©pisodes  | 2-5 minutes | Tests rapides |
| 800 Ã©pisodes  | 5-15 minutes | Analyse complÃ¨te |
| 1500 Ã©pisodes | 15-30 minutes | Recherche approfondie |

---

**ğŸ‰ Explorez les mystÃ¨res des environnements secrets avec Monte Carlo !** 