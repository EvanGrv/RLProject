"""
Module de Temporal Difference pour le Reinforcement Learning.

Ce module impl√©mente les algorithmes de diff√©rence temporelle :
- Sarsa (On-policy TD control)
- Q-Learning (Off-policy TD control)
- Expected Sarsa (Variation de Sarsa avec expectation)

Ces algorithmes apprennent √† partir d'exp√©riences individuelles
sans attendre la fin d'un √©pisode.
"""

import numpy as np
from typing import Dict, Tuple, Optional, Any
from collections import defaultdict
from src.utils_io import save_model, load_model


import numpy as np
import random
from collections import defaultdict
from typing import Any, List, Tuple, Dict

# (Supposons que save_model et load_model sont d√©finis dans vos utilitaires)
# from utils import save_model, load_model

class Sarsa:
    """
    SARSA - On-policy TD Control.

    Met √† jour Q(s,a) en utilisant l'action r√©ellement prise par la politique Œµ-greedy.
    """

    def __init__(
        self,
        env: Any,
        alpha: float = 0.1,
        gamma: float = 0.99,
        epsilon: float = 1.0,
    ):
        """
        Args:
            env: Environnement Gym-like (avec env.reset(), env.step())
            alpha: taux d'apprentissage ‚àà (0,1]
            gamma: facteur d'actualisation ‚àà [0,1]
            epsilon: param√®tre initial Œµ pour la politique Œµ-greedy
        """
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.initial_epsilon = epsilon

        # Structures MDP
        self._init_mdp_structures()

        # Pour suivre r√©compenses, pertes, Œµ
        self.history: List[Dict[str, float]] = []

    def _init_mdp_structures(self):
        """D√©tecte nS, nA et initialise Q et policy."""
        if hasattr(self.env, 'nS') and hasattr(self.env, 'nA'):
            self.nS, self.nA = self.env.nS, self.env.nA
        else:
            # Gym standard
            self.nS = getattr(self.env.observation_space, 'n', None)
            self.nA = getattr(self.env.action_space,    'n', None)
            if self.nS is None or self.nA is None:
                raise ValueError("Impossible de d√©terminer nS ou nA.")
        # Q(s,a) initialis√© √† z√©ro
        self.Q = np.zeros((self.nS, self.nA), dtype=float)
        # Politique gloutonne par d√©faut (sera extraite apr√®s training)
        self.policy = np.zeros(self.nS, dtype=int)

    def epsilon_greedy(self, state: int) -> int:
        """
        S√©lectionne une action selon une strat√©gie Œµ-greedy sur Q[state]:
        - avec prob. Œµ: action al√©atoire
        - sinon: argmax_a Q(state,a) (tie-breaking al√©atoire)
        """
        if random.random() < self.epsilon:
            return random.randrange(self.nA)
        q_s = self.Q[state]
        max_q = np.max(q_s)
        best_actions = np.flatnonzero(q_s == max_q)
        return int(random.choice(best_actions))

    def train_episode(self) -> Dict[str, float]:
        """
        Ex√©cute un √©pisode complet selon SARSA:
        1) Initialise s ‚Üê env.reset()
        2) Choisit a ‚Üê Œµ-greedy(s)
        3) Pour chaque pas t:
             observe r, s' ‚Üê env.step(a)
             choisit a' ‚Üê Œµ-greedy(s')
             met √† jour Q(s,a):
               Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ Q(s',a') ‚àí Q(s,a) ]
             (s,a) ‚Üê (s',a')
        4) Termine quand s' est terminal
        """
        # 1) √©tat initial
        obs = self.env.reset()
        state = obs if not hasattr(self.env, 'state') else self.env.state

        # 2) action initiale
        action = self.epsilon_greedy(state)

        total_reward = 0.0
        total_td_error2 = 0.0
        steps = 0

        # boucle jusqu'√† √©tat terminal ou plafond d'√©tapes
        done = False
        while not done and steps < 1000:
            # prendre l'action et observer
            next_obs, reward, done, _ = self.env.step(action)
            next_state = next_obs if not hasattr(self.env, 'state') else self.env.state
            total_reward += reward

            # choisir action suivante selon policy Œµ-greedy
            next_action = self.epsilon_greedy(next_state)

            # 3) mise √† jour SARSA
            # Formule TD-target:    r + Œ≥ Q(s',a')
            # TD-error Œ¥ = target ‚àí Q(s,a)
            td_target = reward + self.gamma * self.Q[next_state, next_action]
            td_error  = td_target - self.Q[state, action]
            self.Q[state, action] += self.alpha * td_error

            total_td_error2 += td_error**2

            # passer au prochain pas
            state, action = next_state, next_action
            steps += 1

        # moyenne quadratique des TD-errors
        avg_loss = total_td_error2 / steps if steps>0 else 0.0

        return {
            'reward': total_reward,
            'loss':   avg_loss,
            'steps':  steps
        }

    def train(self, num_episodes: int = 1000) -> Dict[str, Any]:
        """
        Entra√Ænement complet SARSA sur num_episodes √©pisodes.
        D√©cro√Æt Œµ lin√©airement jusqu'√† Œµ_min=0.01.
        """
        self.history.clear()
        epsilon_decay = (self.initial_epsilon - 0.01) / num_episodes

        for ep in range(1, num_episodes+1):
            stats = self.train_episode()

            # d√©cay de Œµ
            self.epsilon = max(0.01, self.epsilon - epsilon_decay)

            # suivre les m√©triques
            self.history.append({
                'episode':   ep,
                'reward':    stats['reward'],
                'loss':      stats['loss'],
                'steps':     stats['steps'],
                'epsilon':   self.epsilon
            })
            # log p√©riodique
            if ep % max(1, num_episodes//10) == 0:
                print(f"[SARSA] Ep {ep}/{num_episodes} ‚îÇ R={stats['reward']:.2f} ‚îÇ loss={stats['loss']:.4f} ‚îÇ Œµ={self.epsilon:.3f}")

        # extraire politique gloutonne finale
        for s in range(self.nS):
            self.policy[s] = int(np.argmax(self.Q[s]))

        return {
            'Q':       self.Q,
            'policy':  self.policy,
            'history': self.history
        }

    def evaluate(self, num_episodes: int = 100) -> Dict[str, float]:
        """
        √âvalue la politique gloutonne (Œµ=0) pendant num_episodes √©pisodes.
        """
        old_eps = self.epsilon
        self.epsilon = 0.0

        rewards, lengths, success = [], [], 0
        for _ in range(num_episodes):
            obs = self.env.reset()
            state = obs if not hasattr(self.env, 'state') else self.env.state
            done, G, steps = False, 0.0, 0

            while not done and steps<1000:
                a = self.policy[state]
                next_obs, r, done, _ = self.env.step(a)
                state = next_obs if not hasattr(self.env, 'state') else self.env.state
                G += r; steps += 1
            rewards.append(G); lengths.append(steps)
            if G>0: success += 1

        self.epsilon = old_eps
        return {
            'avg_reward':    np.mean(rewards),
            'std_reward':    np.std(rewards),
            'success_rate':  success/num_episodes,
            'avg_steps':     np.mean(lengths)
        }

    def save(self, filepath: str):
        save_model({
            'Q':      self.Q,
            'policy': self.policy,
            'alpha':  self.alpha,
            'gamma':  self.gamma,
            'epsilon': self.epsilon,
            'history': self.history
        }, filepath)

    def load(self, filepath: str):
        data = load_model(filepath)
        self.Q       = data['Q']
        self.policy  = data['policy']
        self.alpha   = data['alpha']
        self.gamma   = data['gamma']
        self.epsilon = data['epsilon']
        self.history = data['history']


class QLearning:
    """
    Q-Learning (Off-policy TD control).

    Utilise la r√®gle de mise √† jour :
      Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ max_{a'} Q(s',a') ‚àí Q(s,a) ]
    avec une politique Œµ-greedy pour explorer.
    """

    def __init__(
        self,
        env: Any,
        alpha: float = 0.1,
        gamma: float = 0.99,
        epsilon: float = 1.0,
    ):
        """
        Args:
            env: Environnement Gym-like
            alpha: pas d'apprentissage ‚àà (0,1]
            gamma: facteur d'actualisation ‚àà [0,1]
            epsilon: Œµ initial pour Œµ-greedy
        """
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.initial_epsilon = epsilon

        # D√©tection du nombre d'√©tats et d'actions
        if hasattr(env, 'nS') and hasattr(env, 'nA'):
            self.nS, self.nA = env.nS, env.nA
        else:
            self.nS = env.observation_space.n
            self.nA = env.action_space.n

        # Table Q(s,a) initialis√©e √† z√©ro
        self.Q = np.zeros((self.nS, self.nA), dtype=float)
        # Politique gloutonne construite apr√®s entra√Ænement
        self.policy = np.zeros(self.nS, dtype=int)

        # Historique pour suivi des m√©triques
        self.history: List[Dict[str, float]] = []

    def epsilon_greedy(self, state: int) -> int:
        """S√©lectionne une action selon Œµ-greedy sur Q[state]."""
        if random.random() < self.epsilon:
            return random.randrange(self.nA)
        q_s = self.Q[state]
        max_q = np.max(q_s)
        best_actions = np.flatnonzero(q_s == max_q)
        return int(random.choice(best_actions))

    def train_episode(self) -> Dict[str, float]:
        """
        Ex√©cute un √©pisode complet selon Q-Learning:
        1) s ‚Üê env.reset()
        2) Tant que non terminal:
             a ‚Üê Œµ-greedy(s)
             r, s' ‚Üê env.step(a)
             Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ max_a' Q(s',a') ‚àí Q(s,a) ]
             s ‚Üê s'
        """
        state = self.env.reset()
        total_reward = 0.0
        total_td2 = 0.0
        steps = 0
        done = False

        while not done and steps < 1000:
            action = self.epsilon_greedy(state)
            next_state, reward, done, _ = self.env.step(action)

            # TD-target et TD-error
            best_next = np.max(self.Q[next_state])
            td_target = reward + self.gamma * best_next
            td_error  = td_target - self.Q[state, action]

            # Mise √† jour Q-Learning
            self.Q[state, action] += self.alpha * td_error

            total_reward += reward
            total_td2 += td_error ** 2
            state = next_state
            steps += 1

        avg_loss = total_td2 / steps if steps > 0 else 0.0
        return {'reward': total_reward, 'loss': avg_loss, 'steps': steps}

    def train(self, num_episodes: int = 1000) -> Dict[str, Any]:
        """
        Entra√Æne sur num_episodes:
        - appel de train_episode()
        - d√©croissance lin√©aire de Œµ vers 0.01
        - extraction de la politique gloutonne finale
        """
        self.history.clear()
        decay = (self.initial_epsilon - 0.01) / num_episodes

        for ep in range(1, num_episodes + 1):
            stats = self.train_episode()

            # D√©croissance d'epsilon
            self.epsilon = max(0.01, self.epsilon - decay)

            self.history.append({
                'episode': ep,
                'reward':  stats['reward'],
                'loss':    stats['loss'],
                'steps':   stats['steps'],
                'epsilon': self.epsilon
            })
            if ep % max(1, num_episodes // 10) == 0:
                print(f"[Q-Learning] Ep {ep}/{num_episodes}  "
                      f"R={stats['reward']:.2f}  loss={stats['loss']:.4f}  Œµ={self.epsilon:.3f}")

        # Construire la politique gloutonne √† partir de Q
        for s in range(self.nS):
            self.policy[s] = int(np.argmax(self.Q[s]))

        return {'Q': self.Q, 'policy': self.policy, 'history': self.history}

    def evaluate(self, num_episodes: int = 100) -> Dict[str, float]:
        """
        √âvalue la policy gloutonne (Œµ=0) sur num_episodes.
        """
        old_eps = self.epsilon
        self.epsilon = 0.0

        rewards, lengths = [], []
        for _ in range(num_episodes):
            state = self.env.reset()
            done, G, steps = False, 0.0, 0
            while not done and steps < 1000:
                a = self.policy[state]
                state, r, done, _ = self.env.step(a)
                G += r; steps += 1
            rewards.append(G); lengths.append(steps)

        self.epsilon = old_eps
        return {
            'avg_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'avg_steps':  np.mean(lengths)
        }

    def save(self, filepath: str):
        save_model({
            'Q':       self.Q,
            'policy':  self.policy,
            'alpha':   self.alpha,
            'gamma':   self.gamma,
            'epsilon': self.epsilon,
            'history': self.history
        }, filepath)

    def load(self, filepath: str):
        data = load_model(filepath)
        self.Q       = data['Q']
        self.policy  = data['policy']
        self.alpha   = data['alpha']
        self.gamma   = data['gamma']
        self.epsilon = data['epsilon']
        self.history = data['history']


class ExpectedSarsa:
    """
    Expected SARSA ‚Äì On-policy TD Control using the expectation under Œµ-greedy.

    Met √† jour Q(s,a) selon :
      Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ ùîº_{a'‚àºœÄ}[Q(s',a')] ‚àí Q(s,a) ]
    """

    def __init__(
        self,
        env: Any,
        alpha: float = 0.1,
        gamma: float = 0.99,
        epsilon: float = 1.0,
    ):
        """
        Args:
            env: Environnement Gym-like
            alpha: pas d'apprentissage ‚àà (0,1]
            gamma: facteur d'actualisation ‚àà [0,1]
            epsilon: Œµ initial pour Œµ-greedy
        """
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.initial_epsilon = epsilon

        # Nombre d'√©tats/actions
        if hasattr(env, 'nS') and hasattr(env, 'nA'):
            self.nS, self.nA = env.nS, env.nA
        else:
            self.nS = env.observation_space.n
            self.nA = env.action_space.n

        # Table Q(s,a)
        self.Q = np.zeros((self.nS, self.nA), dtype=float)
        # Politique gloutonne extraite apr√®s training
        self.policy = np.zeros(self.nS, dtype=int)

        # Historique des m√©triques
        self.history: List[Dict[str, float]] = []

    def epsilon_greedy(self, state: int) -> int:
        """Œµ-greedy avec tie-breaking al√©atoire."""
        if random.random() < self.epsilon:
            return random.randrange(self.nA)
        q_s = self.Q[state]
        best = np.flatnonzero(q_s == q_s.max())
        return int(random.choice(best))

    def expected_q(self, state: int) -> float:
        """
        Calcule l'esp√©rance E_{a‚àºœÄ}[Q(state,a)] sous Œµ-greedy :
        œÄ(a|s) = Œµ/|A|  sauf pour a* = argmax Q(s,a) o√π œÄ(a*|s)=1‚àíŒµ+Œµ/|A|.
        """
        q_s = self.Q[state]
        greedy = int(np.argmax(q_s))
        base = self.epsilon / self.nA
        exp_q = base * q_s.sum() + (1.0 - self.epsilon) * q_s[greedy]
        return float(exp_q)

    def train_episode(self) -> Dict[str, float]:
        """
        Un √©pisode Expected SARSA :
        s ‚Üê env.reset()
        tant que s non terminal :
          a ‚Üê Œµ-greedy(s)
          r, s' ‚Üê env.step(a)
          G = expected_q(s')
          td = r + Œ≥ G ‚àí Q(s,a)
          Q(s,a) += Œ± td
          s ‚Üê s'
        """
        state = self.env.reset()
        total_reward = 0.0
        total_td2 = 0.0
        steps = 0
        done = False

        while not done and steps < 1000:
            action = self.epsilon_greedy(state)
            next_state, reward, done, _ = self.env.step(action)

            # calcul de l'esp√©rance
            exp_q_next = 0.0 if done else self.expected_q(next_state)
            td_target = reward + self.gamma * exp_q_next
            td_error  = td_target - self.Q[state, action]
            self.Q[state, action] += self.alpha * td_error

            total_reward += reward
            total_td2 += td_error ** 2
            state = next_state
            steps += 1

        avg_loss = total_td2 / steps if steps > 0 else 0.0
        return {'reward': total_reward, 'loss': avg_loss, 'steps': steps}

    def train(self, num_episodes: int = 1000) -> Dict[str, List[float]]:
        """
        Entra√Æne Expected SARSA sur num_episodes :
        - appel r√©p√©titif de train_episode()
        - d√©croissance lin√©aire de Œµ vers 0.01
        - extraction de la politique gloutonne finale
        """
        self.history.clear()
        decay = (self.initial_epsilon - 0.01) / num_episodes

        for ep in range(1, num_episodes + 1):
            stats = self.train_episode()
            self.epsilon = max(0.01, self.epsilon - decay)

            self.history.append({
                'episode':  ep,
                'reward':   stats['reward'],
                'loss':     stats['loss'],
                'steps':    stats['steps'],
                'epsilon':  self.epsilon
            })
            if ep % max(1, num_episodes//10) == 0:
                print(f"[ExpSARSA] Ep{ep}/{num_episodes}  R={stats['reward']:.2f}  loss={stats['loss']:.4f}  Œµ={self.epsilon:.3f}")

        # Politique gloutonne finale
        for s in range(self.nS):
            self.policy[s] = int(np.argmax(self.Q[s]))
        return {'Q': self.Q, 'policy': self.policy, 'history': self.history}

    def evaluate(self, num_episodes: int = 100) -> Dict[str, float]:
        """√âvalue la politique gloutonne (Œµ=0)."""
        old_eps = self.epsilon
        self.epsilon = 0.0

        rewards, lengths = [], []
        for _ in range(num_episodes):
            state = self.env.reset()
            done, G, steps = False, 0.0, 0
            while not done and steps < 1000:
                a = self.policy[state]
                state, r, done, _ = self.env.step(a)
                G += r; steps += 1
            rewards.append(G); lengths.append(steps)

        self.epsilon = old_eps
        return {
            'avg_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'avg_steps':  np.mean(lengths)
        }

    def save(self, filepath: str):
        save_model({
            'Q':       self.Q,
            'policy':  self.policy,
            'alpha':   self.alpha,
            'gamma':   self.gamma,
            'epsilon': self.epsilon,
            'history': self.history
        }, filepath)

    def load(self, filepath: str):
        data = load_model(filepath)
        self.Q       = data['Q']
        self.policy  = data['policy']
        self.alpha   = data['alpha']
        self.gamma   = data['gamma']
        self.epsilon = data['epsilon']
        self.history = data['history']