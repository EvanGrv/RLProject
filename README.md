# Projet de Reinforcement Learning - 2√®me Ann√©e

## Description

Ce projet impl√©mente et compare diff√©rents algorithmes de reinforcement learning sur des environnements de jeux personnalis√©s. Il permet d'analyser automatiquement les performances de chaque algorithme et de d√©terminer le meilleur mod√®le pour chaque environnement.

## Structure du Projet

```
‚îú‚îÄ‚îÄ game/                    # Environnements de jeux
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Module d'initialisation
‚îÇ   ‚îî‚îÄ‚îÄ environments.py     # Tous les environnements (LineWorld, GridWorld, MontyHall 1&2)
‚îú‚îÄ‚îÄ src/                    # Code source des algorithmes
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/         # Impl√©mentations des algorithmes RL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dp.py          # Dynamic Programming (Policy/Value Iteration)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monte_carlo.py # Monte Carlo Methods
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ td.py          # Temporal Difference (Sarsa, Q-Learning, Expected Sarsa)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dyna.py        # Planning (Dyna-Q, Dyna-Q+)
‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Utilitaires
‚îú‚îÄ‚îÄ notebooks/             # Analyse des performances
‚îÇ   ‚îî‚îÄ‚îÄ analysis.ipynb     # Notebook d'analyse comparative automatique
‚îú‚îÄ‚îÄ models/                # Mod√®les sauvegard√©s
‚îî‚îÄ‚îÄ requirements.txt       # D√©pendances
```

## Algorithmes Impl√©ment√©s

### Dynamic Programming
- **Policy Iteration** : Am√©lioration it√©rative de la politique
- **Value Iteration** : Calcul direct de la fonction valeur optimale

### Monte Carlo Methods
- **Monte Carlo ES** : Exploring Starts
- **On-Policy Monte Carlo** : Apprentissage on-policy
- **Off-Policy Monte Carlo** : Apprentissage off-policy

### Temporal Difference
- **Sarsa** : State-Action-Reward-State-Action (on-policy)
- **Q-Learning** : Off-policy TD control
- **Expected Sarsa** : Utilise l'esp√©rance des valeurs Q

### Planning
- **Dyna-Q** : Combine apprentissage direct et planification
- **Dyna-Q+** : Dyna-Q avec bonus d'exploration

## Environnements de Jeux

### LineWorld
- **Description** : Environnement lin√©aire avec navigation gauche/droite
- **Objectif** : Atteindre une extr√©mit√© de la ligne
- **Complexit√©** : Simple, id√©al pour tester les algorithmes de base

### GridWorld
- **Description** : Grille 2D avec navigation dans 4 directions
- **Objectif** : Atteindre une des sorties (coins)
- **Complexit√©** : Mod√©r√©e, test de navigation spatiale

### MontyHall Paradox 1
- **Description** : Probl√®me de Monty Hall classique (3 portes)
- **Objectif** : Maximiser les chances de gagner
- **Complexit√©** : D√©cision s√©quentielle, strat√©gie optimale non-intuitive

### MontyHall Paradox 2
- **Description** : Probl√®me de Monty Hall √©tendu (5 portes)
- **Objectif** : Maximiser les chances de gagner
- **Complexit√©** : √âlev√©e, d√©cisions multiples

## Installation

1. **Cloner le repository**
   ```bash
   git clone [url-du-repo]
   cd [nom-du-repo]
   ```

2. **Installer les d√©pendances**
   ```bash
   pip install -r requirements.txt
   ```

3. **Lancer l'analyse**
   ```bash
   jupyter notebook notebooks/analysis.ipynb
   ```

## Utilisation

### Analyse Automatique des Performances

Le notebook `notebooks/analysis.ipynb` permet de :

1. **Comparer automatiquement** tous les algorithmes sur tous les environnements
2. **Visualiser les performances** avec des graphiques et heatmaps
3. **Obtenir des recommandations** pour chaque environnement
4. **Sauvegarder les r√©sultats** en format CSV

### Exemple d'utilisation des environnements

```python
from game.environments import LineWorld, GridWorld, MontyHallParadox1, MontyHallParadox2

# Cr√©er un environnement
env = LineWorld(length=8)

# R√©initialiser
state = env.reset()

# Prendre une action
next_state, reward, done, info = env.step(action=1)

# Visualiser
env.visualisation()
```

### Exemple d'utilisation des algorithmes

```python
from src.algorithms.td import QLearning
from game.environments import GridWorld

# Cr√©er l'environnement et l'algorithme
env = GridWorld(n_rows=5, n_cols=5)
algorithm = QLearning(env, alpha=0.1, gamma=0.99, epsilon=0.1)

# Entra√Æner
history = algorithm.train(num_episodes=1000)

# √âvaluer
results = algorithm.evaluate(num_episodes=100)
```

## M√©triques d'√âvaluation

Le syst√®me √©value les algorithmes selon plusieurs crit√®res :

- **R√©compense moyenne** : Performance g√©n√©rale
- **Taux de succ√®s** : Pourcentage d'√©pisodes r√©ussis
- **Temps d'entra√Ænement** : Efficacit√© computationnelle
- **Nombre d'√©tapes** : Efficacit√© de la politique apprise

## R√©sultats et Analyse

Le notebook d'analyse g√©n√®re automatiquement :

### Visualisations
- Courbes d'apprentissage par algorithme
- Heatmaps des performances
- Graphiques comparatifs

### Recommandations
- Meilleur algorithme par environnement
- Algorithme le plus polyvalent
- Algorithme le plus rapide
- Algorithme avec le meilleur taux de succ√®s

### Fichiers de sortie
- `results_comparison.csv` : Tableau comparatif d√©taill√©
- Graphiques sauvegard√©s
- M√©triques de performance

## Personnalisation

### Ajouter un nouvel algorithme
1. Cr√©er une nouvelle classe dans `src/algorithms/`
2. Impl√©menter les m√©thodes `train()` et `evaluate()`
3. Ajouter √† la liste des algorithmes dans le notebook d'analyse

### Ajouter un nouvel environnement
1. Cr√©er une nouvelle classe dans `game/environments.py`
2. H√©riter de `BaseEnvironment`
3. Impl√©menter les m√©thodes requises
4. Ajouter √† la liste des environnements dans le notebook d'analyse

## Contribution

Ce projet est d√©velopp√© dans le cadre d'un projet de 2√®me ann√©e en intelligence artificielle.

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

---

**üéØ Projet de Reinforcement Learning - Analyse Comparative Automatique**  
*Impl√©mentation et comparaison d'algorithmes RL sur environnements de jeux* 