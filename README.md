# Projet Reinforcement Learning - Algorithmes Classiques

Ce projet implÃ©mente les principaux algorithmes de reinforcement learning (RL) en Python, avec une structure modulaire et des outils d'analyse complets.

## ğŸ¯ Objectifs

ImplÃ©menter et comparer les algorithmes fondamentaux de RL :
- **Dynamic Programming** : Policy Iteration, Value Iteration
- **Monte Carlo** : ES, On-policy first-visit, Off-policy
- **Temporal Difference** : Sarsa, Q-Learning, Expected Sarsa
- **Planning** : Dyna-Q, Dyna-Q+

## ğŸ“ Structure du Projet

```
DL - Projet 2eme annÃ©e/
â”œâ”€â”€ src/                    # Code source principal
â”‚   â”œâ”€â”€ __init__.py        # Imports du package
â”‚   â”œâ”€â”€ dp.py              # Dynamic Programming
â”‚   â”œâ”€â”€ monte_carlo.py     # Algorithmes Monte Carlo
â”‚   â”œâ”€â”€ td.py              # Temporal Difference
â”‚   â”œâ”€â”€ dyna.py            # Planning (Dyna-Q, Dyna-Q+)
â”‚   â””â”€â”€ utils_io.py        # Utilitaires I/O (pickle, JSON, NPZ)
â”œâ”€â”€ models/                 # ModÃ¨les sauvegardÃ©s
â”‚   â””â”€â”€ .gitkeep           # Maintient le dossier dans Git
â”œâ”€â”€ tests/                  # Tests unitaires
â”‚   â”œâ”€â”€ test_dp.py         # Tests Dynamic Programming
â”‚   â”œâ”€â”€ test_monte_carlo.py # Tests Monte Carlo
â”‚   â”œâ”€â”€ test_td.py         # Tests Temporal Difference
â”‚   â”œâ”€â”€ test_dyna.py       # Tests Planning
â”‚   â””â”€â”€ test_utils_io.py   # Tests utilitaires I/O
â”œâ”€â”€ notebooks/              # Notebooks d'analyse
â”‚   â””â”€â”€ analysis.ipynb     # Analyse et visualisation
â”œâ”€â”€ .github/               # Configuration CI/CD
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml         # GitHub Actions
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â””â”€â”€ README.md             # Documentation (ce fichier)
```

## ğŸš€ Installation

### 1. Cloner le repository
```bash
git clone <votre-repo-url>
cd "DL - Projet 2eme annÃ©e"
```

### 2. CrÃ©er un environnement virtuel
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

## ğŸ® Utilisation

### Exemple d'utilisation basique

```python
import numpy as np
from src import QLearning, save_model, load_model

# CrÃ©er un environnement (exemple avec gym)
import gym
env = gym.make('FrozenLake-v1')

# Initialiser l'algorithme Q-Learning
q_learning = QLearning(
    env=env,
    alpha=0.1,      # Taux d'apprentissage
    gamma=0.99,     # Facteur de discount
    epsilon=0.1     # Exploration epsilon-greedy
)

# EntraÃ®ner le modÃ¨le
results = q_learning.train(num_episodes=1000)

# Sauvegarder le modÃ¨le
q_learning.save('models/q_learning_model.pkl')

# Charger un modÃ¨le existant
q_learning_loaded = QLearning(env)
q_learning_loaded.load('models/q_learning_model.pkl')
```

### Formats de sauvegarde supportÃ©s

```python
from src.utils_io import save_model, load_model

# DiffÃ©rents formats disponibles
save_model(data, 'models/model.pkl', format='pickle')    # Par dÃ©faut
save_model(data, 'models/model.json', format='json')     # JSON
save_model(data, 'models/model.joblib', format='joblib') # Joblib
save_model(data, 'models/model.npz', format='npz')       # NumPy NPZ

# Chargement automatique basÃ© sur l'extension
data = load_model('models/model.pkl', format='auto')
```

## ğŸ§ª Tests

ExÃ©cuter tous les tests :
```bash
pytest tests/
```

ExÃ©cuter un module spÃ©cifique :
```bash
pytest tests/test_dp.py -v
```

Tests avec couverture :
```bash
pytest --cov=src tests/
```

## ğŸ“Š Analyse des RÃ©sultats

Utilisez le notebook `notebooks/analysis.ipynb` pour :
- Charger et explorer les modÃ¨les sauvegardÃ©s
- Visualiser les courbes de convergence
- Comparer les performances des algorithmes
- Analyser les rÃ©sultats d'entraÃ®nement

```bash
jupyter notebook notebooks/analysis.ipynb
```

## ğŸ¤– Algorithmes ImplÃ©mentÃ©s

### Dynamic Programming (`src/dp.py`)
- **PolicyIteration** : ItÃ©ration de politique avec Ã©valuation complÃ¨te
- **ValueIteration** : ItÃ©ration de valeur avec mise Ã  jour directe

### Monte Carlo (`src/monte_carlo.py`)
- **MonteCarloES** : Exploring Starts avec tous les couples Ã©tat-action
- **OnPolicyFirstVisitMC** : On-policy first-visit avec epsilon-greedy
- **OffPolicyMC** : Off-policy avec importance sampling

### Temporal Difference (`src/td.py`)
- **Sarsa** : On-policy TD control
- **QLearning** : Off-policy TD control
- **ExpectedSarsa** : Sarsa avec valeur espÃ©rÃ©e

### Planning (`src/dyna.py`)
- **DynaQ** : Q-Learning avec planification intÃ©grÃ©e
- **DynaQPlus** : Dyna-Q avec bonus d'exploration

## âš™ï¸ ParamÃ¨tres Principaux

| ParamÃ¨tre | Description | Valeur par dÃ©faut |
|-----------|-------------|-------------------|
| `alpha` | Taux d'apprentissage | 0.1 |
| `gamma` | Facteur de discount | 0.9 |
| `epsilon` | Exploration epsilon-greedy | 0.1 |
| `theta` | Seuil de convergence (DP) | 1e-6 |
| `n_planning` | Ã‰tapes de planification (Dyna) | 5 |
| `kappa` | Bonus d'exploration (Dyna-Q+) | 0.001 |

## ğŸ“ˆ Exemple d'EntraÃ®nement Complet

```python
import gym
from src import QLearning, DynaQ, save_model
import matplotlib.pyplot as plt

# Environnement
env = gym.make('FrozenLake-v1')

# Algorithmes Ã  comparer
algorithms = {
    'Q-Learning': QLearning(env, alpha=0.1, gamma=0.99, epsilon=0.1),
    'Dyna-Q': DynaQ(env, alpha=0.1, gamma=0.99, epsilon=0.1, n_planning=5)
}

# EntraÃ®ner et sauvegarder
results = {}
for name, algo in algorithms.items():
    print(f"EntraÃ®nement {name}...")
    results[name] = algo.train(num_episodes=1000)
    algo.save(f'models/{name.lower().replace("-", "_")}.pkl')

# Visualiser les rÃ©sultats
for name, result in results.items():
    if result and 'history' in result:
        rewards = [entry['reward'] for entry in result['history']]
        plt.plot(rewards, label=name)

plt.xlabel('Ã‰pisode')
plt.ylabel('RÃ©compense')
plt.title('Comparaison des Algorithmes')
plt.legend()
plt.show()
```

## ğŸ”§ DÃ©veloppement

### Ajouter un nouvel algorithme

1. CrÃ©er la classe dans le module appropriÃ©
2. HÃ©riter de la structure commune avec mÃ©thodes `train()`, `save()`, `load()`
3. Ajouter les imports dans `src/__init__.py`
4. CrÃ©er les tests dans `tests/`
5. Documenter dans le README

### Structure d'une classe d'algorithme

```python
class NouvelAlgorithme:
    def __init__(self, env, **params):
        self.env = env
        self.history = []
        # Autres paramÃ¨tres...
    
    def train(self, num_episodes=1000):
        # Logique d'entraÃ®nement
        return {'history': self.history}
    
    def save(self, filepath):
        model_data = {
            'param1': self.param1,
            'history': self.history
        }
        save_model(model_data, filepath)
    
    def load(self, filepath):
        model_data = load_model(filepath)
        self.param1 = model_data['param1']
        self.history = model_data['history']
```

## ğŸ“š Ressources

- [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [OpenAI Gym Documentation](https://gym.openai.com/)
- [NumPy Documentation](https://numpy.org/doc/)

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit les changements (`git commit -m 'Ajout nouvelle fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. CrÃ©er une Pull Request

## ğŸ› ProblÃ¨mes Connus

- Les algorithmes DP nÃ©cessitent un environnement avec dynamique connue
- Les formats JSON ne supportent pas tous les types NumPy complexes
- Joblib requis pour le format joblib (`pip install joblib`)

## ğŸ“ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ‘¥ Auteurs

- **Votre nom** - DÃ©veloppement initial
- **Ã‰quipe** - Contributions diverses

## ğŸ™ Remerciements

- Professeurs et encadrants du cours de Deep Learning
- CommunautÃ© OpenAI Gym
- Sutton & Barto pour leur livre rÃ©fÃ©rence 